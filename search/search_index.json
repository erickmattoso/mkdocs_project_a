{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Presentation"},{"location":"intro/","text":"Introduction \u00b6 Et consequat nulla labore minim ipsum excepteur ullamco irure incididunt quis dolor laboris. In dolor quis in est ex. Irure consectetur proident ullamco dolore in sint enim excepteur nisi laboris. Cillum sit anim et adipisicing qui aliqua ullamco officia occaecat labore sunt. Lorem nisi duis esse veniam enim occaecat dolor aliqua cillum cupidatat. Aliqua in adipisicing Lorem laborum sit ad excepteur aliquip. Non sint sit laboris aute aliqua. Sint minim adipisicing id nisi Lorem duis aliquip sunt cillum laboris enim. Ea ullamco proident irure non in. Aliquip consequat voluptate pariatur sint in ea eiusmod sint laborum reprehenderit deserunt deserunt sit nisi. Ullamco aliquip eiusmod cupidatat in laboris voluptate eiusmod. Nostrud nulla aute consectetur qui reprehenderit excepteur eiusmod et velit commodo sunt magna irure. Ullamco eu enim commodo incididunt cupidatat sit qui. Consequat deserunt labore minim aute laboris. Lorem duis veniam fugiat laboris culpa cupidatat non labore sit pariatur enim. Lorem aliqua eiusmod incididunt laborum excepteur sit cupidatat nostrud exercitation enim est. Duis magna ad aute mollit. Id est duis sit consequat pariatur deserunt commodo cupidatat cupidatat elit culpa. Excepteur anim nostrud minim officia minim esse amet cillum fugiat nisi. Elit cillum qui proident mollit labore qui occaecat ut ex laboris nisi ullamco veniam. Aute consectetur pariatur deserunt velit nostrud cupidatat laboris veniam. Dolor non ex aute non minim reprehenderit tempor labore est id nulla do voluptate. Voluptate consequat velit in dolore qui ullamco fugiat ex aliqua ullamco excepteur do. Incididunt esse irure excepteur id anim anim. Nostrud enim Lorem magna amet. Et in do ea irure culpa. Voluptate aliqua do sunt ea velit cillum proident aute. Dolor ex reprehenderit ad officia cillum dolor fugiat. Sit culpa veniam veniam est velit dolor. Qui ea fugiat laboris nisi adipisicing excepteur ex ex excepteur id tempor minim qui. Reprehenderit pariatur consequat esse pariatur enim id incididunt. Lorem eu adipisicing adipisicing mollit aliquip nulla occaecat aute do consectetur.","title":"Introduction"},{"location":"intro/#introduction","text":"Et consequat nulla labore minim ipsum excepteur ullamco irure incididunt quis dolor laboris. In dolor quis in est ex. Irure consectetur proident ullamco dolore in sint enim excepteur nisi laboris. Cillum sit anim et adipisicing qui aliqua ullamco officia occaecat labore sunt. Lorem nisi duis esse veniam enim occaecat dolor aliqua cillum cupidatat. Aliqua in adipisicing Lorem laborum sit ad excepteur aliquip. Non sint sit laboris aute aliqua. Sint minim adipisicing id nisi Lorem duis aliquip sunt cillum laboris enim. Ea ullamco proident irure non in. Aliquip consequat voluptate pariatur sint in ea eiusmod sint laborum reprehenderit deserunt deserunt sit nisi. Ullamco aliquip eiusmod cupidatat in laboris voluptate eiusmod. Nostrud nulla aute consectetur qui reprehenderit excepteur eiusmod et velit commodo sunt magna irure. Ullamco eu enim commodo incididunt cupidatat sit qui. Consequat deserunt labore minim aute laboris. Lorem duis veniam fugiat laboris culpa cupidatat non labore sit pariatur enim. Lorem aliqua eiusmod incididunt laborum excepteur sit cupidatat nostrud exercitation enim est. Duis magna ad aute mollit. Id est duis sit consequat pariatur deserunt commodo cupidatat cupidatat elit culpa. Excepteur anim nostrud minim officia minim esse amet cillum fugiat nisi. Elit cillum qui proident mollit labore qui occaecat ut ex laboris nisi ullamco veniam. Aute consectetur pariatur deserunt velit nostrud cupidatat laboris veniam. Dolor non ex aute non minim reprehenderit tempor labore est id nulla do voluptate. Voluptate consequat velit in dolore qui ullamco fugiat ex aliqua ullamco excepteur do. Incididunt esse irure excepteur id anim anim. Nostrud enim Lorem magna amet. Et in do ea irure culpa. Voluptate aliqua do sunt ea velit cillum proident aute. Dolor ex reprehenderit ad officia cillum dolor fugiat. Sit culpa veniam veniam est velit dolor. Qui ea fugiat laboris nisi adipisicing excepteur ex ex excepteur id tempor minim qui. Reprehenderit pariatur consequat esse pariatur enim id incididunt. Lorem eu adipisicing adipisicing mollit aliquip nulla occaecat aute do consectetur.","title":"Introduction"},{"location":"Documentation/Code%20Documentation/eda/","text":"import os import math import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.feature_selection import chi2 from sklearn.feature_selection import mutual_info_classif from sklearn.feature_selection import VarianceThreshold from sklearn.utils import resample from scipy.stats import ttest_ind import matplotlib.pyplot as plt import seaborn as sns Data Reading \u00b6 Read Data \u00b6 cols = [ 'age' , 'workclass' , 'fnlwgt' , 'education' , 'education-num' , 'status' , 'occupation' , 'relationship' , 'race' , 'sex' , 'capital-gain' , 'capital-loss' , 'hours-per-week' , 'native-country' , 'income-class' ] df = pd . read_csv ( 'adult.data' , names = cols ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income-class 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States <=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States <=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States <=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States <=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba <=50K Overall Inspection \u00b6 # check data types of each column df . dtypes age int64 workclass object fnlwgt int64 education object education-num int64 status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object income-class object dtype: object # take a look at a obejct-type value in the table. # it's obvious that there are leading whitespaces in it, which can be annoying later on. df . loc [ 0 , 'income-class' ] ' <=50K' # remove leading and trailing whitespaces in values that have strings string_df = df . select_dtypes ( include = 'object' ) df [ string_df . columns ] = string_df . apply ( lambda x : x . str . strip ()) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income-class 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States <=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States <=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States <=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States <=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba <=50K # check if there are missing values df . isnull () . sum () age 0 workclass 0 fnlwgt 0 education 0 education-num 0 status 0 occupation 0 relationship 0 race 0 sex 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 0 income-class 0 dtype: int64 Descriptive Analysis (Univariate) \u00b6 Numeric Variables \u00b6 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age fnlwgt education-num capital-gain capital-loss hours-per-week count 32561.000000 3.256100e+04 32561.000000 32561.000000 32561.000000 32561.000000 mean 38.581647 1.897784e+05 10.080679 1077.648844 87.303830 40.437456 std 13.640433 1.055500e+05 2.572720 7385.292085 402.960219 12.347429 min 17.000000 1.228500e+04 1.000000 0.000000 0.000000 1.000000 25% 28.000000 1.178270e+05 9.000000 0.000000 0.000000 40.000000 50% 37.000000 1.783560e+05 10.000000 0.000000 0.000000 40.000000 75% 48.000000 2.370510e+05 12.000000 0.000000 0.000000 45.000000 max 90.000000 1.484705e+06 16.000000 99999.000000 4356.000000 99.000000 From the summary above, we have several findings: 1. The majority of data in capital-gain and capital-loss is 0. 2. 99999 in capital-gain looks suspicious, it might be a default value when there's no entry of data. 3. Minimum hours-per-week is 1 hour, and maximum hours-per-week is 99 hours. Both can be outliers. In order to identify outliers in these numeric columns, we need to draw histograms and boxplots to check their distributions. To save space on this page, the histograms are not printed. # histogram and boxplot for numeric columns # num_df = df.select_dtypes(include=np.number) # num_cols = num_df.shape[1] # plot_height = 3 # plot_width = 8 # fig, axes = plt.subplots(nrows=num_cols, sharey=True, figsize=(plot_width, num_cols*plot_height)) # for idx, col in enumerate(num_df.columns): # ax = axes[idx] # sns.distplot(num_df[col], kde=False , ax=ax) # #ax.hist(num_df[col], bins=20) # ax.set_title('Histogram of %s' % col, fontweight='bold') # plt.tight_layout() fig , axes = plt . subplots ( nrows = num_cols , sharey = True , figsize = ( plot_width , num_cols * plot_height )) for idx , col in enumerate ( num_df . columns ): ax = axes [ idx ] sns . boxplot ( num_df [ col ], ax = ax ) ax . set_title ( 'Boxplot of %s ' % col , fontweight = 'bold' ) plt . tight_layout () The plots above suggest that: 1. The distribution of all numeric columns are very skewed. 2. All numeric columns have data points that fall out of normal ranges. Since we don't have more detailed information about the dataset, we want to be conservative at this point and don't take any actions on the outliers suggested by the boxplots. However, we can process the two special columns: capital-gain and capital-loss where most values are zero. To not lose information that is potentially useful, we create new columns that indicate whether the values are zero or not instead of removing abnormal values or imputing them with zero, and remove column capital-gain and column capital-loss. # create two indicator columns df [ 'capital-gain-zero' ] = 1 df . loc [ df [ 'capital-gain' ] > 0 , 'capital-gain-zero' ] = 0 df [ 'capital-loss-zero' ] = 1 df . loc [ df [ 'capital-loss' ] > 0 , 'capital-loss-zero' ] = 0 df [ 'capital-gain-zero' ] . value_counts () 1 29849 0 2712 Name: capital-gain-zero, dtype: int64 df [ 'capital-loss-zero' ] . value_counts () 1 31042 0 1519 Name: capital-loss-zero, dtype: int64 Categorical Variables \u00b6 # categorical columns cat_df = df . select_dtypes ( include = 'object' ) cat_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } workclass education status occupation relationship race sex native-country income-class 0 State-gov Bachelors Never-married Adm-clerical Not-in-family White Male United-States <=50K 1 Self-emp-not-inc Bachelors Married-civ-spouse Exec-managerial Husband White Male United-States <=50K 2 Private HS-grad Divorced Handlers-cleaners Not-in-family White Male United-States <=50K 3 Private 11th Married-civ-spouse Handlers-cleaners Husband Black Male United-States <=50K 4 Private Bachelors Married-civ-spouse Prof-specialty Wife Black Female Cuba <=50K num_cols = cat_df . shape [ 1 ] plot_height = 6 plot_width = 10 #nrows = math.ceil(num_cols/2) fig , axes = plt . subplots ( nrows = num_cols , ncols = 1 , sharey = True , figsize = ( plot_width , num_cols * plot_height )) for idx , col in enumerate ( cat_df . columns ): # row_idx = idx // 2 # col_idx = idx % 2 vc = cat_df [ col ] . value_counts () / len ( df ) * 100 # ax = axes[row_idx][col_idx] ax = axes [ idx ] sns . barplot ( x = vc . index , y = vc . values , ax = ax ) ax . tick_params ( labelrotation = 45 ) ax . set_title ( 'Value counts of %s ' % col , fontweight = 'bold' ) ax . set_xlabel ( col ) ax . set_ylabel ( 'percentage' ) plt . tight_layout () # the bar plot of native country looks messy so we print out value counts here cat_df [ 'native-country' ] . value_counts () United-States 29170 Mexico 643 ? 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&Tobago 19 Laos 18 Thailand 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Honduras 13 Hungary 13 Scotland 12 Holand-Netherlands 1 Name: native-country, dtype: int64 The bar plots above suggest: 1. Heavily imbalanced distributions exist in: - work class where \"private\" is the dominant value - race where \"white\" is the dominant value - sex where \"male\" is the dominant value - native country where \"United States\" is the dominant value - income-class where \"<=50K\" is the dominant value, meaning the target variable is imbalanced Column education and column ducation-num contain the same information Column native-country has a high cardinality \"Never-worked\" and \"Without-pay\" in column workclass could be outliers Therefore, we want to map column education to column education-num and keep only one of them, in this case, education-num as it's already in a numeric format. In addition, we will encode \"United States\" in column native-country as 1 and all the other countries as 0 to handle the high cardinality, and remove column native-country. We also need to investigate what income-class that \"Never-worked\" and \"Without-pay\" map to. # Maps education to education-num education = df [[ 'education' , 'education-num' ]] education_dict = {} for val in df [ 'education' ] . unique (): num = education [ education [ 'education' ] == val ][ 'education-num' ] . unique () education_dict [ num [ 0 ]] = val education_dict {13: 'Bachelors', 9: 'HS-grad', 7: '11th', 14: 'Masters', 5: '9th', 10: 'Some-college', 12: 'Assoc-acdm', 11: 'Assoc-voc', 4: '7th-8th', 16: 'Doctorate', 15: 'Prof-school', 3: '5th-6th', 6: '10th', 2: '1st-4th', 1: 'Preschool', 8: '12th'} df [ 'is_USA' ] = 1 df . loc [ df [ 'native-country' ] != 'United-States' , 'is_USA' ] = 0 df = df . drop ([ 'capital-gain' , 'capital-loss' , 'native-country' , 'education' ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA 0 39 State-gov 77516 13 Never-married Adm-clerical Not-in-family White Male 40 <=50K 0 1 1 1 50 Self-emp-not-inc 83311 13 Married-civ-spouse Exec-managerial Husband White Male 13 <=50K 1 1 1 2 38 Private 215646 9 Divorced Handlers-cleaners Not-in-family White Male 40 <=50K 1 1 1 3 53 Private 234721 7 Married-civ-spouse Handlers-cleaners Husband Black Male 40 <=50K 1 1 1 4 28 Private 338409 13 Married-civ-spouse Prof-specialty Wife Black Female 40 <=50K 1 1 0 # How much do \"Never-worked\" and \"Without-pay\" earn? no_pay_df = df . loc [ df [ 'workclass' ] . isin ([ 'Never-worked' , 'Without-pay' ]), [ 'workclass' , 'income-class' ]] no_pay_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } workclass income-class 1901 Without-pay <=50K 5361 Never-worked <=50K 9257 Without-pay <=50K 10845 Never-worked <=50K 14772 Never-worked <=50K 15533 Without-pay <=50K 15695 Without-pay <=50K 16812 Without-pay <=50K 20073 Without-pay <=50K 20337 Never-worked <=50K 21944 Without-pay <=50K 22215 Without-pay <=50K 23232 Never-worked <=50K 24596 Without-pay <=50K 25500 Without-pay <=50K 27747 Without-pay <=50K 28829 Without-pay <=50K 29158 Without-pay <=50K 32262 Without-pay <=50K 32304 Never-worked <=50K 32314 Never-worked <=50K From the table above, we can see that there are very few instances in data whose workclass are \"Without-pay\" or \"Never-worked\". All these instances earn less than 50K dollars. Thus, we create a dummy variable \"no_pay_or_work\" to indicate if a workclass is no pay or not. df [ 'no_pay_or_work' ] = 0 df . loc [ df [ 'workclass' ] . isin ([ 'Never-worked' , 'Without-pay' ]), 'no_pay_or_work' ] = 1 df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work 0 39 State-gov 77516 13 Never-married Adm-clerical Not-in-family White Male 40 <=50K 0 1 1 0 1 50 Self-emp-not-inc 83311 13 Married-civ-spouse Exec-managerial Husband White Male 13 <=50K 1 1 1 0 2 38 Private 215646 9 Divorced Handlers-cleaners Not-in-family White Male 40 <=50K 1 1 1 0 3 53 Private 234721 7 Married-civ-spouse Handlers-cleaners Husband Black Male 40 <=50K 1 1 1 0 4 28 Private 338409 13 Married-civ-spouse Prof-specialty Wife Black Female 40 <=50K 1 1 0 0 Correlation Analysis (Bivariate) \u00b6 Part 1: Qualitative Analysis 1. Qualitative correlation between two categorical variables - Contingency table 2. Qualitative correlation between a categorical variable and a numeric variable - Histogram of the numeric variable per unique value of the categorical variable Part 2: Quantitative Analysis 1. Quantitative correlation between two categorical variables - Chi-square - Mutual information 2. Quantitative correlation between a categorical variable and a numeric variable - Student T-test 3. Quantitative correlation between two numeric variables - Pearson Correlation Qualitative Analysis \u00b6 Betweeen Categorical Variables \u00b6 # Investigate the correlation between education-num and income-class given different workclasses pd . crosstab ( df [ 'education-num' ], [ df [ 'workclass' ], df [ 'income-class' ]]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } workclass ? Federal-gov Local-gov Never-worked Private Self-emp-inc Self-emp-not-inc State-gov Without-pay income-class <=50K >50K <=50K >50K <=50K >50K <=50K <=50K >50K <=50K >50K <=50K >50K <=50K >50K <=50K education-num 1 5 0 0 0 4 0 0 41 0 0 0 0 0 1 0 0 2 12 0 0 0 4 0 0 131 5 2 0 12 1 1 0 0 3 28 2 1 0 8 1 0 259 7 2 2 15 4 4 0 0 4 70 2 2 0 27 1 1 406 18 9 5 80 14 10 0 1 5 50 1 2 1 20 3 0 369 18 10 0 30 4 6 0 0 6 98 2 6 0 30 1 2 648 47 16 3 60 7 11 2 0 7 118 0 8 1 34 2 1 878 45 10 4 53 7 13 1 0 8 38 2 5 0 17 2 0 310 23 6 1 16 3 8 2 0 9 486 46 190 73 413 90 1 6661 1119 160 119 687 179 219 49 9 10 479 35 172 82 294 93 2 4171 923 110 116 379 107 294 31 3 11 48 13 23 15 61 25 0 749 256 19 19 87 21 34 12 0 12 41 6 36 19 60 28 0 559 170 17 18 53 18 35 6 1 13 128 45 117 95 315 162 0 2056 1495 102 171 236 163 180 90 0 14 30 18 20 47 169 173 0 360 534 22 57 65 59 98 71 0 15 10 8 6 23 10 19 0 86 171 3 78 25 106 13 18 0 16 4 11 1 15 10 17 0 49 132 6 29 19 31 18 71 0 Observation from above: Education level seems to have influences on income. In some workclass, when education-num exceeds 13 (Bachelor), the number of people who earn >50K is more than that of people who earn <=50k. Thus, we consider creating a dummy variable to suggest whether a person's education level is above bachelor or not. # Investigate the correlation between occupation and income-class given different genders table = pd . crosstab ( df [ 'occupation' ], [ df [ 'sex' ], df [ 'income-class' ]]) # calculate percentages table [( 'Female' , '>50K percent' )] = round ( table [( 'Female' , '>50K' )] / ( table [( 'Female' , '<=50K' )] + table [( 'Female' , '>50K' )]) * 100 , 2 ) table [( 'Male' , '>50K percent' )] = round ( table [( 'Male' , '>50K' )] / ( table [( 'Male' , '<=50K' )] + table [( 'Male' , '>50K' )]) * 100 , 2 ) table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } sex Female Male Female Male income-class <=50K >50K <=50K >50K >50K percent >50K percent occupation ? 789 52 863 139 6.18 13.87 Adm-clerical 2325 212 938 295 8.36 23.93 Armed-Forces 0 0 8 1 NaN 11.11 Craft-repair 202 20 2968 909 9.01 23.45 Exec-managerial 879 280 1219 1688 24.16 58.07 Farming-fishing 63 2 816 113 3.08 12.16 Handlers-cleaners 160 4 1124 82 2.44 6.80 Machine-op-inspct 530 20 1222 230 3.64 15.84 Other-service 1749 51 1409 86 2.83 5.75 Priv-house-serv 140 1 8 0 0.71 0.00 Prof-specialty 1130 385 1151 1474 25.41 56.15 Protective-serv 66 10 372 201 13.16 35.08 Sales 1175 88 1492 895 6.97 37.49 Tech-support 303 45 342 238 12.93 41.03 Transport-moving 81 9 1196 311 10.00 20.64 Observation from above: Gender seems to have a big influence on income. On almost all occupations, male have higher income than female. For example, only 8.36% female make more than 50K as adm-clerical whereas 23.93% male make more than 50K in the same occupation. # Investigate the correlation between occupation and income-class given different races table = pd . crosstab ( df [ 'occupation' ], [ df [ 'race' ], df [ 'income-class' ]]) # calculate percentages table [( 'White' , '>50K percent' )] = round ( table [( 'White' , '>50K' )] / ( table [( 'White' , '<=50K' )] + table [( 'White' , '>50K' )]) * 100 , 2 ) table [( 'Other' , '>50K percent' )] = round ( table [( 'Other' , '>50K' )] / ( table [( 'Other' , '<=50K' )] + table [( 'Other' , '>50K' )]) * 100 , 2 ) table [( 'Black' , '>50K percent' )] = round ( table [( 'Black' , '>50K' )] / ( table [( 'Black' , '<=50K' )] + table [( 'Black' , '>50K' )]) * 100 , 2 ) table [( 'Asian-Pac-Islander' , '>50K percent' )] = round ( table [( 'Asian-Pac-Islander' , '>50K' )] / ( table [( 'Asian-Pac-Islander' , '<=50K' )] + table [( 'Asian-Pac-Islander' , '>50K' )]) * 100 , 2 ) table [( 'Amer-Indian-Eskimo' , '>50K percent' )] = round ( table [( 'Amer-Indian-Eskimo' , '>50K' )] / ( table [( 'Amer-Indian-Eskimo' , '<=50K' )] + table [( 'Amer-Indian-Eskimo' , '>50K' )]) * 100 , 2 ) percent_cols = [ col for col in table . columns if 'percent' in col [ 1 ]] table [ percent_cols ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } race White Other Black Asian-Pac-Islander Amer-Indian-Eskimo income-class >50K percent >50K percent >50K percent >50K percent >50K percent occupation ? 11.42 8.70 4.19 7.69 8.00 Adm-clerical 14.23 3.85 8.57 15.83 9.68 Armed-Forces 14.29 NaN 0.00 NaN 0.00 Craft-repair 22.85 17.86 20.08 28.09 13.64 Exec-managerial 49.86 18.18 34.43 45.19 10.00 Farming-fishing 12.35 0.00 0.00 12.50 0.00 Handlers-cleaners 6.53 0.00 6.15 4.35 0.00 Machine-op-inspct 13.47 2.56 8.03 16.95 0.00 Other-service 4.00 0.00 2.98 13.28 6.06 Priv-house-serv 0.88 0.00 0.00 0.00 NaN Prof-specialty 46.07 29.03 27.62 48.92 33.33 Protective-serv 34.30 20.00 24.51 33.33 25.00 Sales 28.54 12.00 12.20 19.44 15.38 Tech-support 32.01 0.00 18.31 27.27 0.00 Transport-moving 21.62 7.14 10.59 14.29 12.00 Observation from above: Race seems to have an influence on income. White and Asian-Pac-Islanders have higher income than other races on almost all occupations. # Investigate the correlation between education-num and race table = pd . crosstab ( df [ 'education-num' ], df [ 'race' ]) table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } race Amer-Indian-Eskimo Asian-Pac-Islander Black Other White education-num 1 0 6 5 2 38 2 4 5 16 9 134 3 2 18 21 13 278 4 9 11 56 16 551 5 5 9 82 7 385 6 15 13 124 7 634 7 12 20 138 8 817 8 5 9 69 13 300 9 119 225 1171 78 8877 10 79 206 743 51 6195 11 19 38 112 6 1206 12 8 29 106 8 915 13 21 286 329 33 4645 14 5 88 85 7 1520 15 2 36 15 2 475 16 3 28 11 2 357 Observation from above: Asians have an unusually high number in ed level 13 (Bachelor). Betweeen Categorical Variables and Numeric Variables \u00b6 # Hours-per-week vs income-class # seperate high income group and low income group low_df = df . loc [ df [ 'income-class' ] == '<=50K' , 'hours-per-week' ] high_df = df . loc [ df [ 'income-class' ] == '>50K' , 'hours-per-week' ] sns . distplot ( low_df , kde = False , color = 'y' , label = '<=50K' ) sns . distplot ( high_df , kde = False , color = 'k' , label = '>50K' ) plt . legend () <matplotlib.legend.Legend at 0x12928fa20> Observation from above: In high income group, the proportion of people who work more than 40 hours is higher than low income group. # Hours-per-week vs ge # seperate high income group and low income group low_df = df . loc [ df [ 'income-class' ] == '<=50K' , 'age' ] high_df = df . loc [ df [ 'income-class' ] == '>50K' , 'age' ] sns . distplot ( low_df , kde = False , color = 'y' , label = '<=50K' ) sns . distplot ( high_df , kde = False , color = 'k' , label = '>50K' ) plt . legend () <matplotlib.legend.Legend at 0x1293727f0> low_income = df . loc [ df [ 'income-class' ] == '<=50K' , [ 'age' , 'hours-per-week' ]] high_income = df . loc [ df [ 'income-class' ] == '>50K' , [ 'age' , 'hours-per-week' ]] print ( 'low income' ) print ( low_income . describe (), ' \\n ' ) print ( 'high income' ) print ( high_income . describe ()) low income age hours-per-week count 24325.000000 24325.000000 mean 37.104995 39.123947 std 13.903084 12.147302 min 18.000000 1.000000 25% 26.000000 36.000000 50% 34.000000 40.000000 75% 46.000000 40.000000 max 90.000000 99.000000 high income age hours-per-week count 7682.000000 7682.000000 mean 44.206196 45.383494 std 10.507283 10.964122 min 19.000000 1.000000 25% 36.000000 40.000000 50% 43.000000 40.000000 75% 51.000000 50.000000 max 90.000000 99.000000 Observation from above: The median age and average age in high income group are both higher than low income group. Quantitative Analysis \u00b6 Between Categorical Variables \u00b6 # make a copy of the processed data copy_df = df . copy () types = copy_df . dtypes types age int64 workclass object fnlwgt int64 education-num int64 status object occupation object relationship object race object sex object hours-per-week int64 income-class object capital-gain-zero int64 capital-loss-zero int64 is_USA int64 no_pay_or_work int64 dtype: object # to do quantitative anlaysis, we need to transform non-numeric variables into numeric ones oe = OrdinalEncoder () obj_cols = types [ types == 'object' ] . index . tolist () obj_cols . remove ( 'income-class' ) print ( obj_cols ) copy_df [ obj_cols ] = oe . fit_transform ( copy_df [ obj_cols ]) le = LabelEncoder () copy_df [ 'income-class' ] = le . fit_transform ( copy_df [ 'income-class' ]) ['workclass', 'status', 'occupation', 'relationship', 'race', 'sex'] # keep a record to map the original value to the encoded value oe . categories_ [array(['?', 'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc', 'Self-emp-not-inc', 'State-gov', 'Without-pay'], dtype=object), array(['Divorced', 'Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'], dtype=object), array(['?', 'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial', 'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct', 'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv', 'Sales', 'Tech-support', 'Transport-moving'], dtype=object), array(['Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried', 'Wife'], dtype=object), array(['Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'], dtype=object), array(['Female', 'Male'], dtype=object)] copy_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work 0 39 7.0 77516 13 4.0 1.0 1.0 4.0 1.0 40 0 0 1 1 0 1 50 6.0 83311 13 2.0 4.0 0.0 4.0 1.0 13 0 1 1 1 0 2 38 4.0 215646 9 0.0 6.0 1.0 4.0 1.0 40 0 1 1 1 0 3 53 4.0 234721 7 2.0 6.0 0.0 2.0 1.0 40 0 1 1 1 0 4 28 4.0 338409 13 2.0 10.0 5.0 2.0 0.0 40 0 1 1 0 0 # calculate chi2 score between each categorical variable and income-class cat_cols = obj_cols + [ 'education-num' , 'capital-gain-zero' , 'capital-loss-zero' , 'is_USA' ] print ( cat_cols ) chi2 , pval = chi2 ( copy_df [ cat_cols ], copy_df [ 'income-class' ]) ['workclass', 'status', 'occupation', 'relationship', 'race', 'sex', 'education-num', 'capital-gain-zero', 'capital-loss-zero', 'is_USA'] for i , chi2_score in enumerate ( chi2 ): print ( ' %s : %f ' % ( cat_cols [ i ], chi2_score )) workclass: 47.508119 status: 1123.469818 occupation: 504.558854 relationship: 3659.143125 race: 33.031305 sex: 502.439419 education-num: 2401.421777 capital-gain-zero: 192.123998 capital-loss-zero: 29.218864 is_USA: 4.029206 for i , pv in enumerate ( pval ): print ( ' %s : %f ' % ( cat_cols [ i ], pv )) workclass: 0.000000 status: 0.000000 occupation: 0.000000 relationship: 0.000000 race: 0.000000 sex: 0.000000 education-num: 0.000000 capital-gain-zero: 0.000000 capital-loss-zero: 0.000000 is_USA: 0.044719 # calculate mutual information score between each categorical variable and income-class mi = mutual_info_classif ( copy_df [ cat_cols ], copy_df [ 'income-class' ]) for i , v in enumerate ( mi ): print ( ' %s : %f ' % ( cat_cols [ i ], v )) workclass: 0.018099 status: 0.111759 occupation: 0.060578 relationship: 0.116259 race: 0.011263 sex: 0.026260 education-num: 0.067128 capital-gain-zero: 0.034813 capital-loss-zero: 0.011882 is_USA: 0.007318 Observation from above: Chi2 score shows that all categorical variables are dependent with income-class with a significance level set at 0.05. Among them, is_USA is the least dependent one. Relationship, education-num and status are suggested to be the most relevant variables by both chi2 score and MI score. Between Categorical Variables and Numeric Variables \u00b6 # do student T-test on numeric variables and income-class num_cols = [ 'age' , 'fnlwgt' , 'hours-per-week' ] high_income_df = copy_df . loc [ copy_df [ 'income-class' ] == 1 , num_cols ] low_income_df = copy_df . loc [ copy_df [ 'income-class' ] == 0 , num_cols ] # check if the samples in high income group and low income group have equal mean, and equal (close) variance mean_df = pd . concat ([ high_income_df . mean (), low_income_df . mean ()], axis = 1 ) \\ . rename ( columns = { 0 : 'high_income' , 1 : 'low_income' }) # N-1 to calculate variance by default var_df = pd . concat ([ high_income_df . var (), low_income_df . var ()], axis = 1 ) \\ . rename ( columns = { 0 : 'high_income' , 1 : 'low_income' }) mean_df # unequal mean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } high_income low_income age 44.249841 36.783738 fnlwgt 188005.000000 190340.865170 hours-per-week 45.473026 38.840210 var_df # 1/2 < var1/var2 < 2, so very close variance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } high_income low_income age 1.106499e+02 1.965629e+02 fnlwgt 1.051482e+10 1.133847e+10 hours-per-week 1.212855e+02 1.517576e+02 # As we have unequal mean and almost equal variance in two samples for each numeric variables, # we can use the default settings in scipy ttest_ind() for col in num_cols : print ( col ) t_stat , pval = ttest_ind ( high_income_df [ col ], low_income_df [ col ]) print ( 't statistic: %f ' % t_stat ) print ( 'p value: %f ' % pval , ' \\n ' ) age t statistic: 43.436244 p value: 0.000000 fnlwgt t statistic: -1.707511 p value: 0.087737 hours-per-week t statistic: 42.583873 p value: 0.000000 Observation from above: Student T-test shows both age and hours-per-week are dependent with income-class with a significance level set at 0.05. This aligns with our findings in qualitative analysis. fnlwgt appears to be independent so we will remove it. Between Numeric Variables \u00b6 # correlation between numeric features sns . pairplot ( copy_df [ num_cols ]) <seaborn.axisgrid.PairGrid at 0x1308d4860> copy_df [ num_cols ] . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age fnlwgt hours-per-week age 1.000000 -0.079375 0.039007 fnlwgt -0.079375 1.000000 -0.021169 hours-per-week 0.039007 -0.021169 1.000000 Observation from above: No pair of numeric variables have strong correlations. Feature Engineering \u00b6 Part 1: Feature selection - based on the findings in correlation analysis - based on additional criteria Part 2: Skewnewss Part 3: Encoding Feature Selection \u00b6 # remove fnlwgt copy_df . drop ( 'fnlwgt' , axis = 1 , inplace = True ) # create a dummy variable to indicate if a person's education level is above bachelor degree copy_df [ 'bachelor_above' ] = 0 copy_df . loc [ copy_df [ 'education-num' ] >= 13 , 'bachelor_above' ] = 1 copy_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work bachelor_above 0 39 7.0 13 4.0 1.0 1.0 4.0 1.0 40 0 0 1 1 0 1 1 50 6.0 13 2.0 4.0 0.0 4.0 1.0 13 0 1 1 1 0 1 2 38 4.0 9 0.0 6.0 1.0 4.0 1.0 40 0 1 1 1 0 0 3 53 4.0 7 2.0 6.0 0.0 2.0 1.0 40 0 1 1 1 0 0 4 28 4.0 13 2.0 10.0 5.0 2.0 0.0 40 0 1 1 0 0 1 # remove variables where the variace is below a certain threshold print ( 'shape before: %s ' % str ( copy_df . shape )) selector = VarianceThreshold () features = copy_df . drop ( 'income-class' , axis = 1 ) . columns copy_df [ features ] = selector . fit_transform ( copy_df [ features ]) print ( 'shape after: %s ' % str ( copy_df . shape )) shape before: (32561, 15) shape after: (32561, 15) Skewness \u00b6 # distribution of age is very skewed sns . distplot ( copy_df [ 'age' ], kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a108c18> sns . distplot ( np . log ( copy_df [ 'age' ]), kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a28a9e8> # skewness is reduced by applying log transform even though it's still not a perfect bell shape print ( copy_df [ 'age' ] . skew ()) print ( np . log ( copy_df [ 'age' ]) . skew ()) 0.5587433694130484 -0.1317299194198282 # apply log transform to age copy_df [ 'age' ] = np . log ( copy_df [ 'age' ]) # distribution of hours-per-week is not as skewed sns . distplot ( copy_df [ 'hours-per-week' ], kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a24ed30> copy_df [ 'hours-per-week' ] . skew () 0.22764253680450092 # save a checkpoint # ordinal encoded, imbalanced #copy_df.to_csv('adult_oe_im.csv', index=False) Encoding \u00b6 # Apply one hot encoding for modeling purposes ohe_cols = [ 'workclass' , 'status' , 'occupation' , 'relationship' , 'race' , 'sex' ] ohe_df = pd . get_dummies ( data = copy_df , columns = ohe_cols ) ohe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age education-num hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work bachelor_above workclass_0.0 ... relationship_3.0 relationship_4.0 relationship_5.0 race_0.0 race_1.0 race_2.0 race_3.0 race_4.0 sex_0.0 sex_1.0 0 3.663562 13.0 40.0 0 0.0 1.0 1.0 0.0 1.0 0 ... 0 0 0 0 0 0 0 1 0 1 1 3.912023 13.0 13.0 0 1.0 1.0 1.0 0.0 1.0 0 ... 0 0 0 0 0 0 0 1 0 1 2 3.637586 9.0 40.0 0 1.0 1.0 1.0 0.0 0.0 0 ... 0 0 0 0 0 0 0 1 0 1 3 3.970292 7.0 40.0 0 1.0 1.0 1.0 0.0 0.0 0 ... 0 0 0 0 0 1 0 0 0 1 4 3.332205 13.0 40.0 0 1.0 1.0 0.0 0.0 1.0 0 ... 0 0 1 0 0 1 0 0 1 0 5 rows \u00d7 53 columns # save a checkpoint # one hot encoded, imbalanced #ohe_df.to_csv('adult_ohe_im.csv', index=False)","title":"Eda"},{"location":"Documentation/Code%20Documentation/eda/#data-reading","text":"","title":"Data Reading"},{"location":"Documentation/Code%20Documentation/eda/#read-data","text":"cols = [ 'age' , 'workclass' , 'fnlwgt' , 'education' , 'education-num' , 'status' , 'occupation' , 'relationship' , 'race' , 'sex' , 'capital-gain' , 'capital-loss' , 'hours-per-week' , 'native-country' , 'income-class' ] df = pd . read_csv ( 'adult.data' , names = cols ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income-class 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States <=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States <=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States <=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States <=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba <=50K","title":"Read Data"},{"location":"Documentation/Code%20Documentation/eda/#overall-inspection","text":"# check data types of each column df . dtypes age int64 workclass object fnlwgt int64 education object education-num int64 status object occupation object relationship object race object sex object capital-gain int64 capital-loss int64 hours-per-week int64 native-country object income-class object dtype: object # take a look at a obejct-type value in the table. # it's obvious that there are leading whitespaces in it, which can be annoying later on. df . loc [ 0 , 'income-class' ] ' <=50K' # remove leading and trailing whitespaces in values that have strings string_df = df . select_dtypes ( include = 'object' ) df [ string_df . columns ] = string_df . apply ( lambda x : x . str . strip ()) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income-class 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States <=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States <=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States <=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States <=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba <=50K # check if there are missing values df . isnull () . sum () age 0 workclass 0 fnlwgt 0 education 0 education-num 0 status 0 occupation 0 relationship 0 race 0 sex 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 0 income-class 0 dtype: int64","title":"Overall Inspection"},{"location":"Documentation/Code%20Documentation/eda/#descriptive-analysis-univariate","text":"","title":"Descriptive Analysis (Univariate)"},{"location":"Documentation/Code%20Documentation/eda/#numeric-variables","text":"df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age fnlwgt education-num capital-gain capital-loss hours-per-week count 32561.000000 3.256100e+04 32561.000000 32561.000000 32561.000000 32561.000000 mean 38.581647 1.897784e+05 10.080679 1077.648844 87.303830 40.437456 std 13.640433 1.055500e+05 2.572720 7385.292085 402.960219 12.347429 min 17.000000 1.228500e+04 1.000000 0.000000 0.000000 1.000000 25% 28.000000 1.178270e+05 9.000000 0.000000 0.000000 40.000000 50% 37.000000 1.783560e+05 10.000000 0.000000 0.000000 40.000000 75% 48.000000 2.370510e+05 12.000000 0.000000 0.000000 45.000000 max 90.000000 1.484705e+06 16.000000 99999.000000 4356.000000 99.000000 From the summary above, we have several findings: 1. The majority of data in capital-gain and capital-loss is 0. 2. 99999 in capital-gain looks suspicious, it might be a default value when there's no entry of data. 3. Minimum hours-per-week is 1 hour, and maximum hours-per-week is 99 hours. Both can be outliers. In order to identify outliers in these numeric columns, we need to draw histograms and boxplots to check their distributions. To save space on this page, the histograms are not printed. # histogram and boxplot for numeric columns # num_df = df.select_dtypes(include=np.number) # num_cols = num_df.shape[1] # plot_height = 3 # plot_width = 8 # fig, axes = plt.subplots(nrows=num_cols, sharey=True, figsize=(plot_width, num_cols*plot_height)) # for idx, col in enumerate(num_df.columns): # ax = axes[idx] # sns.distplot(num_df[col], kde=False , ax=ax) # #ax.hist(num_df[col], bins=20) # ax.set_title('Histogram of %s' % col, fontweight='bold') # plt.tight_layout() fig , axes = plt . subplots ( nrows = num_cols , sharey = True , figsize = ( plot_width , num_cols * plot_height )) for idx , col in enumerate ( num_df . columns ): ax = axes [ idx ] sns . boxplot ( num_df [ col ], ax = ax ) ax . set_title ( 'Boxplot of %s ' % col , fontweight = 'bold' ) plt . tight_layout () The plots above suggest that: 1. The distribution of all numeric columns are very skewed. 2. All numeric columns have data points that fall out of normal ranges. Since we don't have more detailed information about the dataset, we want to be conservative at this point and don't take any actions on the outliers suggested by the boxplots. However, we can process the two special columns: capital-gain and capital-loss where most values are zero. To not lose information that is potentially useful, we create new columns that indicate whether the values are zero or not instead of removing abnormal values or imputing them with zero, and remove column capital-gain and column capital-loss. # create two indicator columns df [ 'capital-gain-zero' ] = 1 df . loc [ df [ 'capital-gain' ] > 0 , 'capital-gain-zero' ] = 0 df [ 'capital-loss-zero' ] = 1 df . loc [ df [ 'capital-loss' ] > 0 , 'capital-loss-zero' ] = 0 df [ 'capital-gain-zero' ] . value_counts () 1 29849 0 2712 Name: capital-gain-zero, dtype: int64 df [ 'capital-loss-zero' ] . value_counts () 1 31042 0 1519 Name: capital-loss-zero, dtype: int64","title":"Numeric Variables"},{"location":"Documentation/Code%20Documentation/eda/#categorical-variables","text":"# categorical columns cat_df = df . select_dtypes ( include = 'object' ) cat_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } workclass education status occupation relationship race sex native-country income-class 0 State-gov Bachelors Never-married Adm-clerical Not-in-family White Male United-States <=50K 1 Self-emp-not-inc Bachelors Married-civ-spouse Exec-managerial Husband White Male United-States <=50K 2 Private HS-grad Divorced Handlers-cleaners Not-in-family White Male United-States <=50K 3 Private 11th Married-civ-spouse Handlers-cleaners Husband Black Male United-States <=50K 4 Private Bachelors Married-civ-spouse Prof-specialty Wife Black Female Cuba <=50K num_cols = cat_df . shape [ 1 ] plot_height = 6 plot_width = 10 #nrows = math.ceil(num_cols/2) fig , axes = plt . subplots ( nrows = num_cols , ncols = 1 , sharey = True , figsize = ( plot_width , num_cols * plot_height )) for idx , col in enumerate ( cat_df . columns ): # row_idx = idx // 2 # col_idx = idx % 2 vc = cat_df [ col ] . value_counts () / len ( df ) * 100 # ax = axes[row_idx][col_idx] ax = axes [ idx ] sns . barplot ( x = vc . index , y = vc . values , ax = ax ) ax . tick_params ( labelrotation = 45 ) ax . set_title ( 'Value counts of %s ' % col , fontweight = 'bold' ) ax . set_xlabel ( col ) ax . set_ylabel ( 'percentage' ) plt . tight_layout () # the bar plot of native country looks messy so we print out value counts here cat_df [ 'native-country' ] . value_counts () United-States 29170 Mexico 643 ? 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&Tobago 19 Laos 18 Thailand 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Honduras 13 Hungary 13 Scotland 12 Holand-Netherlands 1 Name: native-country, dtype: int64 The bar plots above suggest: 1. Heavily imbalanced distributions exist in: - work class where \"private\" is the dominant value - race where \"white\" is the dominant value - sex where \"male\" is the dominant value - native country where \"United States\" is the dominant value - income-class where \"<=50K\" is the dominant value, meaning the target variable is imbalanced Column education and column ducation-num contain the same information Column native-country has a high cardinality \"Never-worked\" and \"Without-pay\" in column workclass could be outliers Therefore, we want to map column education to column education-num and keep only one of them, in this case, education-num as it's already in a numeric format. In addition, we will encode \"United States\" in column native-country as 1 and all the other countries as 0 to handle the high cardinality, and remove column native-country. We also need to investigate what income-class that \"Never-worked\" and \"Without-pay\" map to. # Maps education to education-num education = df [[ 'education' , 'education-num' ]] education_dict = {} for val in df [ 'education' ] . unique (): num = education [ education [ 'education' ] == val ][ 'education-num' ] . unique () education_dict [ num [ 0 ]] = val education_dict {13: 'Bachelors', 9: 'HS-grad', 7: '11th', 14: 'Masters', 5: '9th', 10: 'Some-college', 12: 'Assoc-acdm', 11: 'Assoc-voc', 4: '7th-8th', 16: 'Doctorate', 15: 'Prof-school', 3: '5th-6th', 6: '10th', 2: '1st-4th', 1: 'Preschool', 8: '12th'} df [ 'is_USA' ] = 1 df . loc [ df [ 'native-country' ] != 'United-States' , 'is_USA' ] = 0 df = df . drop ([ 'capital-gain' , 'capital-loss' , 'native-country' , 'education' ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA 0 39 State-gov 77516 13 Never-married Adm-clerical Not-in-family White Male 40 <=50K 0 1 1 1 50 Self-emp-not-inc 83311 13 Married-civ-spouse Exec-managerial Husband White Male 13 <=50K 1 1 1 2 38 Private 215646 9 Divorced Handlers-cleaners Not-in-family White Male 40 <=50K 1 1 1 3 53 Private 234721 7 Married-civ-spouse Handlers-cleaners Husband Black Male 40 <=50K 1 1 1 4 28 Private 338409 13 Married-civ-spouse Prof-specialty Wife Black Female 40 <=50K 1 1 0 # How much do \"Never-worked\" and \"Without-pay\" earn? no_pay_df = df . loc [ df [ 'workclass' ] . isin ([ 'Never-worked' , 'Without-pay' ]), [ 'workclass' , 'income-class' ]] no_pay_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } workclass income-class 1901 Without-pay <=50K 5361 Never-worked <=50K 9257 Without-pay <=50K 10845 Never-worked <=50K 14772 Never-worked <=50K 15533 Without-pay <=50K 15695 Without-pay <=50K 16812 Without-pay <=50K 20073 Without-pay <=50K 20337 Never-worked <=50K 21944 Without-pay <=50K 22215 Without-pay <=50K 23232 Never-worked <=50K 24596 Without-pay <=50K 25500 Without-pay <=50K 27747 Without-pay <=50K 28829 Without-pay <=50K 29158 Without-pay <=50K 32262 Without-pay <=50K 32304 Never-worked <=50K 32314 Never-worked <=50K From the table above, we can see that there are very few instances in data whose workclass are \"Without-pay\" or \"Never-worked\". All these instances earn less than 50K dollars. Thus, we create a dummy variable \"no_pay_or_work\" to indicate if a workclass is no pay or not. df [ 'no_pay_or_work' ] = 0 df . loc [ df [ 'workclass' ] . isin ([ 'Never-worked' , 'Without-pay' ]), 'no_pay_or_work' ] = 1 df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work 0 39 State-gov 77516 13 Never-married Adm-clerical Not-in-family White Male 40 <=50K 0 1 1 0 1 50 Self-emp-not-inc 83311 13 Married-civ-spouse Exec-managerial Husband White Male 13 <=50K 1 1 1 0 2 38 Private 215646 9 Divorced Handlers-cleaners Not-in-family White Male 40 <=50K 1 1 1 0 3 53 Private 234721 7 Married-civ-spouse Handlers-cleaners Husband Black Male 40 <=50K 1 1 1 0 4 28 Private 338409 13 Married-civ-spouse Prof-specialty Wife Black Female 40 <=50K 1 1 0 0","title":"Categorical Variables"},{"location":"Documentation/Code%20Documentation/eda/#correlation-analysis-bivariate","text":"Part 1: Qualitative Analysis 1. Qualitative correlation between two categorical variables - Contingency table 2. Qualitative correlation between a categorical variable and a numeric variable - Histogram of the numeric variable per unique value of the categorical variable Part 2: Quantitative Analysis 1. Quantitative correlation between two categorical variables - Chi-square - Mutual information 2. Quantitative correlation between a categorical variable and a numeric variable - Student T-test 3. Quantitative correlation between two numeric variables - Pearson Correlation","title":"Correlation Analysis (Bivariate)"},{"location":"Documentation/Code%20Documentation/eda/#qualitative-analysis","text":"","title":"Qualitative Analysis"},{"location":"Documentation/Code%20Documentation/eda/#betweeen-categorical-variables","text":"# Investigate the correlation between education-num and income-class given different workclasses pd . crosstab ( df [ 'education-num' ], [ df [ 'workclass' ], df [ 'income-class' ]]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } workclass ? Federal-gov Local-gov Never-worked Private Self-emp-inc Self-emp-not-inc State-gov Without-pay income-class <=50K >50K <=50K >50K <=50K >50K <=50K <=50K >50K <=50K >50K <=50K >50K <=50K >50K <=50K education-num 1 5 0 0 0 4 0 0 41 0 0 0 0 0 1 0 0 2 12 0 0 0 4 0 0 131 5 2 0 12 1 1 0 0 3 28 2 1 0 8 1 0 259 7 2 2 15 4 4 0 0 4 70 2 2 0 27 1 1 406 18 9 5 80 14 10 0 1 5 50 1 2 1 20 3 0 369 18 10 0 30 4 6 0 0 6 98 2 6 0 30 1 2 648 47 16 3 60 7 11 2 0 7 118 0 8 1 34 2 1 878 45 10 4 53 7 13 1 0 8 38 2 5 0 17 2 0 310 23 6 1 16 3 8 2 0 9 486 46 190 73 413 90 1 6661 1119 160 119 687 179 219 49 9 10 479 35 172 82 294 93 2 4171 923 110 116 379 107 294 31 3 11 48 13 23 15 61 25 0 749 256 19 19 87 21 34 12 0 12 41 6 36 19 60 28 0 559 170 17 18 53 18 35 6 1 13 128 45 117 95 315 162 0 2056 1495 102 171 236 163 180 90 0 14 30 18 20 47 169 173 0 360 534 22 57 65 59 98 71 0 15 10 8 6 23 10 19 0 86 171 3 78 25 106 13 18 0 16 4 11 1 15 10 17 0 49 132 6 29 19 31 18 71 0 Observation from above: Education level seems to have influences on income. In some workclass, when education-num exceeds 13 (Bachelor), the number of people who earn >50K is more than that of people who earn <=50k. Thus, we consider creating a dummy variable to suggest whether a person's education level is above bachelor or not. # Investigate the correlation between occupation and income-class given different genders table = pd . crosstab ( df [ 'occupation' ], [ df [ 'sex' ], df [ 'income-class' ]]) # calculate percentages table [( 'Female' , '>50K percent' )] = round ( table [( 'Female' , '>50K' )] / ( table [( 'Female' , '<=50K' )] + table [( 'Female' , '>50K' )]) * 100 , 2 ) table [( 'Male' , '>50K percent' )] = round ( table [( 'Male' , '>50K' )] / ( table [( 'Male' , '<=50K' )] + table [( 'Male' , '>50K' )]) * 100 , 2 ) table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } sex Female Male Female Male income-class <=50K >50K <=50K >50K >50K percent >50K percent occupation ? 789 52 863 139 6.18 13.87 Adm-clerical 2325 212 938 295 8.36 23.93 Armed-Forces 0 0 8 1 NaN 11.11 Craft-repair 202 20 2968 909 9.01 23.45 Exec-managerial 879 280 1219 1688 24.16 58.07 Farming-fishing 63 2 816 113 3.08 12.16 Handlers-cleaners 160 4 1124 82 2.44 6.80 Machine-op-inspct 530 20 1222 230 3.64 15.84 Other-service 1749 51 1409 86 2.83 5.75 Priv-house-serv 140 1 8 0 0.71 0.00 Prof-specialty 1130 385 1151 1474 25.41 56.15 Protective-serv 66 10 372 201 13.16 35.08 Sales 1175 88 1492 895 6.97 37.49 Tech-support 303 45 342 238 12.93 41.03 Transport-moving 81 9 1196 311 10.00 20.64 Observation from above: Gender seems to have a big influence on income. On almost all occupations, male have higher income than female. For example, only 8.36% female make more than 50K as adm-clerical whereas 23.93% male make more than 50K in the same occupation. # Investigate the correlation between occupation and income-class given different races table = pd . crosstab ( df [ 'occupation' ], [ df [ 'race' ], df [ 'income-class' ]]) # calculate percentages table [( 'White' , '>50K percent' )] = round ( table [( 'White' , '>50K' )] / ( table [( 'White' , '<=50K' )] + table [( 'White' , '>50K' )]) * 100 , 2 ) table [( 'Other' , '>50K percent' )] = round ( table [( 'Other' , '>50K' )] / ( table [( 'Other' , '<=50K' )] + table [( 'Other' , '>50K' )]) * 100 , 2 ) table [( 'Black' , '>50K percent' )] = round ( table [( 'Black' , '>50K' )] / ( table [( 'Black' , '<=50K' )] + table [( 'Black' , '>50K' )]) * 100 , 2 ) table [( 'Asian-Pac-Islander' , '>50K percent' )] = round ( table [( 'Asian-Pac-Islander' , '>50K' )] / ( table [( 'Asian-Pac-Islander' , '<=50K' )] + table [( 'Asian-Pac-Islander' , '>50K' )]) * 100 , 2 ) table [( 'Amer-Indian-Eskimo' , '>50K percent' )] = round ( table [( 'Amer-Indian-Eskimo' , '>50K' )] / ( table [( 'Amer-Indian-Eskimo' , '<=50K' )] + table [( 'Amer-Indian-Eskimo' , '>50K' )]) * 100 , 2 ) percent_cols = [ col for col in table . columns if 'percent' in col [ 1 ]] table [ percent_cols ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } race White Other Black Asian-Pac-Islander Amer-Indian-Eskimo income-class >50K percent >50K percent >50K percent >50K percent >50K percent occupation ? 11.42 8.70 4.19 7.69 8.00 Adm-clerical 14.23 3.85 8.57 15.83 9.68 Armed-Forces 14.29 NaN 0.00 NaN 0.00 Craft-repair 22.85 17.86 20.08 28.09 13.64 Exec-managerial 49.86 18.18 34.43 45.19 10.00 Farming-fishing 12.35 0.00 0.00 12.50 0.00 Handlers-cleaners 6.53 0.00 6.15 4.35 0.00 Machine-op-inspct 13.47 2.56 8.03 16.95 0.00 Other-service 4.00 0.00 2.98 13.28 6.06 Priv-house-serv 0.88 0.00 0.00 0.00 NaN Prof-specialty 46.07 29.03 27.62 48.92 33.33 Protective-serv 34.30 20.00 24.51 33.33 25.00 Sales 28.54 12.00 12.20 19.44 15.38 Tech-support 32.01 0.00 18.31 27.27 0.00 Transport-moving 21.62 7.14 10.59 14.29 12.00 Observation from above: Race seems to have an influence on income. White and Asian-Pac-Islanders have higher income than other races on almost all occupations. # Investigate the correlation between education-num and race table = pd . crosstab ( df [ 'education-num' ], df [ 'race' ]) table .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } race Amer-Indian-Eskimo Asian-Pac-Islander Black Other White education-num 1 0 6 5 2 38 2 4 5 16 9 134 3 2 18 21 13 278 4 9 11 56 16 551 5 5 9 82 7 385 6 15 13 124 7 634 7 12 20 138 8 817 8 5 9 69 13 300 9 119 225 1171 78 8877 10 79 206 743 51 6195 11 19 38 112 6 1206 12 8 29 106 8 915 13 21 286 329 33 4645 14 5 88 85 7 1520 15 2 36 15 2 475 16 3 28 11 2 357 Observation from above: Asians have an unusually high number in ed level 13 (Bachelor).","title":"Betweeen Categorical Variables"},{"location":"Documentation/Code%20Documentation/eda/#betweeen-categorical-variables-and-numeric-variables","text":"# Hours-per-week vs income-class # seperate high income group and low income group low_df = df . loc [ df [ 'income-class' ] == '<=50K' , 'hours-per-week' ] high_df = df . loc [ df [ 'income-class' ] == '>50K' , 'hours-per-week' ] sns . distplot ( low_df , kde = False , color = 'y' , label = '<=50K' ) sns . distplot ( high_df , kde = False , color = 'k' , label = '>50K' ) plt . legend () <matplotlib.legend.Legend at 0x12928fa20> Observation from above: In high income group, the proportion of people who work more than 40 hours is higher than low income group. # Hours-per-week vs ge # seperate high income group and low income group low_df = df . loc [ df [ 'income-class' ] == '<=50K' , 'age' ] high_df = df . loc [ df [ 'income-class' ] == '>50K' , 'age' ] sns . distplot ( low_df , kde = False , color = 'y' , label = '<=50K' ) sns . distplot ( high_df , kde = False , color = 'k' , label = '>50K' ) plt . legend () <matplotlib.legend.Legend at 0x1293727f0> low_income = df . loc [ df [ 'income-class' ] == '<=50K' , [ 'age' , 'hours-per-week' ]] high_income = df . loc [ df [ 'income-class' ] == '>50K' , [ 'age' , 'hours-per-week' ]] print ( 'low income' ) print ( low_income . describe (), ' \\n ' ) print ( 'high income' ) print ( high_income . describe ()) low income age hours-per-week count 24325.000000 24325.000000 mean 37.104995 39.123947 std 13.903084 12.147302 min 18.000000 1.000000 25% 26.000000 36.000000 50% 34.000000 40.000000 75% 46.000000 40.000000 max 90.000000 99.000000 high income age hours-per-week count 7682.000000 7682.000000 mean 44.206196 45.383494 std 10.507283 10.964122 min 19.000000 1.000000 25% 36.000000 40.000000 50% 43.000000 40.000000 75% 51.000000 50.000000 max 90.000000 99.000000 Observation from above: The median age and average age in high income group are both higher than low income group.","title":"Betweeen Categorical Variables and Numeric Variables"},{"location":"Documentation/Code%20Documentation/eda/#quantitative-analysis","text":"","title":"Quantitative Analysis"},{"location":"Documentation/Code%20Documentation/eda/#between-categorical-variables","text":"# make a copy of the processed data copy_df = df . copy () types = copy_df . dtypes types age int64 workclass object fnlwgt int64 education-num int64 status object occupation object relationship object race object sex object hours-per-week int64 income-class object capital-gain-zero int64 capital-loss-zero int64 is_USA int64 no_pay_or_work int64 dtype: object # to do quantitative anlaysis, we need to transform non-numeric variables into numeric ones oe = OrdinalEncoder () obj_cols = types [ types == 'object' ] . index . tolist () obj_cols . remove ( 'income-class' ) print ( obj_cols ) copy_df [ obj_cols ] = oe . fit_transform ( copy_df [ obj_cols ]) le = LabelEncoder () copy_df [ 'income-class' ] = le . fit_transform ( copy_df [ 'income-class' ]) ['workclass', 'status', 'occupation', 'relationship', 'race', 'sex'] # keep a record to map the original value to the encoded value oe . categories_ [array(['?', 'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc', 'Self-emp-not-inc', 'State-gov', 'Without-pay'], dtype=object), array(['Divorced', 'Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'], dtype=object), array(['?', 'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial', 'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct', 'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv', 'Sales', 'Tech-support', 'Transport-moving'], dtype=object), array(['Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried', 'Wife'], dtype=object), array(['Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'], dtype=object), array(['Female', 'Male'], dtype=object)] copy_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work 0 39 7.0 77516 13 4.0 1.0 1.0 4.0 1.0 40 0 0 1 1 0 1 50 6.0 83311 13 2.0 4.0 0.0 4.0 1.0 13 0 1 1 1 0 2 38 4.0 215646 9 0.0 6.0 1.0 4.0 1.0 40 0 1 1 1 0 3 53 4.0 234721 7 2.0 6.0 0.0 2.0 1.0 40 0 1 1 1 0 4 28 4.0 338409 13 2.0 10.0 5.0 2.0 0.0 40 0 1 1 0 0 # calculate chi2 score between each categorical variable and income-class cat_cols = obj_cols + [ 'education-num' , 'capital-gain-zero' , 'capital-loss-zero' , 'is_USA' ] print ( cat_cols ) chi2 , pval = chi2 ( copy_df [ cat_cols ], copy_df [ 'income-class' ]) ['workclass', 'status', 'occupation', 'relationship', 'race', 'sex', 'education-num', 'capital-gain-zero', 'capital-loss-zero', 'is_USA'] for i , chi2_score in enumerate ( chi2 ): print ( ' %s : %f ' % ( cat_cols [ i ], chi2_score )) workclass: 47.508119 status: 1123.469818 occupation: 504.558854 relationship: 3659.143125 race: 33.031305 sex: 502.439419 education-num: 2401.421777 capital-gain-zero: 192.123998 capital-loss-zero: 29.218864 is_USA: 4.029206 for i , pv in enumerate ( pval ): print ( ' %s : %f ' % ( cat_cols [ i ], pv )) workclass: 0.000000 status: 0.000000 occupation: 0.000000 relationship: 0.000000 race: 0.000000 sex: 0.000000 education-num: 0.000000 capital-gain-zero: 0.000000 capital-loss-zero: 0.000000 is_USA: 0.044719 # calculate mutual information score between each categorical variable and income-class mi = mutual_info_classif ( copy_df [ cat_cols ], copy_df [ 'income-class' ]) for i , v in enumerate ( mi ): print ( ' %s : %f ' % ( cat_cols [ i ], v )) workclass: 0.018099 status: 0.111759 occupation: 0.060578 relationship: 0.116259 race: 0.011263 sex: 0.026260 education-num: 0.067128 capital-gain-zero: 0.034813 capital-loss-zero: 0.011882 is_USA: 0.007318 Observation from above: Chi2 score shows that all categorical variables are dependent with income-class with a significance level set at 0.05. Among them, is_USA is the least dependent one. Relationship, education-num and status are suggested to be the most relevant variables by both chi2 score and MI score.","title":"Between Categorical Variables"},{"location":"Documentation/Code%20Documentation/eda/#between-categorical-variables-and-numeric-variables","text":"# do student T-test on numeric variables and income-class num_cols = [ 'age' , 'fnlwgt' , 'hours-per-week' ] high_income_df = copy_df . loc [ copy_df [ 'income-class' ] == 1 , num_cols ] low_income_df = copy_df . loc [ copy_df [ 'income-class' ] == 0 , num_cols ] # check if the samples in high income group and low income group have equal mean, and equal (close) variance mean_df = pd . concat ([ high_income_df . mean (), low_income_df . mean ()], axis = 1 ) \\ . rename ( columns = { 0 : 'high_income' , 1 : 'low_income' }) # N-1 to calculate variance by default var_df = pd . concat ([ high_income_df . var (), low_income_df . var ()], axis = 1 ) \\ . rename ( columns = { 0 : 'high_income' , 1 : 'low_income' }) mean_df # unequal mean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } high_income low_income age 44.249841 36.783738 fnlwgt 188005.000000 190340.865170 hours-per-week 45.473026 38.840210 var_df # 1/2 < var1/var2 < 2, so very close variance .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } high_income low_income age 1.106499e+02 1.965629e+02 fnlwgt 1.051482e+10 1.133847e+10 hours-per-week 1.212855e+02 1.517576e+02 # As we have unequal mean and almost equal variance in two samples for each numeric variables, # we can use the default settings in scipy ttest_ind() for col in num_cols : print ( col ) t_stat , pval = ttest_ind ( high_income_df [ col ], low_income_df [ col ]) print ( 't statistic: %f ' % t_stat ) print ( 'p value: %f ' % pval , ' \\n ' ) age t statistic: 43.436244 p value: 0.000000 fnlwgt t statistic: -1.707511 p value: 0.087737 hours-per-week t statistic: 42.583873 p value: 0.000000 Observation from above: Student T-test shows both age and hours-per-week are dependent with income-class with a significance level set at 0.05. This aligns with our findings in qualitative analysis. fnlwgt appears to be independent so we will remove it.","title":"Between Categorical Variables and Numeric Variables"},{"location":"Documentation/Code%20Documentation/eda/#between-numeric-variables","text":"# correlation between numeric features sns . pairplot ( copy_df [ num_cols ]) <seaborn.axisgrid.PairGrid at 0x1308d4860> copy_df [ num_cols ] . corr () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age fnlwgt hours-per-week age 1.000000 -0.079375 0.039007 fnlwgt -0.079375 1.000000 -0.021169 hours-per-week 0.039007 -0.021169 1.000000 Observation from above: No pair of numeric variables have strong correlations.","title":"Between Numeric Variables"},{"location":"Documentation/Code%20Documentation/eda/#feature-engineering","text":"Part 1: Feature selection - based on the findings in correlation analysis - based on additional criteria Part 2: Skewnewss Part 3: Encoding","title":"Feature Engineering"},{"location":"Documentation/Code%20Documentation/eda/#feature-selection","text":"# remove fnlwgt copy_df . drop ( 'fnlwgt' , axis = 1 , inplace = True ) # create a dummy variable to indicate if a person's education level is above bachelor degree copy_df [ 'bachelor_above' ] = 0 copy_df . loc [ copy_df [ 'education-num' ] >= 13 , 'bachelor_above' ] = 1 copy_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education-num status occupation relationship race sex hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work bachelor_above 0 39 7.0 13 4.0 1.0 1.0 4.0 1.0 40 0 0 1 1 0 1 1 50 6.0 13 2.0 4.0 0.0 4.0 1.0 13 0 1 1 1 0 1 2 38 4.0 9 0.0 6.0 1.0 4.0 1.0 40 0 1 1 1 0 0 3 53 4.0 7 2.0 6.0 0.0 2.0 1.0 40 0 1 1 1 0 0 4 28 4.0 13 2.0 10.0 5.0 2.0 0.0 40 0 1 1 0 0 1 # remove variables where the variace is below a certain threshold print ( 'shape before: %s ' % str ( copy_df . shape )) selector = VarianceThreshold () features = copy_df . drop ( 'income-class' , axis = 1 ) . columns copy_df [ features ] = selector . fit_transform ( copy_df [ features ]) print ( 'shape after: %s ' % str ( copy_df . shape )) shape before: (32561, 15) shape after: (32561, 15)","title":"Feature Selection"},{"location":"Documentation/Code%20Documentation/eda/#skewness","text":"# distribution of age is very skewed sns . distplot ( copy_df [ 'age' ], kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a108c18> sns . distplot ( np . log ( copy_df [ 'age' ]), kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a28a9e8> # skewness is reduced by applying log transform even though it's still not a perfect bell shape print ( copy_df [ 'age' ] . skew ()) print ( np . log ( copy_df [ 'age' ]) . skew ()) 0.5587433694130484 -0.1317299194198282 # apply log transform to age copy_df [ 'age' ] = np . log ( copy_df [ 'age' ]) # distribution of hours-per-week is not as skewed sns . distplot ( copy_df [ 'hours-per-week' ], kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x12a24ed30> copy_df [ 'hours-per-week' ] . skew () 0.22764253680450092 # save a checkpoint # ordinal encoded, imbalanced #copy_df.to_csv('adult_oe_im.csv', index=False)","title":"Skewness"},{"location":"Documentation/Code%20Documentation/eda/#encoding","text":"# Apply one hot encoding for modeling purposes ohe_cols = [ 'workclass' , 'status' , 'occupation' , 'relationship' , 'race' , 'sex' ] ohe_df = pd . get_dummies ( data = copy_df , columns = ohe_cols ) ohe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age education-num hours-per-week income-class capital-gain-zero capital-loss-zero is_USA no_pay_or_work bachelor_above workclass_0.0 ... relationship_3.0 relationship_4.0 relationship_5.0 race_0.0 race_1.0 race_2.0 race_3.0 race_4.0 sex_0.0 sex_1.0 0 3.663562 13.0 40.0 0 0.0 1.0 1.0 0.0 1.0 0 ... 0 0 0 0 0 0 0 1 0 1 1 3.912023 13.0 13.0 0 1.0 1.0 1.0 0.0 1.0 0 ... 0 0 0 0 0 0 0 1 0 1 2 3.637586 9.0 40.0 0 1.0 1.0 1.0 0.0 0.0 0 ... 0 0 0 0 0 0 0 1 0 1 3 3.970292 7.0 40.0 0 1.0 1.0 1.0 0.0 0.0 0 ... 0 0 0 0 0 1 0 0 0 1 4 3.332205 13.0 40.0 0 1.0 1.0 0.0 0.0 1.0 0 ... 0 0 1 0 0 1 0 0 1 0 5 rows \u00d7 53 columns # save a checkpoint # one hot encoded, imbalanced #ohe_df.to_csv('adult_ohe_im.csv', index=False)","title":"Encoding"},{"location":"Documentation/Product%20Documentation/product/","text":"The purpose of the demo project is to leverage the best practices of model development in Data Science community and speed up the developing process for Braskem DS team by demonstrating an array of methods and tools that are widely used in industry. This project will be documented in MKDocs as an effort to build up a documentation guideline for future factory projects. This project will also use Azure DevOps Repos and Git as an effort to practice version control. Will aid in the Playbook project and support MLOps vision / approach for Digital Factory.","title":"Product"},{"location":"Documentation/Research%20Documentation/research/","text":"Microsoft AutoML Documentations: https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#preprocess https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup","title":"Research"},{"location":"glossary/dictionary/","text":"Glossary \u00b6 Last updated: 12 / May / 2020 Table of Contents \u00b6 Accuracy Active Learning Anomaly detection Artificial Neural Networks (ANN) Backtesting Bagging Bag of words Batch Gradient Descent Bayes Theorem Batch Learning Big Data Binomial distribution Bootstrapping Bregman divergence Central Limit Theorem Co-Variance Confusion Matrix Contingency Table Correlation analysis Correlation analysis, Canonical Correlation analysis - Matthews Correlation Coefficient (MCC) Correlation analysis - Kendall Correlation analysis - Pearson Correlation analysis - Spearman Cosine Similarity Cost function Cross-validation Cross-validation, K-fold Cross-validation, Leave-One-Out Cross-validation, Random Sampling Curse of dimensionality Data mining DBSCAN Decision rule Decision tree classifier Density-based clustering Descriptive modeling Dimensionality reduction Distance Metric Learning Distance, Euclidean Distance, Manhattan Distance, Minkowski Eager learners Eigenvectors and Eigenvalues Ensemble methods Evolutionary algorithms Exhaustive search Expectation Maximization algorithm - EM Feature Selection Feature Space Fuzzy C-Means Clustering Generalization error Genetic algorithm Gradient Descent Greedy Algorithm Heuristic search Hyperparameters iid Imputation Independent Component Analysis Jaccard coefficient Jackknifing Jittering k-D Trees Kernel Density Estimation Kernel (in statistics) Kernel Methods Kernel Trick k-fold Cross-validation K-Means Clustering K-Means++ Clustering K-Medoids Clustering K-nearest neighbors algorithms Knowledge Discovery in Databases (KDD) LASSO Regression Latent Semantic Indexing Law of Large Numbers Lazy learners Least Squares fit Lennard-Jones Potential Linear Discriminant Analysis Linear Discriminant Analysis (LDA) Local Outlier Factor (LOF) Locality-sensitive hashing (LSH) Logistic Regression Machine learning Mahalanobis distance MapReduce Markov chains Monte Carlo simulation Maximum Likelihood Estimates (MLE) Min-Max scaling MinHash Naive Bayes Classifier N-grams Non-parametric statistics Normal distribution (multivariate) Normal distribution (univariate) Normal Modes Normalization - Min-Max scaling Normalization - Standard Scores Objective function On-Line Learning On-Line Analytical Processing (OLAP) Parzen-Rosenblatt Window technique Pattern classification Perceptron Permissive transformations Poisson distribution (univariate) Population mean Power transform Principal Component Analysis (PCA) Precision and Recall Predictive Modeling Proportion of Variance Explained Purity Measure Quantitative and qualitative attributes R-factor Random forest Rayleigh distribution (univariate) Receiver Operating Characteristic (ROC) Regularization Reinforcement learning Rejection sampling Resubstitution error Ridge Regression Rule-based classifier Sampling Sensitivity Sharding Silhouette Measure (clustering) Simple Matching Coefficient Singular Value Decomposition (SVD) Soft classification Specificity Standard deviation Stochastic Gradient Descent (SGD) Supervised Learning Support Vector Machine Term frequency and document frequency Term frequency - inverse document frequency, Tf-idf Tokenization Unsupervised Learning Variance White noise Whitening transformation Z-score A \u00b6 Accuracy \u00b6 [ back to top ] Accuracy is defined as the fraction of correct classifications out of the total number of samples; it resembles one way to assess the performance of a predictor and is often used synonymous to specificity / precision although it is calculated differently. Accuracy is calculated as: \\frac{True Positives + True Negatives}{Positives+Negatives} \\frac{True Positives + True Negatives}{Positives+Negatives} Source: wikipedia Active Learning \u00b6 [ back to top ] Active learning is a variant of the on-line learning machine learning architecture where feedback about the ground truth class labels of unseen data can be requested if the classification is uncertain. New training data that was labeled can then be used to update the model as in on-line learning . Anomaly detection \u00b6 [ back to top ] Anomaly detection describes the task of identifying points that deviate from specific patterns in a dataset -- the so-called outliers. Different types of anomaly detection methods include graph-based, statistical-based and distance-based techniques and can be used in both unsupervised and supervised learning tasks. Artificial Neural Networks (ANN) \u00b6 [ back to top ] Artificial Neural Networks (ANN) are a class of machine learning algorithms that are inspired by the neuron architecture of the human brain. Typically, a (multi-layer) ANN consists of a layer of input nodes, a layer of output nodes, and hidden layers in-between. The nodes are connected by weighted links that can be interpreted as the neuron-connections by axons of different strengths. The simplest version of an ANN is a single-layer perceptron . B \u00b6 Backtesting \u00b6 [ back to top ] Backtesting is a specific case of cross-validation in the context of finance and trading models where empirical data from previous time periods (data from the past) is used to evaluate a trading strategy. Bagging \u00b6 [ back to top ] Bagging is an ensemble method for classification (or regression analysis) in which individual models are trained by random sampling of data, and the final decision is made by voting among individual models with equal weights (or averaging for regression analysis). Bag of words \u00b6 [ back to top ] Bag of words is a model that is used to construct sparse feature vectors for text classification tasks. The bag of words is an unordered set of all words that occur in all documents that are part of the training set. Every word is then associated with a count of how often it occurs whereas the positional information is ignored. Sometimes, the bag of words is also called \"dictionary\" or \"vocabulary\" based on the training data. Batch Gradient Descent \u00b6 [ back to top ] Batch Gradient descent is a variant of a Gradient Descent algorithm to optimize a function by finding its local minimum. In contrast to Stochastic Gradient Descent the gradient is computed from the whole dataset. Bayes Theorem \u00b6 [ back to top ] Naive Bayes' classifier: posterior probability: P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} decision rule: \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} objective functions: g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) Batch Learning \u00b6 [ back to top ] Batch learning is an architecture used in machine learning tasks where the entire training dataset is available upfront to build the model. In contrast to on-line learning , the model is not updated once it was build on a training dataset. Big Data \u00b6 [ back to top ] There are many different, controversial interpretations and definitions for the term \"Big Data\". Typically, one refers to data as \"Big Data\" if its volume and complexity are of a magnitude that the data cannot be processed by \"conventional\" computing platforms anymore; storage space, processing power, and database structures are typically among the limiting factors. Binomial distribution \u00b6 [ back to top ] Probability density function: p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} Bootstrapping \u00b6 [ back to top ] A resampling technique to that is closely related to cross-validation where a training dataset is divided into random subsets. Bootstrapping -- in contrast to cross-validation -- is a random sampling with replacement. Bootstrapping is typically used for statistical estimation of bias and standard error, and a common application in machine learning is to estimate the generalization error of a predictor. Bregman divergence \u00b6 [ back to top ] Bregman divergence describes are family of proximity functions (or distance measures) that share common properties and are often used in clustering algorithms. A popular example is the squared Euclidean distance. C \u00b6 Central Limit Theorem \u00b6 [ back to top ] The Central Limit Theorem is a theorem in the field of probability theory that expresses the idea that the distribution of sample means (from independent random variables) converges to a normal distribution when the sample size approaches infinity. Co-Variance \u00b6 [ back to top ] S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) example covariance matrix: \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} Confusion Matrix \u00b6 [ back to top ] The confusion matrix is used as a way to represent the performance of a classifier and is sometimes also called \"error matrix\". This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios. Contingency Table \u00b6 [ back to top ] A contingency table is used in clustering analysis to compare the overlap between two different clustering (grouping) results. The partitions from the two clustering results are represented as rows and columns in the table, and the individual elements of the table represent the number of elements that are shared between two partitions from each clustering result. Correlation analysis \u00b6 [ back to top ] Correlation analysis describes and quantifies the relationship between two independent variables. Typically, in case of a positive correlation both variables have a tendency to increase, and in the case of negative correlation, one variable increases while the other variable increases. It is important to mention the famous quotation \"correlation does not imply causation\". Correlation analysis, Canonical \u00b6 Let x and y be two vectors, the goal of canonical correlation analysis is to maximize the correlation between linear transformations of those original vectors. With applications in dimensionality reduction and feature selection, CCA tries to find common dimensions between two vectors. Correlation analysis - Matthews Correlation Coefficient (MCC) \u00b6 [ back to top ] MCC is an assessment metric for clustering or binary classification analyses that represents the correlation between the observed (ground truth) and predicted labels. MCC can be directly computed from the confusion matrix and returns a value between -1 and 1. Correlation analysis - Kendall \u00b6 [ back to top ] Similar to the Pearson correlation coefficient , Kendall's tau measures the degree of a monotone relationship between variables, and like Spearman's rho , it calculates the dependence between ranked variables, which makes it feasible for non-normal distributed data. Kendall tau can be calculated for continuous as well as ordinal data. Roughly speaking, Kendall's tau distinguishes itself from Spearman's rho by stronger penalization of non-sequential (in context of the ranked variables) dislocations. \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} where: c = the number of concordant pairs d = the number of discordant pairs [ Source ] If ties are present among the 2 ranked variables, the following equation shall be used instead: \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ where: t = number of observations of variable x that are tied u = number of observations of variable y that are tied Correlation analysis - Pearson \u00b6 [ back to top ] The Pearson correlation coefficient is probably the most widely used measure for linear relationships between two normal distributed variables and thus often just called \"correlation coefficient\". Usually, the Pearson coefficient is obtained via a Least-Squares fit and a value of 1 represents a perfect positive relation-ship, -1 a perfect negative relationship, and 0 indicates the absence of a relationship between variables. \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} And the estimate r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} Correlation analysis - Spearman \u00b6 [ back to top ] Related to the Pearson correlation coefficient , the Spearman correlation coefficient (rho) measures the relationship between two variables. Spearman's rho can be understood as a rank-based version of Pearson's correlation coefficient , which can be used for variables that are not normal-distributed and have a non-linear relationship. Also, its use is not only restricted to continuous data, but can also be used in analyses of ordinal attributes. \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} where: d = the pairwise distances of the ranks of the variables x i and y i . n = the number of samples. Cosine Similarity \u00b6 [ back to top ] Cosine similarity measures the orientation of two n -dimensional sample vectors irrespective to their magnitude. It is calculated by the dot product of two numeric vectors, and it is normalized by the product of the vector lengths, so that output values close to 1 indicate high similarity. cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} Cost function \u00b6 [ back to top ] A cost function (synonymous to loss function) is a special case of an objective function , i.e., a function that is used for solving optimization problems. A cost function can take one or more input variables and the output variable is to be minimized. A typical use case for cost functions is parameter optimization. Cross-validation \u00b6 [ back to top ] Cross-validation is a statistical technique to estimate the prediction error rate by splitting the data into training, cross-validation, and test datasets. A prediction model is obtained using the training set, and model parameters are optimized by the cross-validation set, while the test set is held primarily for empirical error estimation. Cross-validation, K-fold \u00b6 [ back to top ] K-fold cross-validation is a variant of cross validation where contiguous segments of samples are selected from the training dataset to build two new subsets for every iteration (without replacement): a new training and test dataset (while the original test dataset is retained for the final evaluation of the predictor). Cross-validation, Leave-One-Out \u00b6 [ back to top ] Leave-One-Out cross-validation a variant of cross validation one sample is removed for every iteration (without replacement). The model is trained on the remaining N-1 samples and evaluated via the removed sample (while the original test dataset is retained for the final evaluation of the predictor). Cross-validation, Random Sampling \u00b6 [ back to top ] Cross-validation via random sampling is a variant of cross validation where random chunks of samples are extracted from the training dataset to build two new subsets for every iteration (with or without replacement): a new training and test dataset for every iteration (while the original test dataset is retained for the final evaluation of the predictor). Curse of dimensionality \u00b6 [ back to top ] For a fixed number of training samples, the curse of dimensionality describes the increased error rate for a large number of dimensions (or features) due to imprecise parameter estimations. D \u00b6 Data mining \u00b6 [ back to top ] A field that is closely related to machine learning and pattern classification. The focus of data mining does not lie in merely the collection of data, but the extraction of useful information: Discovery of patterns, and making inferences and predictions. Common techniques in data mining include predictive modeling, clustering, association rules, and anomaly detection. DBSCAN \u00b6 [ back to top ] DBSCAN is a variant of a density-based clustering algorithm that identifies core points as regions of high-densities based on their number of neighbors (> MinPts ) in a specified radius (\u03b5). Points that are below MinPts but within \u03b5 are specified as border points; the remaining points are classified as noise points. Decision rule \u00b6 [ back to top ] A function in pattern classification tasks of making an \"action\", e.g., assigning a certain class label to an observation or pattern. Decision tree classifier \u00b6 [ back to top ] Decision tree classifiers are tree like graphs, where nodes in the graph test certain conditions on a particular set of features, and branches split the decision towards the leaf nodes. Leaves represent lowest level in the graph and determine the class labels. Optimal tree are trained by minimizing Gini impurity, or maximizing information gain. Density-based clustering \u00b6 [ back to top ] In density-based clustering, regions of high density in n-dimensional space are identified as clusters. The best advantage of this class of clustering algorithms is that they do not require apriori knowledge of number of clusters (as opposed to k-means algorithm). Descriptive modeling \u00b6 [ back to top ] Descriptive modeling is a common task in the field of data mining where a model is build in order to distinguish between objects and categorize them into classes - a form of data summary. In contrast to predictive modeling , the scope of descriptive modeling does not extend to making prediction for unseen objects. Dimensionality reduction \u00b6 [ back to top ] Dimensionality reduction is a data pre-processing step in machine learning applications that aims to avoid the curse of dimensionality and reduce the effect of overfitting. Dimensionality reduction is related to feature selection , but instead of selecting a feature subset, dimensionality reduction takes as projection-based approach (e.g, linear transformation) in order to create a new feature subspace. Distance Metric Learning \u00b6 [ back to top ] Distance metrics are fundamental for many machine learning algorithms. Distance metric learning - instead of learning a model - incorporates estimated relevances of features to obtain a distance metric for potentially optimal separation of classes and clusters: Large distances for objects from different classes, and small distances for objects of the same class, respectively. Distance, Euclidean \u00b6 [ back to top ] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras' theorem. The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension. \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} Distance, Manhattan \u00b6 [ back to top ] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance, but instead of calculating the shortest diagonal path (\"beeline\") between two points, it calculates the distance based on gridlines. The Manhattan distance was named after the block-like layout of the streets in Manhattan. \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} Distance, Minkowski \u00b6 [ back to top ] The Minkowski distance is a generalized form of the Euclidean distance (if p=2 ) and the Manhattan distance (if p=1 ). \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} E \u00b6 Eager learners \u00b6 [ back to top ] Eager learners (in contrast to lazy learners ) describe machine learning algorithms that learn a model for mapping attributes to class labels as soon as the data becomes available (e.g., Decision tree classifiers or naive Bayes classifiers ) and do not require the training data for making predictions on unseen samples once the model was built. The most computationally expensive step is the creation of a prediction model from the training data, and the actual prediction is considered as relatively inexpensive. Eigenvectors and Eigenvalues \u00b6 [ back to top ] Both eigenvectors and eigenvalues fundamental in many applications involve linear systems and are related via A\u00b7v = \u03bb\u00b7v (where A is a square matrix, v the eigenvector, and \u03bb the eigenvalue). Eigenvectors are describing the direction of the axes of a linear transformation, whereas eigenvalues are describing the scale or magnitude. \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ where: \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} Ensemble methods \u00b6 [ back to top ] Ensemble methods combine multiple classifiers which may differ in algorithms, input features, or input samples. Statistical analyses showed that ensemble methods yield better classification performances and are also less prone to overfitting. Different methods, e.g., bagging or boosting, are used to construct the final classification decision based on weighted votes. Evolutionary algorithms \u00b6 [ back to top ] Evolutionary algorithms are a class of algorithms that are based on heuristic search methods inspired by biological evolution in order to solve optimization problems. Exhaustive search \u00b6 [ back to top ] Exhaustive search (synonymous to brute-force search) is a problem-solving approach where all possible combinations are sequentially evaluated to find the optimal solution. Exhaustive search guarantees to find the optimal solution whereas other approaches (e.g., heuristic searches ) are regarded as sub-optimal. A downside of exhaustive searches is that computational costs increase proportional to the number of combinations to be evaluated. Expectation Maximization algorithm - EM \u00b6 [ back to top ] The Expectation Maximization algorithm (EM) is a technique to estimate parameters of a distribution based on the Maximum Likelihood Estimate (MLE) that is often used for the imputation of missing values in incomplete datasets. After the EM algorithm is initialized with a starting value, alternating iterations between expectation and maximization steps are repeated until convergence. In the expectation step, parameters are estimated based on the current model to impute missing values. In the maximization step, the log-likelihood function of the statistical model is to be maximized by re-estimating the parameters based on the imputed values from the expectation step. F \u00b6 Feature Selection \u00b6 [ back to top ] Feature selection is an important pre-processing step in many machine learning applications in order to avoid the curse of dimensionality and overfitting . A subset of features is typically selected by evaluating different combinations of features and eventually retain the subset that minimizes a specified cost function . Commonly used algorithms for feature selection as alternative to exhaustive search algorithms include sequential selection algorithms and genetic algorithms. Feature Space \u00b6 [ back to top ] A feature space describes the descriptive variables that are available for samples in a dataset as a d -dimensional Euclidean space. E.g., sepal length and width, and petal length and width for each flower sample in the popular Iris dataset. Fuzzy C-Means Clustering \u00b6 [ back to top ] Fuzzy C-Means is a soft clustering algorithm in which each sample point has a membership degree to each cluster; in hard (crisp) clustering, membership of each point to each cluster is either 0 or 1. Fuzzy C-Means considers a weight matrix for cluster memberships, and minimizes sum squared error (SSE) of weighted distances of sample points to the cluster centroids. G \u00b6 Generalization error \u00b6 [ back to top ] The generalization error describes how well new data can be classified and is a useful metric to assess the performance of a classifier. Typically, the generalization error is computed via cross-validation or simply the absolute difference between the error rate on the training and test dataset. Genetic algorithm \u00b6 [ back to top ] The Genetic algorithm is a subclass of evolutionary algorithms that takes a heuristic approach inspired by Charles Darwin's theory of \"natural selection\" in order to solve optimization problems. Gradient Descent \u00b6 [ back to top ] Gradient descent is an algorithm that optimizes a function by finding its local minimum. After the algorithm was initialized with an initial guess, it takes the derivative of the function to make a step towards the direction of deepest descent. This step-wise process is repeated until convergence. Greedy Algorithm \u00b6 [ back to top ] Greedy Algorithms are a family of algorithms that are used in optimization problems. A greedy algorithm makes locally optimal choices in order to find a local optimum (suboptimal solution, also see ( heuristic problem solving ). H \u00b6 Heuristic search \u00b6 [ back to top ] Heuristic search is a problem-solving approach that is focussed on efficiency rather than completeness in order to find a suboptimal solution to a problem. Heuristic search is often used as alternative approach where exhaustive search is too computationally intensive and where solutions need to be approximated. Hyperparameters \u00b6 [ back to top ] Hyperparameters are the parameters of a classifier or estimator that are not directly learned in the machine learning step from the training data but are optimized separately (e.g., via Grid Search ). The goal of hyperparameter optimization is to achieve good generalization of a learning algorithm and to avoid overfitting to the training data. I \u00b6 iid \u00b6 [ back to top ] The abbreviation \"iid\" stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another variable (e.g., time series and network graphs are not independent). One popular example of iid would be the tossing of a coin: One coin toss does not affect the outcome of another coin toss, and the probability of the coin landing on either \"heads\" or \"tails\" is the same for every coin toss. Imputation \u00b6 [ back to top ] Imputations algorithms are designed to replace the missing data (NAs) with certain statistics rather than discarding them for downstream analysis. Commonly used imputation methods include mean imputation (replacement by the sample mean of an attribute), kNN imputation, and regression imputation. Independent Component Analysis \u00b6 [ back to top ] Independent Component Analysis (ICA) is a statistical signal-processing technique that decomposes a multivariate dataset of mixed, non-gaussian distributed source signals into independent components. A popular example is the separation of overlapping voice samples -- the so-called \"cocktail party problem\". J \u00b6 Jaccard coefficient \u00b6 [ back to top ] The Jaccard coefficient (bounded at [0, 1)) is used as similarity measure for asymmetric binary data and calculated by taking the number of matching attributes and divide it by the number of all attributes except those where both variables have a value 0 in contrast to a simple matching coefficient . A popular application is the identification of near-duplicate documents for which the Jaccard coefficient can be calculated by the dividing the intersection of the set of words by the union of the set words in both documents. Jackknifing \u00b6 [ back to top ] Jackknifing is a resampling technique that predates the related cross-validation and bootstrapping techniques and is mostly used for bias and variance estimations. In jackknifing, a dataset is split into N subsets where exactly one sample is removed from every subset so that every subset is of size N-1. Jittering \u00b6 [ back to top ] Jittering is a sampling technique that can be used to measure the stability of a given statistical model (classifiction/regression/clustering). In jittering, some noise is added to sample data points, and then a new model is drawn and compared to the original model. K \u00b6 k-D Trees \u00b6 [ back to top ] k-D trees are a data structures (recursive space partitioning trees) that result from the binary partitioning of multi-dimensional feature spaces. A typical application of k-D trees is to increase the search efficiency for nearest-neighbor searches. A k-D tree construction can be described as a iterating process with the following steps: Select the dimension of largest variance, draw a cutting plane based at the median along the dimension to split the data into 2 halves, choose the next dimension. Kernel Density Estimation \u00b6 [ back to top ] Non-parametric techniques to estimate probability densities from the available data without requiring prior knowledge of the underlying model of the probability distribution. Kernel (in statistics) \u00b6 [ back to top ] In the context of [kernel methods](#kernel-methods the term \u201ckernel\u201d describes a function that calculates the dot product of the images of the samples x under the kernel function \u03c6 (see kernel methods). Roughly speaking, a kernel can be understood as a similarity measure in higher-dimensional space. Kernel Methods \u00b6 [ back to top ] Kernel methods are algorithms that map the sample vectors of a dataset onto a higher-dimensional feature space via a so-called kernel function (\u03c6(x)). The goal is to identify and simplify general relationships between data, which is especially useful for linearly non-separable datasets. Kernel Trick \u00b6 [ back to top ] Since the explicit computation of the kernel is increasingly computationally expensive for large sample sizes and high numbers of dimensions, the kernel trick uses approximations to calculate the kernel implicitly. The most popular kernels used for the kernel trick are Gaussian Radius Basis Function (RBF) kernels, sigmoidal kernels, and polynomial kernels. k-fold Cross-validation \u00b6 [ back to top ] In k-fold cross-validation the data is split into k subsets, then a prediction/classification model is trained k times, each time holding one subset as test set, training the model parameters using the remaining k -1. Finally, cross-validation error is evaluated as the average error out of all k training models. K-Means Clustering \u00b6 [ back to top ] A method of partitioning a dataset into k clusters by picking k random initial points (where k < n , the number or total points - modified by S.R. ), assigning clusters, averaging, reassigning, and repeating until stability is achieved. The number k must be chosen beforehand. K-Means++ Clustering \u00b6 [ back to top ] A variant of k-means where instead of choosing all initial centers randomly, the first is chosen randomly, the second chosen with probability proportional to the squared distance from the first, the third chosen with probability proportional to the square distance from the first two, etc. See this paper . K-Medoids Clustering \u00b6 [ back to top ] K-Medoids clustering is a variant of k-means algorithm in which cluster centroids are picked among the sample points rather than the mean point of each cluster. K-Medoids can overcome some of the limitations of k-means algorithm by avoiding empty clusters, being more robust to outliers, and being more easily applicable to non-numeric data types. K-nearest neighbors algorithms \u00b6 [ back to top ] K-nearest neighbors algorithms find the k-points that are closest to a point of interest based on their attributes using a certain distance measure (e.g., Euclidean distance). K-nearest neighbors algorithms are being used in many different contexts: Non-parametric density estimation, missing value imputation, dimensionality reduction, and classifiers in supervised and unsupervised pattern classification and regression problems. Knowledge Discovery in Databases (KDD) \u00b6 [ back to top ] Knowledge Discovery in Databases (KDD) describes a popular workflow including data mining for extracting useful and meaningful information out of data. Typically, the individual steps are feature selection, pre-processing, transformation, data mining , and post-processing (evaluation and interpretation). L \u00b6 LASSO Regression \u00b6 [ back to top ] LASSO (Least Absolute Shrinkage and Selection Operator) is a regression model that uses the L1-norm (sum of absolute values) of model coefficients to penalize the model complexity. LASSO has the advantage that some coefficients can become zero, as opposed to ridge regression that uses the squared sum of model coefficients. Latent Semantic Indexing \u00b6 [ back to top ] Latent Semantic Indexing (LSI) is a data mining technique to characterize documents by topics, word usage, or other contexts. The structures of the documents are compared by applying singular value decomposition to an input term-document matrix (e.g., a data table of word counts with terms as row labels and document numbers as column labels) in order to obtain the singular values and vectors. Law of Large Numbers \u00b6 [ back to top ] The Law of Large Numbers is a theorem in the field of probability theory that expresses the idea that the actual value of a random sampling process approaches the expected value for growing sample sizes. A common example is that the observed ratio of \"heads\" in an unbiased coin-flip experiment will approach 0.5 for large sample sizes. Lazy learners \u00b6 [ back to top ] Lazy learners (in contrast to eager learners ) are memorizing training data in order to make predictions for unseen samples. While there is no expensive learning step involved, the prediction step is generally considered to be more expensive compared to eager learners since it involves the evaluation of training data. One example of lazy learners are k-nearest neighbor algorithms where the class label of a unseen sample is estimated by e.g., the majority of class labels of its neighbors in the training data. Least Squares fit \u00b6 [ back to top ] A linear regression technique that fits a straight line to a data set (or overdetermined system) by minimizing the sum of the squared residuals, which can be the minimized vertical or perpendicular offsets from the fitted line. Linear equation f(x) = a\\cdot x + b f(x) = a\\cdot x + b f(x) = a\\cdot x + b Slope: a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad Y-axis intercept: b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad where: S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} Matrix equation \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y Lennard-Jones Potential \u00b6 [ back to top ] The Lennard-Jones potential describes the energy potential between two non-bonded atoms based on their distance to each other. V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = intermolecular potential \u03c3 = distance where V is 0 r = distance between atoms, measured from one center to the other \u03b5 = interaction strength Linear Discriminant Analysis \u00b6 [ back to top ] In-between class scatter matrix S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i Where: S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ and \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} Between class scatter matrix S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T Linear Discriminant Analysis (LDA) \u00b6 [ back to top ] A linear transformation technique (related to Principal Component Analysis) that is commonly used to project a dataset onto a new feature space or feature subspace, where the new component axes maximize the spread between multiple classes, or for classification of data. Local Outlier Factor (LOF) \u00b6 [ back to top ] LOF is a density-based anomaly detection technique for outlier identification. The LOF for a point p refers to the average \"reachability distance\" towards its nearest neighbors. Eventually, the points with the largest LOF values (given a particular threshold) are identified as outliers. Locality-sensitive hashing (LSH) \u00b6 [ back to top ] Locality-sensitive hashing (LSH) is a dimensionality reduction technique that groups objects that are likely similar (based on a similarity signature such as MinHash ) into the same buckets in order to reduce the search space for pair-wise similarity comparisons. One application of LSH could be a combination with other dimensionality reduction techniques, e.g., MinHash , in order to reduce the computational costs of finding near-duplicate document pairs. Logistic Regression \u00b6 [ back to top ] Logistic regression is a statistical model used for binary classification (binomial logistic regression) where class labels are mapped to \"0\" or \"1\" outputs. Logistic regression uses the logistic function (a general form of sigmoid function), where its output ranges from (0-1). M \u00b6 Machine learning \u00b6 [ back to top ] A set of algorithmic instructions for discovering and learning patterns from data e.g., to train a classifier for a pattern classification task. Mahalanobis distance \u00b6 [ back to top ] The Mahalanobis distance measure accounts for the covariance among variables by calculating the distance between a sample x and the sample mean \u03bc in units of the standard deviation. The Mahalanobis distance becomes equal to the Euclidean distance for uncorrelated with same variances. MapReduce \u00b6 [ back to top ] MapRedcue is a programming model for analyzing large datasets on distributed computer clusters, in which the task is divided into two steps, a map step and a reducer step. In the map step, the data are filtered by some factors on each compute node, then filtered data are shuffled and passed to the reducer function which performs further analysis on each portion of filtered data separately. Markov chains \u00b6 [ back to top ] Markov chains (names after Andrey Markov) are mathematical systems that describe the transitioning between different states in a model. The transitioning from one state to the other (or back to itself) is a stochastic process. Monte Carlo simulation \u00b6 [ back to top ] A Monte Carlo simulation is an iterative sampling method for solving deterministic models. Random numbers or variables from a particular probability distribution are used as input variables for uncertain parameters to compute the response variables. Maximum Likelihood Estimates (MLE) \u00b6 [ back to top ] A technique to estimate the parameters that have been fit to a model by maximizing a known likelihood function. One common application is the estimation of \"mean\" and \"variance\" for a Gaussian distribution. D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} can be pictured as probability to observe a particular sequence of patterns, where the probability of observing a particular patterns depends on \u03b8 , the parameters the underlying (class-conditional) distribution. In order to apply MLE, we have to make the assumption that the samples are i.i.d. (independent and identically distributed). p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) Where \u03b8 is the parameter vector, that contains the parameters for a particular distribution that we want to estimate and p(D | \u03b8 ) is also called the likelihood of \u03b8 . log-likelihood p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) Differentiation \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} parameter vector \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] Min-Max scaling \u00b6 [ back to top ] X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} MinHash \u00b6 [ back to top ] MinHash is a commonly used technique for dimensionality reduction in document similarity comparisons. The idea behind MinHash is to create a signature of reduced dimensionality while preserving the Jaccard similarity coefficient . A common implementation of MinHash is to generate k random permutations of the columns in a m*x*n -document matrix (rows represent the sparse vectors of words for each document as binary data) and generate a new matrix of size m*x*k . The cells of the new matrix now contain the position labels of the first non-zero value for every document (1 column for each round of random permutation). Based on similarities of the position labels, the Jaccard coefficient for the pairs of documents can be calculated. N \u00b6 Naive Bayes Classifier \u00b6 [ back to top ] A classifier based on a statistical model (i.e., Bayes theorem: calculating posterior probabilities based on the prior probability and the so-called likelihood) in the field of pattern classification. Naive Bayes assumes that all attributes are conditionally independent, thereby, computing the likelihood is simplified to the product of the conditional probabilities of observing individual attributes given a particular class label. N-grams \u00b6 [ back to top ] In context of natural language processing (NLP), a text is typically broken down into individual elements (see tokenization ). N-grams describe the length of the individual elements where n refers to the number of words or symbols in every token. E.g., a unigram (or 1-gram) can represent a single word, and a bigram (or 2-gram) describes a token that consists of 2 words etc. Non-parametric statistics \u00b6 [ back to top ] In contrast to parametric approaches, non-parametric statistics or approaches do not make prior assumptions about the underlying probability distribution of a particular variable or attribute. Normal distribution (multivariate) \u00b6 [ back to top ] Probability density function p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] Normal distribution (univariate) \u00b6 [ back to top ] Probability density function p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } Normal Modes \u00b6 [ back to top ] Normal modes are the harmonic oscillations of a system of masses connected by springs, or roughly speaking \"concerted motions,\" and all normal modes of a system are independent from each other. A classic example describes two masses connected by a middle spring, and each mass is connected to a fixed outer edge ( | m1~~m2 |). The oscillation of this system where the middle spring does not move is defined as its normal mode. Normalization - Min-Max scaling \u00b6 [ back to top ] A data pre-processing step (also often referred to as \"Feature Scaling\") for fitting features from different measurements within a certain range, typically the unit range from 0 to 1. Normalization - Standard Scores \u00b6 [ back to top ] A data pre-processing step (also often just called \"Standardization\") for re-scaling features from different measurements to match proportions of a standard normal distribution (unit variance centered at mean=0). O \u00b6 Objective function \u00b6 [ back to top ] Objective functions are mathematical function that are used for problem-solving and optimization tasks. Depending on the task, the objective function can be omtpimized through minimization ( cost or loss functions ) or maximization (reward function). A typical application of an objective function in pattern classification tasks is to minimize the error rate of a classifier. On-Line Learning \u00b6 [ back to top ] On-line learning is a machine learning architecture where the model is being updated consecutively as new training data arrives in contrast to batch-learning , which requires the entire training dataset to be available upfront. On-line has the advantage that a model can be updated and refined over time to account for changes in the population of training samples. A popular example where on-line learning is beneficial is the task of spam detection. On-Line Analytical Processing (OLAP) \u00b6 [ back to top ] On-Line Analytical Processing (OLAP) describes the general process of working with multidimensional arrays for exploratory analysis and information retrieval; often, OLAP is used to create summary data, e.g., via data aggregation across multiple dimensions or columns. P \u00b6 Parzen-Rosenblatt Window technique \u00b6 [ back to top ] A non-parametric kernel density estimation technique for probability densities of random variables if the underlying distribution/model is unknown. A so-called window function is used to count samples within hypercubes or Gaussian kernels of a specified volume to estimate the probability density. \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} for a hypercube of unit length 1 centered at the coordinate system's origin. What this function basically does is assigning a value 1 to a sample point if it lies within \u00bd of the edges of the hypercube, and 0 if lies outside (note that the evaluation is done for all dimensions of the sample point). If we extend on this concept, we can define a more general equation that applies to hypercubes of any length h n that are centered at x : k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ where: \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) probability density estimation with hypercube kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] where: h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k probability density estimation with Gaussian kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] Pattern classification \u00b6 [ back to top ] The usage of patterns in datasets to discriminate between classes, i.e., to assign a class label to a new observation based on inference. Perceptron \u00b6 [ back to top ] A (single-layer) perceptron is a simple Artificial Neural Network algorithm that consists of only two types of nodes: Input nodes and output nodes connected by weighted links. Perceptrons are being used as linear classifiers in supervised machine learning tasks. Permissive transformations \u00b6 [ back to top ] Permissive transformations are transformations of data that that do not change the \"meaning\" of the attributes, such as scaling or mapping. For example, the transformation of temperature measurements from a Celsius to a Kelvin scale would be a permissive transformation of a numerical attribute. Poisson distribution (univariate) \u00b6 [ back to top ] Probability density function p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} Population mean \u00b6 [ back to top ] \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i example mean vector: \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} Power transform \u00b6 [ back to top ] Power transforms form a category of statistical transformation techniques that are used to transform non-normal distributed data to normality. Principal Component Analysis (PCA) \u00b6 [ back to top ] A linear transformation technique that is commonly used to project a dataset (without utilizing class labels) onto a new feature space or feature subspace (for dimensionality reduction) where the new component axes are the directions that maximize the variance/spread of the data. Scatter matrix S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T where: \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} Precision and Recall \u00b6 [ back to top ] Precision (synonymous to specificity ) and recall (synonymous to sensitivity ) are two measures to assess performance of a classifier if class label distributions are skewed. Precision is defined as the ratio of number of relevant items out of total retrieved items, whereas recall is the fraction of relevant items which are retrieved. Predictive Modeling \u00b6 [ back to top ] Predictive modeling a data mining task for predicting outcomes based on a statistical model that was build on previous observations (in contrast to descriptive modeling ). Predictive modeling can be further divided into the three sub-tasks: Regression, classification, and ranking. Proportion of Variance Explained \u00b6 [ back to top ] In the context of dimensionality reduction, the proportion of variance explained (PVE) describes how much of the total variance is captured by the new selected axes, for example, principal components or discriminant axes. It is computed by the sum of variance of new component axes divided by the total variance. Purity Measure \u00b6 [ back to top ] In a cluster analysis with given truth cluster memberships (or classes), \"purity\" is used to assess the effectiveness of clustering. Purity is measured by assigning each cluster to the class that is maximally represented and computed via the weighted average of maximum number of samples from the same class in each cluster. Q \u00b6 Quantitative and qualitative attributes \u00b6 [ back to top ] Quantitative attributes are also often called \"numeric\"; those are attributes for which calculations and comparisons like ratios and intervals make sense (e.g., temperature in Celsius). Qualitative, or \"categorical\", attributes can be grouped into to subclasses: nominal and ordinal. Where ordinal attributes (e.g., street numbers) can be ordered, nominal attributes can only distinguished by their category names (e.g., colors). R \u00b6 R-factor \u00b6 [ back to top ] The R-factor is one of several measures to assess the quality of a protein crystal structure. After building and refining an atomistic model of the crystal structure, the R-factor measures how well this model can describe the experimental diffraction patterns via the equation: R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} Random forest \u00b6 [ back to top ] Random forest is an ensemble classifier where multiple decision tree classifiers are learned and combined via the bagging technique. Unseen/test objects are then classified by taking the majority of votes from individual decision trees. Rayleigh distribution (univariate) \u00b6 [ back to top ] Probability density function p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} Receiver Operating Characteristic (ROC) \u00b6 [ back to top ] The Receiver Operating Characteristic (ROC, or ROC curve) is a quality measure for binary prediction algorithms by plotting the \"False positive rate\" vs. the \"True positive rate\" ( sensitivity ). Regularization \u00b6 [ back to top ] Regularization is a technique to overcome overfitting by introducing a penalty term for model complexity. Usually, the penalty term is the squared sum of the model parameters, thereby promoting less complex models during training. Regularization may increase the training error but can potentially reduce the classification error on the test dataset. Reinforcement learning \u00b6 [ back to top ] Reinforcement learning is a machine learning algorithm that learns from a series of actions by maximizing a \"reward function\". The reward function can either be maximized by penalizing \"bad actions\" and/or rewarding \"good actions\". Rejection sampling \u00b6 [ back to top ] Rejection sampling is similar to the popular Monte Carlo sampling with the difference of an additional bound. The goal of rejection sampling is to simplify the task of drawing random samples from a complex probability distribution by using a uniform distribution instead; random samples drawn from the uniform distribution that lie outside certain boundary criteria are rejected, and all samples within the boundary are accepted, respectively. Resubstitution error \u00b6 [ back to top ] The resubstitution error represents the classification error rate on the training dataset (the dataset that was used to train the classifier). The performance of a classifier cannot be directly deduced from resubstitution error alone, but it becomes a useful measure for calculating the generalization error . Ridge Regression \u00b6 [ back to top ] Ridge regression is a regularized regression technique in which the squared sum of the model coefficients is used to penalize model complexity. Rule-based classifier \u00b6 [ back to top ] Rule-based classifiers are classifiers that are based on one or more \"IF ... THEN ...\" rules. Rule-based classifiers are related to decision trees and can be extracted from the latter. If the requirements for a rule-based classifier (mutually exclusive: at most one rule per sample; mutuallyy exhaustive: at least one rule per sample) are violated, possible remedies include the addition of rules or the ordering of rules. S \u00b6 Sampling \u00b6 [ back to top ] Sampling is data pre-processing procedure that is used to reduce the overall size of a dataset and to reduce computational costs by selecting a representative subset from the whole input dataset. Sensitivity \u00b6 [ back to top ] Sensitivity (synonymous to precision ), which is related to specificity -- in the context of error rate evaluation -- describes the \"True Positive Rate\" for a binary classification problem: The probability to make a correct prediction for a \"positive/true\" case (e.g., in an attempt to predict a disease, the disease is correctly predicted for a patient who truly has this disease). Sensitivity is calculated as (TP)/(TP+FN), where TP=True Positives, FN=False Negatives. Sharding \u00b6 [ back to top ] Sharding is the non-redundant partitioning of a database into smaller databases; this process can also be understood as horizontal splitting. The rationale behind sharing is to divide a database among separate machines to avoid storage or performance issues that are related to growing database sizes. Silhouette Measure (clustering) \u00b6 [ back to top ] Silhouette measure provides a metric to evaluate the performance of a clustering analysis. For each data point i , it measures the average distance of point i to all other points in the same cluster (a(i)) , and the minimum distance to points from other clusters (b(i)) . The average silhouette measures for each cluster can provide a visual way to pick the proper number of clusters. Simple Matching Coefficient \u00b6 [ back to top ] The simple matching coefficient is a similarity measure for binary data and calculated by dividing the total number of matches by the total number of attributes. For asymmetric binary data, the related Jaccard coefficient is to be preferred in order to avoid highly similar scores. Singular Value Decomposition (SVD) \u00b6 [ back to top ] Singular value decomposition (SVD) is linear algebra technique that decomposes matrix X into U D V T where U (left-singular vectors) and V (right-singular vector) are both column-orthogonal, and D is a diagonal matrix that contains singular values. PCA is closely related to the right0singular vectors of SVD. Soft classification \u00b6 [ back to top ] The general goal of a pattern classification is to assign a pre-defined class labels to particular observations. Typically, in (hard) classification, only one class label is assigned to every instance whereas in soft classification, an instance can have multiple class labels. The degree to which an instance belongs to different classes is then defined by a so-called membership function. Specificity \u00b6 [ back to top ] Specificity (synonymous to recall ), which is related to sensitivity -- in the context of error rate evaluation -- describes the \"True Negative Rate\" for a binary classification problem: The probability to make a correct prediction for a \"false/negative\" case (e.g., in an attempt to predict a disease, no disease is predicted for a healthy patient). Specificity is calculated as (TN)/(FP+TN), where TN=True Negatives, FP=False Positives. Standard deviation \u00b6 [ back to top ] \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} Stochastic Gradient Descent (SGD) \u00b6 [ back to top ] Stochastic Gradient Descent (SGD) (also see Gradient Descent ) is a machine learning algorithm that seeks to minimize an objective (or cost) function and can be grouped into the category of linear classifiers for supervised learning tasks. In contrast to Batch Gradient Descent , the gradient is computed from a single sample. Supervised Learning \u00b6 [ back to top ] The problem of inferring a mapping between the input space X and a target variable y when given labelled training data (i.e. (X,y) pairs). Encompasses the problems of classification (categorical y) and regression (continuous y). Support Vector Machine \u00b6 [ back to top ] SMV is a classification method that tries to find the hyperplane which separates classes with highest margin. The margin is defined as the minimum distance from sample points to the hyperplane. The sample point(s) that form margin are called support vectors and eventually establish the SVM model. T \u00b6 Term frequency and document frequency \u00b6 [ back to top ] Term frequency and document frequency are commonly used measures in context of text classification tasks. Term frequency is the count of how often a particular word occurs in a particular document. In contrast, document frequency measures the presence or absence of a particular word in a document as a binary value. Thus, for a single document, the document frequency is either 1 or 0. Term frequency - inverse document frequency, Tf-idf \u00b6 [ back to top ] Term frequency - inverse document frequency (Tf-idf) is a weighting scheme for term frequencies and document frequencies in text classification tasks that favors terms that occur in relatively few documents. The Tf-idf is calculated as a simple product of term frequency and the inverse document frequency, and the latter is calculated is calculated by log(\"number of documents in total\" / \"number of documents that contain a particular term\"). Tokenization \u00b6 [ back to top ] Tokenization, in the context of natural language processing (NLP) is the process of breaking down a text into individual elements which can consist of words or symbols. Tokenization is usually accompanied by other processing procedures such as stemming, the removal of stop words, or the creation of n-grams. U \u00b6 Unsupervised Learning \u00b6 [ back to top ] The problem of inferring latent structure in data when not given any training cases. Encompasses the problems of clustering, dimensionality reduction and density estimation. V \u00b6 Variance \u00b6 [ back to top ] \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad W \u00b6 White noise \u00b6 [ back to top ] White noise is a source that produces random, statistically independent variables following a particular distribution. In the field of sound processing, white noise is also often referred to as a mixture of tones or sounds of different frequencies. Whitening transformation \u00b6 [ back to top ] Whitening transformation is a normalization procedure to de-correlate samples in a dataset if the covariance matrix is not a diagonal matrix. Features are uncorrelated after \"whitening\" and their variances are equal unity, thus the covariance matrix becomes an identity matrix. Z \u00b6 Z-score \u00b6 [ back to top ] z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma}","title":"Dictionary"},{"location":"glossary/dictionary/#glossary","text":"Last updated: 12 / May / 2020","title":"Glossary"},{"location":"glossary/dictionary/#table-of-contents","text":"Accuracy Active Learning Anomaly detection Artificial Neural Networks (ANN) Backtesting Bagging Bag of words Batch Gradient Descent Bayes Theorem Batch Learning Big Data Binomial distribution Bootstrapping Bregman divergence Central Limit Theorem Co-Variance Confusion Matrix Contingency Table Correlation analysis Correlation analysis, Canonical Correlation analysis - Matthews Correlation Coefficient (MCC) Correlation analysis - Kendall Correlation analysis - Pearson Correlation analysis - Spearman Cosine Similarity Cost function Cross-validation Cross-validation, K-fold Cross-validation, Leave-One-Out Cross-validation, Random Sampling Curse of dimensionality Data mining DBSCAN Decision rule Decision tree classifier Density-based clustering Descriptive modeling Dimensionality reduction Distance Metric Learning Distance, Euclidean Distance, Manhattan Distance, Minkowski Eager learners Eigenvectors and Eigenvalues Ensemble methods Evolutionary algorithms Exhaustive search Expectation Maximization algorithm - EM Feature Selection Feature Space Fuzzy C-Means Clustering Generalization error Genetic algorithm Gradient Descent Greedy Algorithm Heuristic search Hyperparameters iid Imputation Independent Component Analysis Jaccard coefficient Jackknifing Jittering k-D Trees Kernel Density Estimation Kernel (in statistics) Kernel Methods Kernel Trick k-fold Cross-validation K-Means Clustering K-Means++ Clustering K-Medoids Clustering K-nearest neighbors algorithms Knowledge Discovery in Databases (KDD) LASSO Regression Latent Semantic Indexing Law of Large Numbers Lazy learners Least Squares fit Lennard-Jones Potential Linear Discriminant Analysis Linear Discriminant Analysis (LDA) Local Outlier Factor (LOF) Locality-sensitive hashing (LSH) Logistic Regression Machine learning Mahalanobis distance MapReduce Markov chains Monte Carlo simulation Maximum Likelihood Estimates (MLE) Min-Max scaling MinHash Naive Bayes Classifier N-grams Non-parametric statistics Normal distribution (multivariate) Normal distribution (univariate) Normal Modes Normalization - Min-Max scaling Normalization - Standard Scores Objective function On-Line Learning On-Line Analytical Processing (OLAP) Parzen-Rosenblatt Window technique Pattern classification Perceptron Permissive transformations Poisson distribution (univariate) Population mean Power transform Principal Component Analysis (PCA) Precision and Recall Predictive Modeling Proportion of Variance Explained Purity Measure Quantitative and qualitative attributes R-factor Random forest Rayleigh distribution (univariate) Receiver Operating Characteristic (ROC) Regularization Reinforcement learning Rejection sampling Resubstitution error Ridge Regression Rule-based classifier Sampling Sensitivity Sharding Silhouette Measure (clustering) Simple Matching Coefficient Singular Value Decomposition (SVD) Soft classification Specificity Standard deviation Stochastic Gradient Descent (SGD) Supervised Learning Support Vector Machine Term frequency and document frequency Term frequency - inverse document frequency, Tf-idf Tokenization Unsupervised Learning Variance White noise Whitening transformation Z-score","title":"Table of Contents"},{"location":"glossary/dictionary/#a","text":"","title":"A"},{"location":"glossary/dictionary/#accuracy","text":"[ back to top ] Accuracy is defined as the fraction of correct classifications out of the total number of samples; it resembles one way to assess the performance of a predictor and is often used synonymous to specificity / precision although it is calculated differently. Accuracy is calculated as: \\frac{True Positives + True Negatives}{Positives+Negatives} \\frac{True Positives + True Negatives}{Positives+Negatives} Source: wikipedia","title":"Accuracy"},{"location":"glossary/dictionary/#active-learning","text":"[ back to top ] Active learning is a variant of the on-line learning machine learning architecture where feedback about the ground truth class labels of unseen data can be requested if the classification is uncertain. New training data that was labeled can then be used to update the model as in on-line learning .","title":"Active Learning"},{"location":"glossary/dictionary/#anomaly-detection","text":"[ back to top ] Anomaly detection describes the task of identifying points that deviate from specific patterns in a dataset -- the so-called outliers. Different types of anomaly detection methods include graph-based, statistical-based and distance-based techniques and can be used in both unsupervised and supervised learning tasks.","title":"Anomaly detection"},{"location":"glossary/dictionary/#artificial-neural-networks-ann","text":"[ back to top ] Artificial Neural Networks (ANN) are a class of machine learning algorithms that are inspired by the neuron architecture of the human brain. Typically, a (multi-layer) ANN consists of a layer of input nodes, a layer of output nodes, and hidden layers in-between. The nodes are connected by weighted links that can be interpreted as the neuron-connections by axons of different strengths. The simplest version of an ANN is a single-layer perceptron .","title":"Artificial Neural Networks (ANN)"},{"location":"glossary/dictionary/#b","text":"","title":"B"},{"location":"glossary/dictionary/#backtesting","text":"[ back to top ] Backtesting is a specific case of cross-validation in the context of finance and trading models where empirical data from previous time periods (data from the past) is used to evaluate a trading strategy.","title":"Backtesting"},{"location":"glossary/dictionary/#bagging","text":"[ back to top ] Bagging is an ensemble method for classification (or regression analysis) in which individual models are trained by random sampling of data, and the final decision is made by voting among individual models with equal weights (or averaging for regression analysis).","title":"Bagging"},{"location":"glossary/dictionary/#bag-of-words","text":"[ back to top ] Bag of words is a model that is used to construct sparse feature vectors for text classification tasks. The bag of words is an unordered set of all words that occur in all documents that are part of the training set. Every word is then associated with a count of how often it occurs whereas the positional information is ignored. Sometimes, the bag of words is also called \"dictionary\" or \"vocabulary\" based on the training data.","title":"Bag of words"},{"location":"glossary/dictionary/#batch-gradient-descent","text":"[ back to top ] Batch Gradient descent is a variant of a Gradient Descent algorithm to optimize a function by finding its local minimum. In contrast to Stochastic Gradient Descent the gradient is computed from the whole dataset.","title":"Batch Gradient Descent"},{"location":"glossary/dictionary/#bayes-theorem","text":"[ back to top ] Naive Bayes' classifier: posterior probability: P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} decision rule: \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} objective functions: g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg)","title":"Bayes Theorem"},{"location":"glossary/dictionary/#batch-learning","text":"[ back to top ] Batch learning is an architecture used in machine learning tasks where the entire training dataset is available upfront to build the model. In contrast to on-line learning , the model is not updated once it was build on a training dataset.","title":"Batch Learning"},{"location":"glossary/dictionary/#big-data","text":"[ back to top ] There are many different, controversial interpretations and definitions for the term \"Big Data\". Typically, one refers to data as \"Big Data\" if its volume and complexity are of a magnitude that the data cannot be processed by \"conventional\" computing platforms anymore; storage space, processing power, and database structures are typically among the limiting factors.","title":"Big Data"},{"location":"glossary/dictionary/#binomial-distribution","text":"[ back to top ] Probability density function: p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k}","title":"Binomial distribution"},{"location":"glossary/dictionary/#bootstrapping","text":"[ back to top ] A resampling technique to that is closely related to cross-validation where a training dataset is divided into random subsets. Bootstrapping -- in contrast to cross-validation -- is a random sampling with replacement. Bootstrapping is typically used for statistical estimation of bias and standard error, and a common application in machine learning is to estimate the generalization error of a predictor.","title":"Bootstrapping"},{"location":"glossary/dictionary/#bregman-divergence","text":"[ back to top ] Bregman divergence describes are family of proximity functions (or distance measures) that share common properties and are often used in clustering algorithms. A popular example is the squared Euclidean distance.","title":"Bregman divergence"},{"location":"glossary/dictionary/#c","text":"","title":"C"},{"location":"glossary/dictionary/#central-limit-theorem","text":"[ back to top ] The Central Limit Theorem is a theorem in the field of probability theory that expresses the idea that the distribution of sample means (from independent random variables) converges to a normal distribution when the sample size approaches infinity.","title":"Central Limit Theorem"},{"location":"glossary/dictionary/#co-variance","text":"[ back to top ] S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) example covariance matrix: \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}","title":"Co-Variance"},{"location":"glossary/dictionary/#confusion-matrix","text":"[ back to top ] The confusion matrix is used as a way to represent the performance of a classifier and is sometimes also called \"error matrix\". This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.","title":"Confusion Matrix"},{"location":"glossary/dictionary/#contingency-table","text":"[ back to top ] A contingency table is used in clustering analysis to compare the overlap between two different clustering (grouping) results. The partitions from the two clustering results are represented as rows and columns in the table, and the individual elements of the table represent the number of elements that are shared between two partitions from each clustering result.","title":"Contingency Table"},{"location":"glossary/dictionary/#correlation-analysis","text":"[ back to top ] Correlation analysis describes and quantifies the relationship between two independent variables. Typically, in case of a positive correlation both variables have a tendency to increase, and in the case of negative correlation, one variable increases while the other variable increases. It is important to mention the famous quotation \"correlation does not imply causation\".","title":"Correlation analysis"},{"location":"glossary/dictionary/#correlation-analysis-canonical","text":"Let x and y be two vectors, the goal of canonical correlation analysis is to maximize the correlation between linear transformations of those original vectors. With applications in dimensionality reduction and feature selection, CCA tries to find common dimensions between two vectors.","title":"Correlation analysis, Canonical"},{"location":"glossary/dictionary/#correlation-analysis-matthews-correlation-coefficient-mcc","text":"[ back to top ] MCC is an assessment metric for clustering or binary classification analyses that represents the correlation between the observed (ground truth) and predicted labels. MCC can be directly computed from the confusion matrix and returns a value between -1 and 1.","title":"Correlation analysis - Matthews Correlation Coefficient (MCC)"},{"location":"glossary/dictionary/#correlation-analysis-kendall","text":"[ back to top ] Similar to the Pearson correlation coefficient , Kendall's tau measures the degree of a monotone relationship between variables, and like Spearman's rho , it calculates the dependence between ranked variables, which makes it feasible for non-normal distributed data. Kendall tau can be calculated for continuous as well as ordinal data. Roughly speaking, Kendall's tau distinguishes itself from Spearman's rho by stronger penalization of non-sequential (in context of the ranked variables) dislocations. \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} where: c = the number of concordant pairs d = the number of discordant pairs [ Source ] If ties are present among the 2 ranked variables, the following equation shall be used instead: \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ where: t = number of observations of variable x that are tied u = number of observations of variable y that are tied","title":"Correlation analysis - Kendall"},{"location":"glossary/dictionary/#correlation-analysis-pearson","text":"[ back to top ] The Pearson correlation coefficient is probably the most widely used measure for linear relationships between two normal distributed variables and thus often just called \"correlation coefficient\". Usually, the Pearson coefficient is obtained via a Least-Squares fit and a value of 1 represents a perfect positive relation-ship, -1 a perfect negative relationship, and 0 indicates the absence of a relationship between variables. \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} And the estimate r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}}","title":"Correlation analysis - Pearson"},{"location":"glossary/dictionary/#correlation-analysis-spearman","text":"[ back to top ] Related to the Pearson correlation coefficient , the Spearman correlation coefficient (rho) measures the relationship between two variables. Spearman's rho can be understood as a rank-based version of Pearson's correlation coefficient , which can be used for variables that are not normal-distributed and have a non-linear relationship. Also, its use is not only restricted to continuous data, but can also be used in analyses of ordinal attributes. \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} where: d = the pairwise distances of the ranks of the variables x i and y i . n = the number of samples.","title":"Correlation analysis - Spearman"},{"location":"glossary/dictionary/#cosine-similarity","text":"[ back to top ] Cosine similarity measures the orientation of two n -dimensional sample vectors irrespective to their magnitude. It is calculated by the dot product of two numeric vectors, and it is normalized by the product of the vector lengths, so that output values close to 1 indicate high similarity. cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}","title":"Cosine Similarity"},{"location":"glossary/dictionary/#cost-function","text":"[ back to top ] A cost function (synonymous to loss function) is a special case of an objective function , i.e., a function that is used for solving optimization problems. A cost function can take one or more input variables and the output variable is to be minimized. A typical use case for cost functions is parameter optimization.","title":"Cost function"},{"location":"glossary/dictionary/#cross-validation","text":"[ back to top ] Cross-validation is a statistical technique to estimate the prediction error rate by splitting the data into training, cross-validation, and test datasets. A prediction model is obtained using the training set, and model parameters are optimized by the cross-validation set, while the test set is held primarily for empirical error estimation.","title":"Cross-validation"},{"location":"glossary/dictionary/#cross-validation-k-fold","text":"[ back to top ] K-fold cross-validation is a variant of cross validation where contiguous segments of samples are selected from the training dataset to build two new subsets for every iteration (without replacement): a new training and test dataset (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, K-fold"},{"location":"glossary/dictionary/#cross-validation-leave-one-out","text":"[ back to top ] Leave-One-Out cross-validation a variant of cross validation one sample is removed for every iteration (without replacement). The model is trained on the remaining N-1 samples and evaluated via the removed sample (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, Leave-One-Out"},{"location":"glossary/dictionary/#cross-validation-random-sampling","text":"[ back to top ] Cross-validation via random sampling is a variant of cross validation where random chunks of samples are extracted from the training dataset to build two new subsets for every iteration (with or without replacement): a new training and test dataset for every iteration (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, Random Sampling"},{"location":"glossary/dictionary/#curse-of-dimensionality","text":"[ back to top ] For a fixed number of training samples, the curse of dimensionality describes the increased error rate for a large number of dimensions (or features) due to imprecise parameter estimations.","title":"Curse of dimensionality"},{"location":"glossary/dictionary/#d","text":"","title":"D"},{"location":"glossary/dictionary/#data-mining","text":"[ back to top ] A field that is closely related to machine learning and pattern classification. The focus of data mining does not lie in merely the collection of data, but the extraction of useful information: Discovery of patterns, and making inferences and predictions. Common techniques in data mining include predictive modeling, clustering, association rules, and anomaly detection.","title":"Data mining"},{"location":"glossary/dictionary/#dbscan","text":"[ back to top ] DBSCAN is a variant of a density-based clustering algorithm that identifies core points as regions of high-densities based on their number of neighbors (> MinPts ) in a specified radius (\u03b5). Points that are below MinPts but within \u03b5 are specified as border points; the remaining points are classified as noise points.","title":"DBSCAN"},{"location":"glossary/dictionary/#decision-rule","text":"[ back to top ] A function in pattern classification tasks of making an \"action\", e.g., assigning a certain class label to an observation or pattern.","title":"Decision rule"},{"location":"glossary/dictionary/#decision-tree-classifier","text":"[ back to top ] Decision tree classifiers are tree like graphs, where nodes in the graph test certain conditions on a particular set of features, and branches split the decision towards the leaf nodes. Leaves represent lowest level in the graph and determine the class labels. Optimal tree are trained by minimizing Gini impurity, or maximizing information gain.","title":"Decision tree classifier"},{"location":"glossary/dictionary/#density-based-clustering","text":"[ back to top ] In density-based clustering, regions of high density in n-dimensional space are identified as clusters. The best advantage of this class of clustering algorithms is that they do not require apriori knowledge of number of clusters (as opposed to k-means algorithm).","title":"Density-based clustering"},{"location":"glossary/dictionary/#descriptive-modeling","text":"[ back to top ] Descriptive modeling is a common task in the field of data mining where a model is build in order to distinguish between objects and categorize them into classes - a form of data summary. In contrast to predictive modeling , the scope of descriptive modeling does not extend to making prediction for unseen objects.","title":"Descriptive modeling"},{"location":"glossary/dictionary/#dimensionality-reduction","text":"[ back to top ] Dimensionality reduction is a data pre-processing step in machine learning applications that aims to avoid the curse of dimensionality and reduce the effect of overfitting. Dimensionality reduction is related to feature selection , but instead of selecting a feature subset, dimensionality reduction takes as projection-based approach (e.g, linear transformation) in order to create a new feature subspace.","title":"Dimensionality reduction"},{"location":"glossary/dictionary/#distance-metric-learning","text":"[ back to top ] Distance metrics are fundamental for many machine learning algorithms. Distance metric learning - instead of learning a model - incorporates estimated relevances of features to obtain a distance metric for potentially optimal separation of classes and clusters: Large distances for objects from different classes, and small distances for objects of the same class, respectively.","title":"Distance Metric Learning"},{"location":"glossary/dictionary/#distance-euclidean","text":"[ back to top ] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras' theorem. The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension. \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}","title":"Distance, Euclidean"},{"location":"glossary/dictionary/#distance-manhattan","text":"[ back to top ] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance, but instead of calculating the shortest diagonal path (\"beeline\") between two points, it calculates the distance based on gridlines. The Manhattan distance was named after the block-like layout of the streets in Manhattan. \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}","title":"Distance, Manhattan"},{"location":"glossary/dictionary/#distance-minkowski","text":"[ back to top ] The Minkowski distance is a generalized form of the Euclidean distance (if p=2 ) and the Manhattan distance (if p=1 ). \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}","title":"Distance, Minkowski"},{"location":"glossary/dictionary/#e","text":"","title":"E"},{"location":"glossary/dictionary/#eager-learners","text":"[ back to top ] Eager learners (in contrast to lazy learners ) describe machine learning algorithms that learn a model for mapping attributes to class labels as soon as the data becomes available (e.g., Decision tree classifiers or naive Bayes classifiers ) and do not require the training data for making predictions on unseen samples once the model was built. The most computationally expensive step is the creation of a prediction model from the training data, and the actual prediction is considered as relatively inexpensive.","title":"Eager learners"},{"location":"glossary/dictionary/#eigenvectors-and-eigenvalues","text":"[ back to top ] Both eigenvectors and eigenvalues fundamental in many applications involve linear systems and are related via A\u00b7v = \u03bb\u00b7v (where A is a square matrix, v the eigenvector, and \u03bb the eigenvalue). Eigenvectors are describing the direction of the axes of a linear transformation, whereas eigenvalues are describing the scale or magnitude. \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ where: \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue}","title":"Eigenvectors and Eigenvalues"},{"location":"glossary/dictionary/#ensemble-methods","text":"[ back to top ] Ensemble methods combine multiple classifiers which may differ in algorithms, input features, or input samples. Statistical analyses showed that ensemble methods yield better classification performances and are also less prone to overfitting. Different methods, e.g., bagging or boosting, are used to construct the final classification decision based on weighted votes.","title":"Ensemble methods"},{"location":"glossary/dictionary/#evolutionary-algorithms","text":"[ back to top ] Evolutionary algorithms are a class of algorithms that are based on heuristic search methods inspired by biological evolution in order to solve optimization problems.","title":"Evolutionary algorithms"},{"location":"glossary/dictionary/#exhaustive-search","text":"[ back to top ] Exhaustive search (synonymous to brute-force search) is a problem-solving approach where all possible combinations are sequentially evaluated to find the optimal solution. Exhaustive search guarantees to find the optimal solution whereas other approaches (e.g., heuristic searches ) are regarded as sub-optimal. A downside of exhaustive searches is that computational costs increase proportional to the number of combinations to be evaluated.","title":"Exhaustive search"},{"location":"glossary/dictionary/#expectation-maximization-algorithm-em","text":"[ back to top ] The Expectation Maximization algorithm (EM) is a technique to estimate parameters of a distribution based on the Maximum Likelihood Estimate (MLE) that is often used for the imputation of missing values in incomplete datasets. After the EM algorithm is initialized with a starting value, alternating iterations between expectation and maximization steps are repeated until convergence. In the expectation step, parameters are estimated based on the current model to impute missing values. In the maximization step, the log-likelihood function of the statistical model is to be maximized by re-estimating the parameters based on the imputed values from the expectation step.","title":"Expectation Maximization algorithm - EM"},{"location":"glossary/dictionary/#f","text":"","title":"F"},{"location":"glossary/dictionary/#feature-selection","text":"[ back to top ] Feature selection is an important pre-processing step in many machine learning applications in order to avoid the curse of dimensionality and overfitting . A subset of features is typically selected by evaluating different combinations of features and eventually retain the subset that minimizes a specified cost function . Commonly used algorithms for feature selection as alternative to exhaustive search algorithms include sequential selection algorithms and genetic algorithms.","title":"Feature Selection"},{"location":"glossary/dictionary/#feature-space","text":"[ back to top ] A feature space describes the descriptive variables that are available for samples in a dataset as a d -dimensional Euclidean space. E.g., sepal length and width, and petal length and width for each flower sample in the popular Iris dataset.","title":"Feature Space"},{"location":"glossary/dictionary/#fuzzy-c-means-clustering","text":"[ back to top ] Fuzzy C-Means is a soft clustering algorithm in which each sample point has a membership degree to each cluster; in hard (crisp) clustering, membership of each point to each cluster is either 0 or 1. Fuzzy C-Means considers a weight matrix for cluster memberships, and minimizes sum squared error (SSE) of weighted distances of sample points to the cluster centroids.","title":"Fuzzy C-Means Clustering"},{"location":"glossary/dictionary/#g","text":"","title":"G"},{"location":"glossary/dictionary/#generalization-error","text":"[ back to top ] The generalization error describes how well new data can be classified and is a useful metric to assess the performance of a classifier. Typically, the generalization error is computed via cross-validation or simply the absolute difference between the error rate on the training and test dataset.","title":"Generalization error"},{"location":"glossary/dictionary/#genetic-algorithm","text":"[ back to top ] The Genetic algorithm is a subclass of evolutionary algorithms that takes a heuristic approach inspired by Charles Darwin's theory of \"natural selection\" in order to solve optimization problems.","title":"Genetic algorithm"},{"location":"glossary/dictionary/#gradient-descent","text":"[ back to top ] Gradient descent is an algorithm that optimizes a function by finding its local minimum. After the algorithm was initialized with an initial guess, it takes the derivative of the function to make a step towards the direction of deepest descent. This step-wise process is repeated until convergence.","title":"Gradient Descent"},{"location":"glossary/dictionary/#greedy-algorithm","text":"[ back to top ] Greedy Algorithms are a family of algorithms that are used in optimization problems. A greedy algorithm makes locally optimal choices in order to find a local optimum (suboptimal solution, also see ( heuristic problem solving ).","title":"Greedy Algorithm"},{"location":"glossary/dictionary/#h","text":"","title":"H"},{"location":"glossary/dictionary/#heuristic-search","text":"[ back to top ] Heuristic search is a problem-solving approach that is focussed on efficiency rather than completeness in order to find a suboptimal solution to a problem. Heuristic search is often used as alternative approach where exhaustive search is too computationally intensive and where solutions need to be approximated.","title":"Heuristic search"},{"location":"glossary/dictionary/#hyperparameters","text":"[ back to top ] Hyperparameters are the parameters of a classifier or estimator that are not directly learned in the machine learning step from the training data but are optimized separately (e.g., via Grid Search ). The goal of hyperparameter optimization is to achieve good generalization of a learning algorithm and to avoid overfitting to the training data.","title":"Hyperparameters"},{"location":"glossary/dictionary/#i","text":"","title":"I"},{"location":"glossary/dictionary/#iid","text":"[ back to top ] The abbreviation \"iid\" stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another variable (e.g., time series and network graphs are not independent). One popular example of iid would be the tossing of a coin: One coin toss does not affect the outcome of another coin toss, and the probability of the coin landing on either \"heads\" or \"tails\" is the same for every coin toss.","title":"iid"},{"location":"glossary/dictionary/#imputation","text":"[ back to top ] Imputations algorithms are designed to replace the missing data (NAs) with certain statistics rather than discarding them for downstream analysis. Commonly used imputation methods include mean imputation (replacement by the sample mean of an attribute), kNN imputation, and regression imputation.","title":"Imputation"},{"location":"glossary/dictionary/#independent-component-analysis","text":"[ back to top ] Independent Component Analysis (ICA) is a statistical signal-processing technique that decomposes a multivariate dataset of mixed, non-gaussian distributed source signals into independent components. A popular example is the separation of overlapping voice samples -- the so-called \"cocktail party problem\".","title":"Independent Component Analysis"},{"location":"glossary/dictionary/#j","text":"","title":"J"},{"location":"glossary/dictionary/#jaccard-coefficient","text":"[ back to top ] The Jaccard coefficient (bounded at [0, 1)) is used as similarity measure for asymmetric binary data and calculated by taking the number of matching attributes and divide it by the number of all attributes except those where both variables have a value 0 in contrast to a simple matching coefficient . A popular application is the identification of near-duplicate documents for which the Jaccard coefficient can be calculated by the dividing the intersection of the set of words by the union of the set words in both documents.","title":"Jaccard coefficient"},{"location":"glossary/dictionary/#jackknifing","text":"[ back to top ] Jackknifing is a resampling technique that predates the related cross-validation and bootstrapping techniques and is mostly used for bias and variance estimations. In jackknifing, a dataset is split into N subsets where exactly one sample is removed from every subset so that every subset is of size N-1.","title":"Jackknifing"},{"location":"glossary/dictionary/#jittering","text":"[ back to top ] Jittering is a sampling technique that can be used to measure the stability of a given statistical model (classifiction/regression/clustering). In jittering, some noise is added to sample data points, and then a new model is drawn and compared to the original model.","title":"Jittering"},{"location":"glossary/dictionary/#k","text":"","title":"K"},{"location":"glossary/dictionary/#k-d-trees","text":"[ back to top ] k-D trees are a data structures (recursive space partitioning trees) that result from the binary partitioning of multi-dimensional feature spaces. A typical application of k-D trees is to increase the search efficiency for nearest-neighbor searches. A k-D tree construction can be described as a iterating process with the following steps: Select the dimension of largest variance, draw a cutting plane based at the median along the dimension to split the data into 2 halves, choose the next dimension.","title":"k-D Trees"},{"location":"glossary/dictionary/#kernel-density-estimation","text":"[ back to top ] Non-parametric techniques to estimate probability densities from the available data without requiring prior knowledge of the underlying model of the probability distribution.","title":"Kernel Density Estimation"},{"location":"glossary/dictionary/#kernel-in-statistics","text":"[ back to top ] In the context of [kernel methods](#kernel-methods the term \u201ckernel\u201d describes a function that calculates the dot product of the images of the samples x under the kernel function \u03c6 (see kernel methods). Roughly speaking, a kernel can be understood as a similarity measure in higher-dimensional space.","title":"Kernel (in statistics)"},{"location":"glossary/dictionary/#kernel-methods","text":"[ back to top ] Kernel methods are algorithms that map the sample vectors of a dataset onto a higher-dimensional feature space via a so-called kernel function (\u03c6(x)). The goal is to identify and simplify general relationships between data, which is especially useful for linearly non-separable datasets.","title":"Kernel Methods"},{"location":"glossary/dictionary/#kernel-trick","text":"[ back to top ] Since the explicit computation of the kernel is increasingly computationally expensive for large sample sizes and high numbers of dimensions, the kernel trick uses approximations to calculate the kernel implicitly. The most popular kernels used for the kernel trick are Gaussian Radius Basis Function (RBF) kernels, sigmoidal kernels, and polynomial kernels.","title":"Kernel Trick"},{"location":"glossary/dictionary/#k-fold-cross-validation","text":"[ back to top ] In k-fold cross-validation the data is split into k subsets, then a prediction/classification model is trained k times, each time holding one subset as test set, training the model parameters using the remaining k -1. Finally, cross-validation error is evaluated as the average error out of all k training models.","title":"k-fold Cross-validation"},{"location":"glossary/dictionary/#k-means-clustering","text":"[ back to top ] A method of partitioning a dataset into k clusters by picking k random initial points (where k < n , the number or total points - modified by S.R. ), assigning clusters, averaging, reassigning, and repeating until stability is achieved. The number k must be chosen beforehand.","title":"K-Means Clustering"},{"location":"glossary/dictionary/#k-means-clustering_1","text":"[ back to top ] A variant of k-means where instead of choosing all initial centers randomly, the first is chosen randomly, the second chosen with probability proportional to the squared distance from the first, the third chosen with probability proportional to the square distance from the first two, etc. See this paper .","title":"K-Means++ Clustering"},{"location":"glossary/dictionary/#k-medoids-clustering","text":"[ back to top ] K-Medoids clustering is a variant of k-means algorithm in which cluster centroids are picked among the sample points rather than the mean point of each cluster. K-Medoids can overcome some of the limitations of k-means algorithm by avoiding empty clusters, being more robust to outliers, and being more easily applicable to non-numeric data types.","title":"K-Medoids Clustering"},{"location":"glossary/dictionary/#k-nearest-neighbors-algorithms","text":"[ back to top ] K-nearest neighbors algorithms find the k-points that are closest to a point of interest based on their attributes using a certain distance measure (e.g., Euclidean distance). K-nearest neighbors algorithms are being used in many different contexts: Non-parametric density estimation, missing value imputation, dimensionality reduction, and classifiers in supervised and unsupervised pattern classification and regression problems.","title":"K-nearest neighbors algorithms"},{"location":"glossary/dictionary/#knowledge-discovery-in-databases-kdd","text":"[ back to top ] Knowledge Discovery in Databases (KDD) describes a popular workflow including data mining for extracting useful and meaningful information out of data. Typically, the individual steps are feature selection, pre-processing, transformation, data mining , and post-processing (evaluation and interpretation).","title":"Knowledge Discovery in Databases (KDD)"},{"location":"glossary/dictionary/#l","text":"","title":"L"},{"location":"glossary/dictionary/#lasso-regression","text":"[ back to top ] LASSO (Least Absolute Shrinkage and Selection Operator) is a regression model that uses the L1-norm (sum of absolute values) of model coefficients to penalize the model complexity. LASSO has the advantage that some coefficients can become zero, as opposed to ridge regression that uses the squared sum of model coefficients.","title":"LASSO Regression"},{"location":"glossary/dictionary/#latent-semantic-indexing","text":"[ back to top ] Latent Semantic Indexing (LSI) is a data mining technique to characterize documents by topics, word usage, or other contexts. The structures of the documents are compared by applying singular value decomposition to an input term-document matrix (e.g., a data table of word counts with terms as row labels and document numbers as column labels) in order to obtain the singular values and vectors.","title":"Latent Semantic Indexing"},{"location":"glossary/dictionary/#law-of-large-numbers","text":"[ back to top ] The Law of Large Numbers is a theorem in the field of probability theory that expresses the idea that the actual value of a random sampling process approaches the expected value for growing sample sizes. A common example is that the observed ratio of \"heads\" in an unbiased coin-flip experiment will approach 0.5 for large sample sizes.","title":"Law of Large Numbers"},{"location":"glossary/dictionary/#lazy-learners","text":"[ back to top ] Lazy learners (in contrast to eager learners ) are memorizing training data in order to make predictions for unseen samples. While there is no expensive learning step involved, the prediction step is generally considered to be more expensive compared to eager learners since it involves the evaluation of training data. One example of lazy learners are k-nearest neighbor algorithms where the class label of a unseen sample is estimated by e.g., the majority of class labels of its neighbors in the training data.","title":"Lazy learners"},{"location":"glossary/dictionary/#least-squares-fit","text":"[ back to top ] A linear regression technique that fits a straight line to a data set (or overdetermined system) by minimizing the sum of the squared residuals, which can be the minimized vertical or perpendicular offsets from the fitted line. Linear equation f(x) = a\\cdot x + b f(x) = a\\cdot x + b f(x) = a\\cdot x + b Slope: a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad Y-axis intercept: b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad where: S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} Matrix equation \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y","title":"Least Squares fit"},{"location":"glossary/dictionary/#lennard-jones-potential","text":"[ back to top ] The Lennard-Jones potential describes the energy potential between two non-bonded atoms based on their distance to each other. V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = intermolecular potential \u03c3 = distance where V is 0 r = distance between atoms, measured from one center to the other \u03b5 = interaction strength","title":"Lennard-Jones Potential"},{"location":"glossary/dictionary/#linear-discriminant-analysis","text":"[ back to top ] In-between class scatter matrix S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i Where: S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ and \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} Between class scatter matrix S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T","title":"Linear Discriminant Analysis"},{"location":"glossary/dictionary/#linear-discriminant-analysis-lda","text":"[ back to top ] A linear transformation technique (related to Principal Component Analysis) that is commonly used to project a dataset onto a new feature space or feature subspace, where the new component axes maximize the spread between multiple classes, or for classification of data.","title":"Linear Discriminant Analysis (LDA)"},{"location":"glossary/dictionary/#local-outlier-factor-lof","text":"[ back to top ] LOF is a density-based anomaly detection technique for outlier identification. The LOF for a point p refers to the average \"reachability distance\" towards its nearest neighbors. Eventually, the points with the largest LOF values (given a particular threshold) are identified as outliers.","title":"Local Outlier Factor (LOF)"},{"location":"glossary/dictionary/#locality-sensitive-hashing-lsh","text":"[ back to top ] Locality-sensitive hashing (LSH) is a dimensionality reduction technique that groups objects that are likely similar (based on a similarity signature such as MinHash ) into the same buckets in order to reduce the search space for pair-wise similarity comparisons. One application of LSH could be a combination with other dimensionality reduction techniques, e.g., MinHash , in order to reduce the computational costs of finding near-duplicate document pairs.","title":"Locality-sensitive hashing (LSH)"},{"location":"glossary/dictionary/#logistic-regression","text":"[ back to top ] Logistic regression is a statistical model used for binary classification (binomial logistic regression) where class labels are mapped to \"0\" or \"1\" outputs. Logistic regression uses the logistic function (a general form of sigmoid function), where its output ranges from (0-1).","title":"Logistic Regression"},{"location":"glossary/dictionary/#m","text":"","title":"M"},{"location":"glossary/dictionary/#machine-learning","text":"[ back to top ] A set of algorithmic instructions for discovering and learning patterns from data e.g., to train a classifier for a pattern classification task.","title":"Machine learning"},{"location":"glossary/dictionary/#mahalanobis-distance","text":"[ back to top ] The Mahalanobis distance measure accounts for the covariance among variables by calculating the distance between a sample x and the sample mean \u03bc in units of the standard deviation. The Mahalanobis distance becomes equal to the Euclidean distance for uncorrelated with same variances.","title":"Mahalanobis distance"},{"location":"glossary/dictionary/#mapreduce","text":"[ back to top ] MapRedcue is a programming model for analyzing large datasets on distributed computer clusters, in which the task is divided into two steps, a map step and a reducer step. In the map step, the data are filtered by some factors on each compute node, then filtered data are shuffled and passed to the reducer function which performs further analysis on each portion of filtered data separately.","title":"MapReduce"},{"location":"glossary/dictionary/#markov-chains","text":"[ back to top ] Markov chains (names after Andrey Markov) are mathematical systems that describe the transitioning between different states in a model. The transitioning from one state to the other (or back to itself) is a stochastic process.","title":"Markov chains"},{"location":"glossary/dictionary/#monte-carlo-simulation","text":"[ back to top ] A Monte Carlo simulation is an iterative sampling method for solving deterministic models. Random numbers or variables from a particular probability distribution are used as input variables for uncertain parameters to compute the response variables.","title":"Monte Carlo simulation"},{"location":"glossary/dictionary/#maximum-likelihood-estimates-mle","text":"[ back to top ] A technique to estimate the parameters that have been fit to a model by maximizing a known likelihood function. One common application is the estimation of \"mean\" and \"variance\" for a Gaussian distribution. D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} can be pictured as probability to observe a particular sequence of patterns, where the probability of observing a particular patterns depends on \u03b8 , the parameters the underlying (class-conditional) distribution. In order to apply MLE, we have to make the assumption that the samples are i.i.d. (independent and identically distributed). p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) Where \u03b8 is the parameter vector, that contains the parameters for a particular distribution that we want to estimate and p(D | \u03b8 ) is also called the likelihood of \u03b8 . log-likelihood p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) Differentiation \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} parameter vector \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg]","title":"Maximum Likelihood Estimates (MLE)"},{"location":"glossary/dictionary/#min-max-scaling","text":"[ back to top ] X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}","title":"Min-Max scaling"},{"location":"glossary/dictionary/#minhash","text":"[ back to top ] MinHash is a commonly used technique for dimensionality reduction in document similarity comparisons. The idea behind MinHash is to create a signature of reduced dimensionality while preserving the Jaccard similarity coefficient . A common implementation of MinHash is to generate k random permutations of the columns in a m*x*n -document matrix (rows represent the sparse vectors of words for each document as binary data) and generate a new matrix of size m*x*k . The cells of the new matrix now contain the position labels of the first non-zero value for every document (1 column for each round of random permutation). Based on similarities of the position labels, the Jaccard coefficient for the pairs of documents can be calculated.","title":"MinHash"},{"location":"glossary/dictionary/#n","text":"","title":"N"},{"location":"glossary/dictionary/#naive-bayes-classifier","text":"[ back to top ] A classifier based on a statistical model (i.e., Bayes theorem: calculating posterior probabilities based on the prior probability and the so-called likelihood) in the field of pattern classification. Naive Bayes assumes that all attributes are conditionally independent, thereby, computing the likelihood is simplified to the product of the conditional probabilities of observing individual attributes given a particular class label.","title":"Naive Bayes Classifier"},{"location":"glossary/dictionary/#n-grams","text":"[ back to top ] In context of natural language processing (NLP), a text is typically broken down into individual elements (see tokenization ). N-grams describe the length of the individual elements where n refers to the number of words or symbols in every token. E.g., a unigram (or 1-gram) can represent a single word, and a bigram (or 2-gram) describes a token that consists of 2 words etc.","title":"N-grams"},{"location":"glossary/dictionary/#non-parametric-statistics","text":"[ back to top ] In contrast to parametric approaches, non-parametric statistics or approaches do not make prior assumptions about the underlying probability distribution of a particular variable or attribute.","title":"Non-parametric statistics"},{"location":"glossary/dictionary/#normal-distribution-multivariate","text":"[ back to top ] Probability density function p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg]","title":"Normal distribution (multivariate)"},{"location":"glossary/dictionary/#normal-distribution-univariate","text":"[ back to top ] Probability density function p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] }","title":"Normal distribution (univariate)"},{"location":"glossary/dictionary/#normal-modes","text":"[ back to top ] Normal modes are the harmonic oscillations of a system of masses connected by springs, or roughly speaking \"concerted motions,\" and all normal modes of a system are independent from each other. A classic example describes two masses connected by a middle spring, and each mass is connected to a fixed outer edge ( | m1~~m2 |). The oscillation of this system where the middle spring does not move is defined as its normal mode.","title":"Normal Modes"},{"location":"glossary/dictionary/#normalization-min-max-scaling","text":"[ back to top ] A data pre-processing step (also often referred to as \"Feature Scaling\") for fitting features from different measurements within a certain range, typically the unit range from 0 to 1.","title":"Normalization - Min-Max scaling"},{"location":"glossary/dictionary/#normalization-standard-scores","text":"[ back to top ] A data pre-processing step (also often just called \"Standardization\") for re-scaling features from different measurements to match proportions of a standard normal distribution (unit variance centered at mean=0).","title":"Normalization - Standard Scores"},{"location":"glossary/dictionary/#o","text":"","title":"O"},{"location":"glossary/dictionary/#objective-function","text":"[ back to top ] Objective functions are mathematical function that are used for problem-solving and optimization tasks. Depending on the task, the objective function can be omtpimized through minimization ( cost or loss functions ) or maximization (reward function). A typical application of an objective function in pattern classification tasks is to minimize the error rate of a classifier.","title":"Objective function"},{"location":"glossary/dictionary/#on-line-learning","text":"[ back to top ] On-line learning is a machine learning architecture where the model is being updated consecutively as new training data arrives in contrast to batch-learning , which requires the entire training dataset to be available upfront. On-line has the advantage that a model can be updated and refined over time to account for changes in the population of training samples. A popular example where on-line learning is beneficial is the task of spam detection.","title":"On-Line Learning"},{"location":"glossary/dictionary/#on-line-analytical-processing-olap","text":"[ back to top ] On-Line Analytical Processing (OLAP) describes the general process of working with multidimensional arrays for exploratory analysis and information retrieval; often, OLAP is used to create summary data, e.g., via data aggregation across multiple dimensions or columns.","title":"On-Line Analytical Processing (OLAP)"},{"location":"glossary/dictionary/#p","text":"","title":"P"},{"location":"glossary/dictionary/#parzen-rosenblatt-window-technique","text":"[ back to top ] A non-parametric kernel density estimation technique for probability densities of random variables if the underlying distribution/model is unknown. A so-called window function is used to count samples within hypercubes or Gaussian kernels of a specified volume to estimate the probability density. \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} for a hypercube of unit length 1 centered at the coordinate system's origin. What this function basically does is assigning a value 1 to a sample point if it lies within \u00bd of the edges of the hypercube, and 0 if lies outside (note that the evaluation is done for all dimensions of the sample point). If we extend on this concept, we can define a more general equation that applies to hypercubes of any length h n that are centered at x : k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ where: \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) probability density estimation with hypercube kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] where: h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k probability density estimation with Gaussian kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg]","title":"Parzen-Rosenblatt Window technique"},{"location":"glossary/dictionary/#pattern-classification","text":"[ back to top ] The usage of patterns in datasets to discriminate between classes, i.e., to assign a class label to a new observation based on inference.","title":"Pattern classification"},{"location":"glossary/dictionary/#perceptron","text":"[ back to top ] A (single-layer) perceptron is a simple Artificial Neural Network algorithm that consists of only two types of nodes: Input nodes and output nodes connected by weighted links. Perceptrons are being used as linear classifiers in supervised machine learning tasks.","title":"Perceptron"},{"location":"glossary/dictionary/#permissive-transformations","text":"[ back to top ] Permissive transformations are transformations of data that that do not change the \"meaning\" of the attributes, such as scaling or mapping. For example, the transformation of temperature measurements from a Celsius to a Kelvin scale would be a permissive transformation of a numerical attribute.","title":"Permissive transformations"},{"location":"glossary/dictionary/#poisson-distribution-univariate","text":"[ back to top ] Probability density function p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!}","title":"Poisson distribution (univariate)"},{"location":"glossary/dictionary/#population-mean","text":"[ back to top ] \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i example mean vector: \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}","title":"Population mean"},{"location":"glossary/dictionary/#power-transform","text":"[ back to top ] Power transforms form a category of statistical transformation techniques that are used to transform non-normal distributed data to normality.","title":"Power transform"},{"location":"glossary/dictionary/#principal-component-analysis-pca","text":"[ back to top ] A linear transformation technique that is commonly used to project a dataset (without utilizing class labels) onto a new feature space or feature subspace (for dimensionality reduction) where the new component axes are the directions that maximize the variance/spread of the data. Scatter matrix S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T where: \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)}","title":"Principal Component Analysis (PCA)"},{"location":"glossary/dictionary/#precision-and-recall","text":"[ back to top ] Precision (synonymous to specificity ) and recall (synonymous to sensitivity ) are two measures to assess performance of a classifier if class label distributions are skewed. Precision is defined as the ratio of number of relevant items out of total retrieved items, whereas recall is the fraction of relevant items which are retrieved.","title":"Precision and Recall"},{"location":"glossary/dictionary/#predictive-modeling","text":"[ back to top ] Predictive modeling a data mining task for predicting outcomes based on a statistical model that was build on previous observations (in contrast to descriptive modeling ). Predictive modeling can be further divided into the three sub-tasks: Regression, classification, and ranking.","title":"Predictive Modeling"},{"location":"glossary/dictionary/#proportion-of-variance-explained","text":"[ back to top ] In the context of dimensionality reduction, the proportion of variance explained (PVE) describes how much of the total variance is captured by the new selected axes, for example, principal components or discriminant axes. It is computed by the sum of variance of new component axes divided by the total variance.","title":"Proportion of Variance Explained"},{"location":"glossary/dictionary/#purity-measure","text":"[ back to top ] In a cluster analysis with given truth cluster memberships (or classes), \"purity\" is used to assess the effectiveness of clustering. Purity is measured by assigning each cluster to the class that is maximally represented and computed via the weighted average of maximum number of samples from the same class in each cluster.","title":"Purity Measure"},{"location":"glossary/dictionary/#q","text":"","title":"Q"},{"location":"glossary/dictionary/#quantitative-and-qualitative-attributes","text":"[ back to top ] Quantitative attributes are also often called \"numeric\"; those are attributes for which calculations and comparisons like ratios and intervals make sense (e.g., temperature in Celsius). Qualitative, or \"categorical\", attributes can be grouped into to subclasses: nominal and ordinal. Where ordinal attributes (e.g., street numbers) can be ordered, nominal attributes can only distinguished by their category names (e.g., colors).","title":"Quantitative and qualitative attributes"},{"location":"glossary/dictionary/#r","text":"","title":"R"},{"location":"glossary/dictionary/#r-factor","text":"[ back to top ] The R-factor is one of several measures to assess the quality of a protein crystal structure. After building and refining an atomistic model of the crystal structure, the R-factor measures how well this model can describe the experimental diffraction patterns via the equation: R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|}","title":"R-factor"},{"location":"glossary/dictionary/#random-forest","text":"[ back to top ] Random forest is an ensemble classifier where multiple decision tree classifiers are learned and combined via the bagging technique. Unseen/test objects are then classified by taking the majority of votes from individual decision trees.","title":"Random forest"},{"location":"glossary/dictionary/#rayleigh-distribution-univariate","text":"[ back to top ] Probability density function p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array}","title":"Rayleigh distribution (univariate)"},{"location":"glossary/dictionary/#receiver-operating-characteristic-roc","text":"[ back to top ] The Receiver Operating Characteristic (ROC, or ROC curve) is a quality measure for binary prediction algorithms by plotting the \"False positive rate\" vs. the \"True positive rate\" ( sensitivity ).","title":"Receiver Operating Characteristic (ROC)"},{"location":"glossary/dictionary/#regularization","text":"[ back to top ] Regularization is a technique to overcome overfitting by introducing a penalty term for model complexity. Usually, the penalty term is the squared sum of the model parameters, thereby promoting less complex models during training. Regularization may increase the training error but can potentially reduce the classification error on the test dataset.","title":"Regularization"},{"location":"glossary/dictionary/#reinforcement-learning","text":"[ back to top ] Reinforcement learning is a machine learning algorithm that learns from a series of actions by maximizing a \"reward function\". The reward function can either be maximized by penalizing \"bad actions\" and/or rewarding \"good actions\".","title":"Reinforcement learning"},{"location":"glossary/dictionary/#rejection-sampling","text":"[ back to top ] Rejection sampling is similar to the popular Monte Carlo sampling with the difference of an additional bound. The goal of rejection sampling is to simplify the task of drawing random samples from a complex probability distribution by using a uniform distribution instead; random samples drawn from the uniform distribution that lie outside certain boundary criteria are rejected, and all samples within the boundary are accepted, respectively.","title":"Rejection sampling"},{"location":"glossary/dictionary/#resubstitution-error","text":"[ back to top ] The resubstitution error represents the classification error rate on the training dataset (the dataset that was used to train the classifier). The performance of a classifier cannot be directly deduced from resubstitution error alone, but it becomes a useful measure for calculating the generalization error .","title":"Resubstitution error"},{"location":"glossary/dictionary/#ridge-regression","text":"[ back to top ] Ridge regression is a regularized regression technique in which the squared sum of the model coefficients is used to penalize model complexity.","title":"Ridge Regression"},{"location":"glossary/dictionary/#rule-based-classifier","text":"[ back to top ] Rule-based classifiers are classifiers that are based on one or more \"IF ... THEN ...\" rules. Rule-based classifiers are related to decision trees and can be extracted from the latter. If the requirements for a rule-based classifier (mutually exclusive: at most one rule per sample; mutuallyy exhaustive: at least one rule per sample) are violated, possible remedies include the addition of rules or the ordering of rules.","title":"Rule-based classifier"},{"location":"glossary/dictionary/#s","text":"","title":"S"},{"location":"glossary/dictionary/#sampling","text":"[ back to top ] Sampling is data pre-processing procedure that is used to reduce the overall size of a dataset and to reduce computational costs by selecting a representative subset from the whole input dataset.","title":"Sampling"},{"location":"glossary/dictionary/#sensitivity","text":"[ back to top ] Sensitivity (synonymous to precision ), which is related to specificity -- in the context of error rate evaluation -- describes the \"True Positive Rate\" for a binary classification problem: The probability to make a correct prediction for a \"positive/true\" case (e.g., in an attempt to predict a disease, the disease is correctly predicted for a patient who truly has this disease). Sensitivity is calculated as (TP)/(TP+FN), where TP=True Positives, FN=False Negatives.","title":"Sensitivity"},{"location":"glossary/dictionary/#sharding","text":"[ back to top ] Sharding is the non-redundant partitioning of a database into smaller databases; this process can also be understood as horizontal splitting. The rationale behind sharing is to divide a database among separate machines to avoid storage or performance issues that are related to growing database sizes.","title":"Sharding"},{"location":"glossary/dictionary/#silhouette-measure-clustering","text":"[ back to top ] Silhouette measure provides a metric to evaluate the performance of a clustering analysis. For each data point i , it measures the average distance of point i to all other points in the same cluster (a(i)) , and the minimum distance to points from other clusters (b(i)) . The average silhouette measures for each cluster can provide a visual way to pick the proper number of clusters.","title":"Silhouette Measure (clustering)"},{"location":"glossary/dictionary/#simple-matching-coefficient","text":"[ back to top ] The simple matching coefficient is a similarity measure for binary data and calculated by dividing the total number of matches by the total number of attributes. For asymmetric binary data, the related Jaccard coefficient is to be preferred in order to avoid highly similar scores.","title":"Simple Matching Coefficient"},{"location":"glossary/dictionary/#singular-value-decomposition-svd","text":"[ back to top ] Singular value decomposition (SVD) is linear algebra technique that decomposes matrix X into U D V T where U (left-singular vectors) and V (right-singular vector) are both column-orthogonal, and D is a diagonal matrix that contains singular values. PCA is closely related to the right0singular vectors of SVD.","title":"Singular Value Decomposition (SVD)"},{"location":"glossary/dictionary/#soft-classification","text":"[ back to top ] The general goal of a pattern classification is to assign a pre-defined class labels to particular observations. Typically, in (hard) classification, only one class label is assigned to every instance whereas in soft classification, an instance can have multiple class labels. The degree to which an instance belongs to different classes is then defined by a so-called membership function.","title":"Soft classification"},{"location":"glossary/dictionary/#specificity","text":"[ back to top ] Specificity (synonymous to recall ), which is related to sensitivity -- in the context of error rate evaluation -- describes the \"True Negative Rate\" for a binary classification problem: The probability to make a correct prediction for a \"false/negative\" case (e.g., in an attempt to predict a disease, no disease is predicted for a healthy patient). Specificity is calculated as (TN)/(FP+TN), where TN=True Negatives, FP=False Positives.","title":"Specificity"},{"location":"glossary/dictionary/#standard-deviation","text":"[ back to top ] \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}","title":"Standard deviation"},{"location":"glossary/dictionary/#stochastic-gradient-descent-sgd","text":"[ back to top ] Stochastic Gradient Descent (SGD) (also see Gradient Descent ) is a machine learning algorithm that seeks to minimize an objective (or cost) function and can be grouped into the category of linear classifiers for supervised learning tasks. In contrast to Batch Gradient Descent , the gradient is computed from a single sample.","title":"Stochastic Gradient Descent (SGD)"},{"location":"glossary/dictionary/#supervised-learning","text":"[ back to top ] The problem of inferring a mapping between the input space X and a target variable y when given labelled training data (i.e. (X,y) pairs). Encompasses the problems of classification (categorical y) and regression (continuous y).","title":"Supervised Learning"},{"location":"glossary/dictionary/#support-vector-machine","text":"[ back to top ] SMV is a classification method that tries to find the hyperplane which separates classes with highest margin. The margin is defined as the minimum distance from sample points to the hyperplane. The sample point(s) that form margin are called support vectors and eventually establish the SVM model.","title":"Support Vector Machine"},{"location":"glossary/dictionary/#t","text":"","title":"T"},{"location":"glossary/dictionary/#term-frequency-and-document-frequency","text":"[ back to top ] Term frequency and document frequency are commonly used measures in context of text classification tasks. Term frequency is the count of how often a particular word occurs in a particular document. In contrast, document frequency measures the presence or absence of a particular word in a document as a binary value. Thus, for a single document, the document frequency is either 1 or 0.","title":"Term frequency and document frequency"},{"location":"glossary/dictionary/#term-frequency-inverse-document-frequency-tf-idf","text":"[ back to top ] Term frequency - inverse document frequency (Tf-idf) is a weighting scheme for term frequencies and document frequencies in text classification tasks that favors terms that occur in relatively few documents. The Tf-idf is calculated as a simple product of term frequency and the inverse document frequency, and the latter is calculated is calculated by log(\"number of documents in total\" / \"number of documents that contain a particular term\").","title":"Term frequency - inverse document frequency, Tf-idf"},{"location":"glossary/dictionary/#tokenization","text":"[ back to top ] Tokenization, in the context of natural language processing (NLP) is the process of breaking down a text into individual elements which can consist of words or symbols. Tokenization is usually accompanied by other processing procedures such as stemming, the removal of stop words, or the creation of n-grams.","title":"Tokenization"},{"location":"glossary/dictionary/#u","text":"","title":"U"},{"location":"glossary/dictionary/#unsupervised-learning","text":"[ back to top ] The problem of inferring latent structure in data when not given any training cases. Encompasses the problems of clustering, dimensionality reduction and density estimation.","title":"Unsupervised Learning"},{"location":"glossary/dictionary/#v","text":"","title":"V"},{"location":"glossary/dictionary/#variance","text":"[ back to top ] \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad","title":"Variance"},{"location":"glossary/dictionary/#w","text":"","title":"W"},{"location":"glossary/dictionary/#white-noise","text":"[ back to top ] White noise is a source that produces random, statistically independent variables following a particular distribution. In the field of sound processing, white noise is also often referred to as a mixture of tones or sounds of different frequencies.","title":"White noise"},{"location":"glossary/dictionary/#whitening-transformation","text":"[ back to top ] Whitening transformation is a normalization procedure to de-correlate samples in a dataset if the covariance matrix is not a diagonal matrix. Features are uncorrelated after \"whitening\" and their variances are equal unity, thus the covariance matrix becomes an identity matrix.","title":"Whitening transformation"},{"location":"glossary/dictionary/#z","text":"","title":"Z"},{"location":"glossary/dictionary/#z-score","text":"[ back to top ] z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma}","title":"Z-score"}]}