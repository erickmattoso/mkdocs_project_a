{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Braskem"},{"location":"intro/","text":"Introduction \u00b6 O futuro j\u00e1 chegou! O mundo anunciado por Philip Dick em suas obras como Minority Report (1956), Andr\u00f3ides Sonham Com Carneiros El\u00e9tricos? (1968) e Ubik (1969), previa um futuro com autom\u00f3veis aut\u00f4nomos, sistema de predi\u00e7\u00e3o de crimes, rob\u00f4s com intelig\u00eancia sofisticada capazes n\u00e3o s\u00f3 de aconselhar as personagens mas como tamb\u00e9m a trabalhar por elas ou at\u00e9 mesmo a encontrar o par perfeito. Aquilo que nos anos 60 parecia apenas ilus\u00f5es de livros de fic\u00e7\u00e3o cient\u00edfica hoje s\u00e3o realidades encontradas no nosso dia a dia. Essas tecnologias se tornaram real gra\u00e7as aos avan\u00e7os tecnol\u00f3gicos de processamento de informa\u00e7\u00e3o e principalmente com nossa capacidade de produ\u00e7\u00e3o de dados. De acordo com a 30\u00aa Pesquisa Anual de Administra\u00e7\u00e3o realizada pela Funda\u00e7\u00e3o Get\u00falio Vargas de S\u00e3o Paulo (FGV-SP), existe em m\u00e9dia 230 milh\u00f5es de celulares ativos no Brasil, cada um produzindo em m\u00e9dia 1,7 Megabytes por segundo, o que resulta em m\u00e9dia 370 terabytes de dados pessoais como fotos, conversas, \u00e1udios e diversas outras informa\u00e7\u00f5es que n\u00f3s compartilhamos. Para dar conta dessa revolu\u00e7\u00e3o tecnol\u00f3gica e entender o funcionamento dessa nova din\u00e2mica, uma nova ci\u00eancia surgiu, a ci\u00eancia de dados. A ci\u00eancia de dados \u00e9 o estudo que busca interpretar informa\u00e7\u00f5es para a tomada de decis\u00f5es mais precisas, seja para compreender os comportamentos humanos ou para entregar uma maior efic\u00e1cia da produtividade e desempenho das organiza\u00e7\u00f5es. A Braskem, frente aos desafios de competitividade do mercado, tem desenvolvido projetos de intelig\u00eancia computacional para aperfei\u00e7oar a sua produ\u00e7\u00e3o fazendo uso de dados com o objetivo de entregar produtos de melhor qualidade para seus cliente. E a partir disso a Braskem quer tamb\u00e9m divulgar o conhecimento absorvido. Esse livro tem como objetivo apresentar e discutir o processo de desenvolvimento de um projeto de an\u00e1lise e intelig\u00eancia de neg\u00f3cios, de forma a ajudar na implementa\u00e7\u00e3o mais r\u00e1pida e efetiva.","title":"Introduction"},{"location":"intro/#introduction","text":"O futuro j\u00e1 chegou! O mundo anunciado por Philip Dick em suas obras como Minority Report (1956), Andr\u00f3ides Sonham Com Carneiros El\u00e9tricos? (1968) e Ubik (1969), previa um futuro com autom\u00f3veis aut\u00f4nomos, sistema de predi\u00e7\u00e3o de crimes, rob\u00f4s com intelig\u00eancia sofisticada capazes n\u00e3o s\u00f3 de aconselhar as personagens mas como tamb\u00e9m a trabalhar por elas ou at\u00e9 mesmo a encontrar o par perfeito. Aquilo que nos anos 60 parecia apenas ilus\u00f5es de livros de fic\u00e7\u00e3o cient\u00edfica hoje s\u00e3o realidades encontradas no nosso dia a dia. Essas tecnologias se tornaram real gra\u00e7as aos avan\u00e7os tecnol\u00f3gicos de processamento de informa\u00e7\u00e3o e principalmente com nossa capacidade de produ\u00e7\u00e3o de dados. De acordo com a 30\u00aa Pesquisa Anual de Administra\u00e7\u00e3o realizada pela Funda\u00e7\u00e3o Get\u00falio Vargas de S\u00e3o Paulo (FGV-SP), existe em m\u00e9dia 230 milh\u00f5es de celulares ativos no Brasil, cada um produzindo em m\u00e9dia 1,7 Megabytes por segundo, o que resulta em m\u00e9dia 370 terabytes de dados pessoais como fotos, conversas, \u00e1udios e diversas outras informa\u00e7\u00f5es que n\u00f3s compartilhamos. Para dar conta dessa revolu\u00e7\u00e3o tecnol\u00f3gica e entender o funcionamento dessa nova din\u00e2mica, uma nova ci\u00eancia surgiu, a ci\u00eancia de dados. A ci\u00eancia de dados \u00e9 o estudo que busca interpretar informa\u00e7\u00f5es para a tomada de decis\u00f5es mais precisas, seja para compreender os comportamentos humanos ou para entregar uma maior efic\u00e1cia da produtividade e desempenho das organiza\u00e7\u00f5es. A Braskem, frente aos desafios de competitividade do mercado, tem desenvolvido projetos de intelig\u00eancia computacional para aperfei\u00e7oar a sua produ\u00e7\u00e3o fazendo uso de dados com o objetivo de entregar produtos de melhor qualidade para seus cliente. E a partir disso a Braskem quer tamb\u00e9m divulgar o conhecimento absorvido. Esse livro tem como objetivo apresentar e discutir o processo de desenvolvimento de um projeto de an\u00e1lise e intelig\u00eancia de neg\u00f3cios, de forma a ajudar na implementa\u00e7\u00e3o mais r\u00e1pida e efetiva.","title":"Introduction"},{"location":"courses/books/","text":"Books \u00b6 Last updated: 10/May/2020 Free Machine Learning eBooks \u00b6 Machine Learning, Neural and Statistical Classification http://www1.maths.leeds.ac.uk/~charles/statlog/ Authors: D. Michie, D.J. Spiegelhalter, C.C. Taylor (eds) Year: 1994 Pages: 290 pages Book Description: This book is based on the EC (ESPRIT) project StatLog which compare and evaluated a range of classification techniques, with an assessment of their merits, disadvantages and range of application. This integrated volume provides a concise introduction to each method, and reviews comparative trials in large-scale commercial and industrial problems. It makes accessible to a wide range of workers the complex issue of classification as approached through machine learning, statistics and neural networks, encouraging a cross-fertilization between these disciplines. Inductive Logic Programming Techniques and Applications http://www-ai.ijs.si/SasoDzeroski/ILPBook/ Authors: Nada Lavrac and Saso Dzeroski Year: 1994 Pages: 400 pages Book Description: This book is an introduction to inductive logic programming (ILP), a research field at the intersection of machine learning and logic programming, which aims at a formal framework as well as practical algorithms for inductively learning relational descriptions in the form of logic programs. The book extensively covers empirical inductive logic programming, one of the two major subfields of ILP, which has already shown its application potential in the following areas: knowledge acquisition, inductive program synthesis, inductive data engineering, and knowledge discovery in databases. The book provides the reader with an in-depth understanding of empirical ILP techniques and applications. It is divided into four parts. Part I is an introduction to the field of ILP. Part II describes in detail empirical ILP techniques and systems. Part III presents the techniques of handling imperfect data in ILP, whereas Part IV gives an overview of empirical ILP applications. Practical Artificial Intelligence Programming With Java http://markwatson.com/opencontent_data/JavaAI3rd.pdf Author: Mark Watson Year: 2008 Pages: 210 pages Book Description: I wrote this book for both professional programmers and home hobbyists who al- ready know how to program in Java and who want to learn practical Artificial In- telligence (AI) programming and information processing techniques. I have tried to make this an enjoyable book to work through. In the style of a \u201ccook book,\u201d the chapters can be studied in any order. Each chapter follows the same pattern: a mo- tivation for learning a technique, some theory for the technique, and a Java example program that you can experiment with. Information Theory, Inference, and Learning Algorithms http://www.inference.phy.cam.ac.uk/itila/book.html Author: David J.C. MacKay Year: 2003 Pages: 628 pages Book Description: This book is aimed at senior undergraduates and graduate students in Engi- neering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or second- year undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beauti- ful theoretical ideas of Shannon, but also practical solutions to communica- tion problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single field, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning. A Course in Machine Learning http://ciml.info Author: Hal Daume\u0301 III Year: 2012 Pages: 189 pages Book Description: Machine learning is a broad and fascinating field. It has been called one of the sexiest fields to work in1. It has applications in an incredibly wide variety of application areas, from medicine to advertising, from military to pedestrian. Its importance is likely to grow, as more and more areas turn to it as a way of dealing with the massive amounts of data available. The purpose of this book is to provide a gentle and pedagogically orga- nized introduction to the field. This is in contrast to most existing ma- chine learning texts, which tend to organize things topically, rather than pedagogically (an exception is Mitchell\u2019s book2, but unfortu- nately that is getting more and more outdated). This makes sense for researchers in the field, but less sense for learners. A second goal of this book is to provide a view of machine learning that focuses on ideas and models, not on math. It is not possible (or even advisable) to avoid math. But math should be there to aid understanding, not hinder it. Finally, this book attempts to have minimal dependencies, so that one can fairly easily pick and choose chapters to read. When dependencies exist, they are listed at the start of the chapter, as well as the list of dependencies at the end of this chapter. The audience of this book is anyone who knows differential calcu- lus and discrete math, and can program reasonably well. (A little bit of linear algebra and probability will not hurt.) An undergraduate in their fourth or fifth semester should be fully capable of understand- ing this material. However, it should also be suitable for first year graduate students, perhaps at a slightly faster pace. Bayesian Reasoning and Machine Learning http://www.cs.ucl.ac.uk/staff/d.barber/brml/ Author: David Barber Year: 2014 Pages: 648 pages Book Description: The book begins with the basic concepts of graphical models and inference. For the independent reader chapters 1,2,3,4,5,9,10,13,14,15,16,17,21 and 23 would form a good introduction to probabilistic reasoning, modelling and Machine Learning. The material in chapters 19, 24, 25 and 28 is more advanced, with the remaining material being of more specialised interest. Note that in each chapter the level of material is of varying difficulty, typically with the more challenging material placed towards the end of each chapter. As an introduction to the area of probabilistic modelling, a course can be constructed from the material as indicated in the chart. The material from parts I and II has been successfully used for courses on Graphical Models. I have also taught an introduction to Probabilistic Machine Learning using material largely from part III, as indicated. These two courses can be taught separately and a useful approach would be to teach first the Graphical Models course, followed by a separate Probabilistic Machine Learning course. A short course on approximate inference can be constructed from introductory material in part I and the more advanced material in part V, as indicated. The exact inference methods in part I can be covered relatively quickly with the material in part V considered in more in depth. A timeseries course can be made by using primarily the material in part IV, possibly combined with material from part I for students that are unfamiliar with probabilistic modelling approaches. Some of this material, particularly in chapter 25 is more advanced and can be deferred until the end of the course, or considered for a more advanced course. The references are generally to works at a level consistent with the book material and which are in the most part readily available. Introduction to Machine Learning http://arxiv.org/pdf/0904.3664.pdf Author: Amnon Shashua Year: 2008 Pages: 105 pages Book Description: A nice introductory book that covers the most important and basic topics: Bayesian Decision Theory, Maximum Likelihood/ Maximum Entropy Duality, EM Algorithm: ML over Mixture of Distributions, Support Vector Machines and Kernel Functions, Spectral Analysis I: PCA, LDA, CCA, Spectral Analysis II: Clustering ... The Elements of Statistical Learning: Data Mining, Inference, and Prediction http://statweb.stanford.edu/~tibs/ElemStatLearn/ Authors: \ufffcTrevor Hastie, Robert Tibshirani, Jerome Fried Year: 2009 Pages: 763 pages Book Description: During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data (italics p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap . Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. Gaussian Processes for Machine Learning http://www.gaussianprocess.org/gpml/chapters/ Authors: Carl Edward Rasmussen and Christopher K. I. Williams Year: 2006 Pages: 266 pages Book Description: Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes. Reinforcement Learning: An Introduction http://www.cse.wustl.edu/~kilian/introductions/reinforcement_learning.pdf Authors: Richard S. Sutton and Andrew G. Barto Year: 1998 Pages: 322 pages Book Description: Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning. Machine Learning http://www.intechopen.com/books/machine_learning Authors: Abdelhamid Mellouk and Abdennacer Chebira Year: 2009 Pages: 450 pages Book Description: Machine Learning can be defined in various ways related to a scientific domain concerned with the design and development of theoretical and implementation tools that allow building systems with some Human Like intelligent behavior. Machine learning addresses more specifically the ability to improve automatically through experience. Reinforcement Learning http://www.intechopen.com/books/reinforcement_learning Authors: Cornelius Weber, Mark Elshaw and Norbert Michael Mayer Year: 2008 Pages: 434 pages Book Description: Brains rule the world, and brain-like computation is increasingly used in computers and electronic devices. Brain-like computation is about processing and interpreting data or directly putting forward and performing actions. Learning is a very important aspect. This book is on reinforcement learning which involves performing actions to achieve a goal. The first 11 chapters of this book describe and extend the scope of reinforcement learning. The remaining 11 chapters show that there is already wide usage in numerous fields. Reinforcement learning can tackle control tasks that are too complex for traditional, hand-designed, non-learning controllers. As learning computers can deal with technical complexities, the tasks of human operators remain to specify goals on increasingly higher levels. This book shows that reinforcement learning is a very dynamic area in terms of theory and applications and it shall stimulate and encourage new research in this field. An Introduction to Statistical Learning with Applications in R http://www-bcf.usc.edu/%7Egareth/ISL/ Authors: Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani Year: 2013 Pages: 426 pages Book Description: This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist.","title":"Books"},{"location":"courses/books/#books","text":"Last updated: 10/May/2020","title":"Books"},{"location":"courses/books/#free-machine-learning-ebooks","text":"Machine Learning, Neural and Statistical Classification http://www1.maths.leeds.ac.uk/~charles/statlog/ Authors: D. Michie, D.J. Spiegelhalter, C.C. Taylor (eds) Year: 1994 Pages: 290 pages Book Description: This book is based on the EC (ESPRIT) project StatLog which compare and evaluated a range of classification techniques, with an assessment of their merits, disadvantages and range of application. This integrated volume provides a concise introduction to each method, and reviews comparative trials in large-scale commercial and industrial problems. It makes accessible to a wide range of workers the complex issue of classification as approached through machine learning, statistics and neural networks, encouraging a cross-fertilization between these disciplines. Inductive Logic Programming Techniques and Applications http://www-ai.ijs.si/SasoDzeroski/ILPBook/ Authors: Nada Lavrac and Saso Dzeroski Year: 1994 Pages: 400 pages Book Description: This book is an introduction to inductive logic programming (ILP), a research field at the intersection of machine learning and logic programming, which aims at a formal framework as well as practical algorithms for inductively learning relational descriptions in the form of logic programs. The book extensively covers empirical inductive logic programming, one of the two major subfields of ILP, which has already shown its application potential in the following areas: knowledge acquisition, inductive program synthesis, inductive data engineering, and knowledge discovery in databases. The book provides the reader with an in-depth understanding of empirical ILP techniques and applications. It is divided into four parts. Part I is an introduction to the field of ILP. Part II describes in detail empirical ILP techniques and systems. Part III presents the techniques of handling imperfect data in ILP, whereas Part IV gives an overview of empirical ILP applications. Practical Artificial Intelligence Programming With Java http://markwatson.com/opencontent_data/JavaAI3rd.pdf Author: Mark Watson Year: 2008 Pages: 210 pages Book Description: I wrote this book for both professional programmers and home hobbyists who al- ready know how to program in Java and who want to learn practical Artificial In- telligence (AI) programming and information processing techniques. I have tried to make this an enjoyable book to work through. In the style of a \u201ccook book,\u201d the chapters can be studied in any order. Each chapter follows the same pattern: a mo- tivation for learning a technique, some theory for the technique, and a Java example program that you can experiment with. Information Theory, Inference, and Learning Algorithms http://www.inference.phy.cam.ac.uk/itila/book.html Author: David J.C. MacKay Year: 2003 Pages: 628 pages Book Description: This book is aimed at senior undergraduates and graduate students in Engi- neering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or second- year undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beauti- ful theoretical ideas of Shannon, but also practical solutions to communica- tion problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single field, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning. A Course in Machine Learning http://ciml.info Author: Hal Daume\u0301 III Year: 2012 Pages: 189 pages Book Description: Machine learning is a broad and fascinating field. It has been called one of the sexiest fields to work in1. It has applications in an incredibly wide variety of application areas, from medicine to advertising, from military to pedestrian. Its importance is likely to grow, as more and more areas turn to it as a way of dealing with the massive amounts of data available. The purpose of this book is to provide a gentle and pedagogically orga- nized introduction to the field. This is in contrast to most existing ma- chine learning texts, which tend to organize things topically, rather than pedagogically (an exception is Mitchell\u2019s book2, but unfortu- nately that is getting more and more outdated). This makes sense for researchers in the field, but less sense for learners. A second goal of this book is to provide a view of machine learning that focuses on ideas and models, not on math. It is not possible (or even advisable) to avoid math. But math should be there to aid understanding, not hinder it. Finally, this book attempts to have minimal dependencies, so that one can fairly easily pick and choose chapters to read. When dependencies exist, they are listed at the start of the chapter, as well as the list of dependencies at the end of this chapter. The audience of this book is anyone who knows differential calcu- lus and discrete math, and can program reasonably well. (A little bit of linear algebra and probability will not hurt.) An undergraduate in their fourth or fifth semester should be fully capable of understand- ing this material. However, it should also be suitable for first year graduate students, perhaps at a slightly faster pace. Bayesian Reasoning and Machine Learning http://www.cs.ucl.ac.uk/staff/d.barber/brml/ Author: David Barber Year: 2014 Pages: 648 pages Book Description: The book begins with the basic concepts of graphical models and inference. For the independent reader chapters 1,2,3,4,5,9,10,13,14,15,16,17,21 and 23 would form a good introduction to probabilistic reasoning, modelling and Machine Learning. The material in chapters 19, 24, 25 and 28 is more advanced, with the remaining material being of more specialised interest. Note that in each chapter the level of material is of varying difficulty, typically with the more challenging material placed towards the end of each chapter. As an introduction to the area of probabilistic modelling, a course can be constructed from the material as indicated in the chart. The material from parts I and II has been successfully used for courses on Graphical Models. I have also taught an introduction to Probabilistic Machine Learning using material largely from part III, as indicated. These two courses can be taught separately and a useful approach would be to teach first the Graphical Models course, followed by a separate Probabilistic Machine Learning course. A short course on approximate inference can be constructed from introductory material in part I and the more advanced material in part V, as indicated. The exact inference methods in part I can be covered relatively quickly with the material in part V considered in more in depth. A timeseries course can be made by using primarily the material in part IV, possibly combined with material from part I for students that are unfamiliar with probabilistic modelling approaches. Some of this material, particularly in chapter 25 is more advanced and can be deferred until the end of the course, or considered for a more advanced course. The references are generally to works at a level consistent with the book material and which are in the most part readily available. Introduction to Machine Learning http://arxiv.org/pdf/0904.3664.pdf Author: Amnon Shashua Year: 2008 Pages: 105 pages Book Description: A nice introductory book that covers the most important and basic topics: Bayesian Decision Theory, Maximum Likelihood/ Maximum Entropy Duality, EM Algorithm: ML over Mixture of Distributions, Support Vector Machines and Kernel Functions, Spectral Analysis I: PCA, LDA, CCA, Spectral Analysis II: Clustering ... The Elements of Statistical Learning: Data Mining, Inference, and Prediction http://statweb.stanford.edu/~tibs/ElemStatLearn/ Authors: \ufffcTrevor Hastie, Robert Tibshirani, Jerome Fried Year: 2009 Pages: 763 pages Book Description: During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data (italics p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap . Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. Gaussian Processes for Machine Learning http://www.gaussianprocess.org/gpml/chapters/ Authors: Carl Edward Rasmussen and Christopher K. I. Williams Year: 2006 Pages: 266 pages Book Description: Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes. Reinforcement Learning: An Introduction http://www.cse.wustl.edu/~kilian/introductions/reinforcement_learning.pdf Authors: Richard S. Sutton and Andrew G. Barto Year: 1998 Pages: 322 pages Book Description: Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning. Machine Learning http://www.intechopen.com/books/machine_learning Authors: Abdelhamid Mellouk and Abdennacer Chebira Year: 2009 Pages: 450 pages Book Description: Machine Learning can be defined in various ways related to a scientific domain concerned with the design and development of theoretical and implementation tools that allow building systems with some Human Like intelligent behavior. Machine learning addresses more specifically the ability to improve automatically through experience. Reinforcement Learning http://www.intechopen.com/books/reinforcement_learning Authors: Cornelius Weber, Mark Elshaw and Norbert Michael Mayer Year: 2008 Pages: 434 pages Book Description: Brains rule the world, and brain-like computation is increasingly used in computers and electronic devices. Brain-like computation is about processing and interpreting data or directly putting forward and performing actions. Learning is a very important aspect. This book is on reinforcement learning which involves performing actions to achieve a goal. The first 11 chapters of this book describe and extend the scope of reinforcement learning. The remaining 11 chapters show that there is already wide usage in numerous fields. Reinforcement learning can tackle control tasks that are too complex for traditional, hand-designed, non-learning controllers. As learning computers can deal with technical complexities, the tasks of human operators remain to specify goals on increasingly higher levels. This book shows that reinforcement learning is a very dynamic area in terms of theory and applications and it shall stimulate and encourage new research in this field. An Introduction to Statistical Learning with Applications in R http://www-bcf.usc.edu/%7Egareth/ISL/ Authors: Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani Year: 2013 Pages: 426 pages Book Description: This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist.","title":"Free Machine Learning eBooks"},{"location":"courses/data-journey/","text":"Data Journey \u00b6 Mission Statement \u00b6 The Data Journey is a cycle of courses on Data Science offered by the Braskem Digital Factory , a team of scientists in computing, statistics and applied mathematics. The objective of the journey is clear: change your professional life . Here you will find, in four different courses, the content to develop practical and theoretical skills in the field of Data Science . You will learn about collection, transport, protection, storage and processing, data analysis, modeling and sharing, so at the end of the journey, students can offer a solution to problems in their area of expertise. Syllabus \u00b6 Track 1 Introduction to Data Science Who should take it? Everyone! If you wish to become a Data Scientist or if just want to get a more comprehensive overview of this field this should be your starting point. Effort Required: Minimal, plan to attend a 2 hours workshop. By the end of this track you will be able to: Understand what is Data Science and how you can derive value from it in your business; Have a basic intuition on how is a Data Science process conducted; Get familiar with Braskem's initiatives and other market benchmarks. Content What is Data Science Braskem's Initiaves & Market outlook Effort Workshop Track 2 Business Analyst Who should take it? Analysts, Coordinator or Managers in business areas that wish to get familiar with development of solutions. Professionals seeking tools to enhance productivity or to help them develop business solutions with data. Effort Required Duration: TBD By the end of this track you will be able to: - Have a practical knowledge of the tools for development of business solutions; - Ingest and process data in Python; - Know the concepts of different databases and cloud solutions, such as Data lake and data pipelines; - Get a basic concept of the process involved in developing and deploying a digital product; Content Developing Tools Anaconda Navigator Jupyter Notebook Spyder / Sublime / Visual Studio Introduction to Python Objects & Structure Logic Operators & Loops Classes, Methods & Functions Libraries (Pandas, Numpy & Matplotlib) Data Wrangling Importing Data Data Structure Tidying Data Combining Data Cleaning Data Software & Data Engineering | Azure Microsoft Azure Introduction SQL & NoSQL Databases Data Pipeline Standard Tables Architecture DevOps & Continuous Deployment Big Data with PySpark & Databricks (optional) Track 3 Data Analyst Who should take it? Professionals seeking a technical approach to Data driven decisions. Business units undergoing digital transformation where more data driven reports and analysis are required. Effort Required: Duration: TBD By the end of this track you will be able to: How to derive insights from data; Perform basic statistical analysis with available data; Build graphs and reports in Python; Know how to transform and choose the best features to enhance the analysis of data; Be equipped to provide your team with more data driven insights. Content Exploratory Data Analysis I Types of Variables Variable Summary | Statiscal Moments Correlation Matrix & ANOVA Feature Engineering Feature Selection Feature Creation & Transformation Data Normalization & Balance Data Interpolation Data Visualization Matplotlib & Seaborn Graph & Figure Types Track 4 Data Scientist Who should take it? Any professional aspiring to specialize in Data Science with development of Machine Learning algorithms and scientific data analysis. Effort Required: Duration: TBD By the end of this track you will be able to: Conduct scientific processes for data analysis; Develop Machine Learning algorithms; Deploy and deliver Data Science based digital products; Provide the business with huge data driven insights and strategies; Fully understand the intuition, math and code behind the main Machine Learning algorithms; Conduct explanatory data analysis to extract causation to enhance business decisions; Develop, deploy and maintain any Data Science project. Understand what are the best approaches and algorithms for each business problem; Content Linear Algebra Matrices & Vectors with Numpy Alternate Corrdinate Systems Probability Theory Probability Models & Axioms Conditioning & Independence Counting Discrete & Continuous Variables Distributions Bayesian Inference The Monte Carlo Simulation with Scipy Statistical Inference Sample Means & Central Limit Theorem Confidence Intervals & Hypothesis Testing Causality Analysis Linear Modeling & Experimental Design Exploratory Data Analysis II Hypothesis Creation & Testing Endogenous Variables Transformation Time Series Analysis Calculus (optional) Limits & Continuity Differentials & Derivatives Univariate Integration Multivariate Calculus Learning Techniques Supervised & Unsupervised Learning Regressions, Classifications & Clusterings Train & Test Sets | Dimensionality Model Scoring Regression Metrics Classification Metrics Clustering Metrics Cross Validation Back Propagation & Hyper Parameters Parameters vs Hyper Parameters Grid Search & Hyper Parameters Settings Back Propagation Main Algorithms Intuition Linear Regression (Regression) SVM (Regression & Classification) Random Forest (Regression & Classification) Logistic Regression (Classification) K-Means (Clustering) Special Models Boosting Neural Networks (Deep Learning) Reinforcement Learning Natural Language Processing (NLP) Computer Vision Disclaimer: The materials produced to the Data Journey does not contain Braskem sensitive data. Content \u00b6 Track 0 - Introduction to Data Journey \u00b6 Pt-br Track 0 - Introdu\u00e7\u00e3o \u00e0 Jornada de Dados t\u00edtulo video slides script exerc\u00edcios Data Journey - T0V1 - Data Journey - Apresenta\u00e7\u00e3o Data Journey - T0V2 - Data Journey - Apresenta\u00e7\u00e3o En Track 0 - Introduction to Data Journey TBD Track 1 - Introduction to Data Science \u00b6 Pt-br Track 1 - Introdu\u00e7\u00e3o \u00e0 Ci\u00eancia de Dados t\u00edtulo video slides script exercicios Data Journey - T1V1 - Introdu\u00e7\u00e3o a DS - Apresenta\u00e7\u00e3o Data Journey - T1V2 - Introdu\u00e7\u00e3o a DS - O que \u00e9 Ci\u00eancia de Dados Data Journey - T1V3 - Introdu\u00e7\u00e3o a DS - Iniciativas na Braskem Data Journey - T1V9 - Introdu\u00e7\u00e3o a DS - Exerc\u00edcio 1 - perguntas Data Journey - T1V9 - Introdu\u00e7\u00e3o a DS - Exerc\u00edcio 1 - respostas En Track 1 - Introduction to Data Science TBD Track 2 - Business Analyst \u00b6 Pt-br Track 2 - Analista de Nego\u0301cios t\u00edtulo video slides script exercicios Data Journey - T2V1 - Analista de Neg\u00f3cios - Apresenta\u00e7\u00e3o Data Journey - T2V2 - Analista de Neg\u00f3cios - Ferramentas de desenvolvimento Data Journey - T2V3 - Analista de neg\u00f3cios - Anaconda Data Journey - T2V4 - Analista de Neg\u00f3cios - Exerc\u00edcios 1 En Track 2 - Business Analyst TBD Track 3 - Data Analyst \u00b6 Pt-br Track 3 - Analista de Dados TBD En Track 3 - Data Analyst TBD Track 4 - Data Scientist \u00b6 Pt-br Track 4 - Cientista de Dados TBD En Track 4 - Data Scientist TBD","title":"Data Journey"},{"location":"courses/data-journey/#data-journey","text":"","title":"Data Journey"},{"location":"courses/data-journey/#mission-statement","text":"The Data Journey is a cycle of courses on Data Science offered by the Braskem Digital Factory , a team of scientists in computing, statistics and applied mathematics. The objective of the journey is clear: change your professional life . Here you will find, in four different courses, the content to develop practical and theoretical skills in the field of Data Science . You will learn about collection, transport, protection, storage and processing, data analysis, modeling and sharing, so at the end of the journey, students can offer a solution to problems in their area of expertise.","title":"Mission Statement"},{"location":"courses/data-journey/#syllabus","text":"Track 1 Introduction to Data Science Who should take it? Everyone! If you wish to become a Data Scientist or if just want to get a more comprehensive overview of this field this should be your starting point. Effort Required: Minimal, plan to attend a 2 hours workshop. By the end of this track you will be able to: Understand what is Data Science and how you can derive value from it in your business; Have a basic intuition on how is a Data Science process conducted; Get familiar with Braskem's initiatives and other market benchmarks. Content What is Data Science Braskem's Initiaves & Market outlook Effort Workshop Track 2 Business Analyst Who should take it? Analysts, Coordinator or Managers in business areas that wish to get familiar with development of solutions. Professionals seeking tools to enhance productivity or to help them develop business solutions with data. Effort Required Duration: TBD By the end of this track you will be able to: - Have a practical knowledge of the tools for development of business solutions; - Ingest and process data in Python; - Know the concepts of different databases and cloud solutions, such as Data lake and data pipelines; - Get a basic concept of the process involved in developing and deploying a digital product; Content Developing Tools Anaconda Navigator Jupyter Notebook Spyder / Sublime / Visual Studio Introduction to Python Objects & Structure Logic Operators & Loops Classes, Methods & Functions Libraries (Pandas, Numpy & Matplotlib) Data Wrangling Importing Data Data Structure Tidying Data Combining Data Cleaning Data Software & Data Engineering | Azure Microsoft Azure Introduction SQL & NoSQL Databases Data Pipeline Standard Tables Architecture DevOps & Continuous Deployment Big Data with PySpark & Databricks (optional) Track 3 Data Analyst Who should take it? Professionals seeking a technical approach to Data driven decisions. Business units undergoing digital transformation where more data driven reports and analysis are required. Effort Required: Duration: TBD By the end of this track you will be able to: How to derive insights from data; Perform basic statistical analysis with available data; Build graphs and reports in Python; Know how to transform and choose the best features to enhance the analysis of data; Be equipped to provide your team with more data driven insights. Content Exploratory Data Analysis I Types of Variables Variable Summary | Statiscal Moments Correlation Matrix & ANOVA Feature Engineering Feature Selection Feature Creation & Transformation Data Normalization & Balance Data Interpolation Data Visualization Matplotlib & Seaborn Graph & Figure Types Track 4 Data Scientist Who should take it? Any professional aspiring to specialize in Data Science with development of Machine Learning algorithms and scientific data analysis. Effort Required: Duration: TBD By the end of this track you will be able to: Conduct scientific processes for data analysis; Develop Machine Learning algorithms; Deploy and deliver Data Science based digital products; Provide the business with huge data driven insights and strategies; Fully understand the intuition, math and code behind the main Machine Learning algorithms; Conduct explanatory data analysis to extract causation to enhance business decisions; Develop, deploy and maintain any Data Science project. Understand what are the best approaches and algorithms for each business problem; Content Linear Algebra Matrices & Vectors with Numpy Alternate Corrdinate Systems Probability Theory Probability Models & Axioms Conditioning & Independence Counting Discrete & Continuous Variables Distributions Bayesian Inference The Monte Carlo Simulation with Scipy Statistical Inference Sample Means & Central Limit Theorem Confidence Intervals & Hypothesis Testing Causality Analysis Linear Modeling & Experimental Design Exploratory Data Analysis II Hypothesis Creation & Testing Endogenous Variables Transformation Time Series Analysis Calculus (optional) Limits & Continuity Differentials & Derivatives Univariate Integration Multivariate Calculus Learning Techniques Supervised & Unsupervised Learning Regressions, Classifications & Clusterings Train & Test Sets | Dimensionality Model Scoring Regression Metrics Classification Metrics Clustering Metrics Cross Validation Back Propagation & Hyper Parameters Parameters vs Hyper Parameters Grid Search & Hyper Parameters Settings Back Propagation Main Algorithms Intuition Linear Regression (Regression) SVM (Regression & Classification) Random Forest (Regression & Classification) Logistic Regression (Classification) K-Means (Clustering) Special Models Boosting Neural Networks (Deep Learning) Reinforcement Learning Natural Language Processing (NLP) Computer Vision Disclaimer: The materials produced to the Data Journey does not contain Braskem sensitive data.","title":"Syllabus"},{"location":"courses/data-journey/#content","text":"","title":"Content"},{"location":"courses/data-journey/#track-0-introduction-to-data-journey","text":"Pt-br Track 0 - Introdu\u00e7\u00e3o \u00e0 Jornada de Dados t\u00edtulo video slides script exerc\u00edcios Data Journey - T0V1 - Data Journey - Apresenta\u00e7\u00e3o Data Journey - T0V2 - Data Journey - Apresenta\u00e7\u00e3o En Track 0 - Introduction to Data Journey TBD","title":"Track 0 - Introduction to Data Journey"},{"location":"courses/data-journey/#track-1-introduction-to-data-science","text":"Pt-br Track 1 - Introdu\u00e7\u00e3o \u00e0 Ci\u00eancia de Dados t\u00edtulo video slides script exercicios Data Journey - T1V1 - Introdu\u00e7\u00e3o a DS - Apresenta\u00e7\u00e3o Data Journey - T1V2 - Introdu\u00e7\u00e3o a DS - O que \u00e9 Ci\u00eancia de Dados Data Journey - T1V3 - Introdu\u00e7\u00e3o a DS - Iniciativas na Braskem Data Journey - T1V9 - Introdu\u00e7\u00e3o a DS - Exerc\u00edcio 1 - perguntas Data Journey - T1V9 - Introdu\u00e7\u00e3o a DS - Exerc\u00edcio 1 - respostas En Track 1 - Introduction to Data Science TBD","title":"Track 1 - Introduction to Data Science"},{"location":"courses/data-journey/#track-2-business-analyst","text":"Pt-br Track 2 - Analista de Nego\u0301cios t\u00edtulo video slides script exercicios Data Journey - T2V1 - Analista de Neg\u00f3cios - Apresenta\u00e7\u00e3o Data Journey - T2V2 - Analista de Neg\u00f3cios - Ferramentas de desenvolvimento Data Journey - T2V3 - Analista de neg\u00f3cios - Anaconda Data Journey - T2V4 - Analista de Neg\u00f3cios - Exerc\u00edcios 1 En Track 2 - Business Analyst TBD","title":"Track 2 - Business Analyst"},{"location":"courses/data-journey/#track-3-data-analyst","text":"Pt-br Track 3 - Analista de Dados TBD En Track 3 - Data Analyst TBD","title":"Track 3 - Data Analyst"},{"location":"courses/data-journey/#track-4-data-scientist","text":"Pt-br Track 4 - Cientista de Dados TBD En Track 4 - Data Scientist TBD","title":"Track 4 - Data Scientist"},{"location":"glossary/dictionary/","text":"Glossary \u00b6 Last updated: 12 / May / 2020 Table of Contents \u00b6 Accuracy Active Learning Anomaly detection Artificial Neural Networks (ANN) Backtesting Bagging Bag of words Batch Gradient Descent Bayes Theorem Batch Learning Big Data Binomial distribution Bootstrapping Bregman divergence Central Limit Theorem Co-Variance Confusion Matrix Contingency Table Correlation analysis Correlation analysis, Canonical Correlation analysis - Matthews Correlation Coefficient (MCC) Correlation analysis - Kendall Correlation analysis - Pearson Correlation analysis - Spearman Cosine Similarity Cost function Cross-validation Cross-validation, K-fold Cross-validation, Leave-One-Out Cross-validation, Random Sampling Curse of dimensionality Data mining DBSCAN Decision rule Decision tree classifier Density-based clustering Descriptive modeling Dimensionality reduction Distance Metric Learning Distance, Euclidean Distance, Manhattan Distance, Minkowski Eager learners Eigenvectors and Eigenvalues Ensemble methods Evolutionary algorithms Exhaustive search Expectation Maximization algorithm - EM Feature Selection Feature Space Fuzzy C-Means Clustering Generalization error Genetic algorithm Gradient Descent Greedy Algorithm Heuristic search Hyperparameters iid Imputation Independent Component Analysis Jaccard coefficient Jackknifing Jittering k-D Trees Kernel Density Estimation Kernel (in statistics) Kernel Methods Kernel Trick k-fold Cross-validation K-Means Clustering K-Means++ Clustering K-Medoids Clustering K-nearest neighbors algorithms Knowledge Discovery in Databases (KDD) LASSO Regression Latent Semantic Indexing Law of Large Numbers Lazy learners Least Squares fit Lennard-Jones Potential Linear Discriminant Analysis Linear Discriminant Analysis (LDA) Local Outlier Factor (LOF) Locality-sensitive hashing (LSH) Logistic Regression Machine learning Mahalanobis distance MapReduce Markov chains Monte Carlo simulation Maximum Likelihood Estimates (MLE) Min-Max scaling MinHash Naive Bayes Classifier N-grams Non-parametric statistics Normal distribution (multivariate) Normal distribution (univariate) Normal Modes Normalization - Min-Max scaling Normalization - Standard Scores Objective function On-Line Learning On-Line Analytical Processing (OLAP) Parzen-Rosenblatt Window technique Pattern classification Perceptron Permissive transformations Poisson distribution (univariate) Population mean Power transform Principal Component Analysis (PCA) Precision and Recall Predictive Modeling Proportion of Variance Explained Purity Measure Quantitative and qualitative attributes R-factor Random forest Rayleigh distribution (univariate) Receiver Operating Characteristic (ROC) Regularization Reinforcement learning Rejection sampling Resubstitution error Ridge Regression Rule-based classifier Sampling Sensitivity Sharding Silhouette Measure (clustering) Simple Matching Coefficient Singular Value Decomposition (SVD) Soft classification Specificity Standard deviation Stochastic Gradient Descent (SGD) Supervised Learning Support Vector Machine Term frequency and document frequency Term frequency - inverse document frequency, Tf-idf Tokenization Unsupervised Learning Variance White noise Whitening transformation Z-score A \u00b6 Accuracy \u00b6 [ back to top ] Accuracy is defined as the fraction of correct classifications out of the total number of samples; it resembles one way to assess the performance of a predictor and is often used synonymous to specificity / precision although it is calculated differently. Accuracy is calculated as: \\frac{True Positives + True Negatives}{Positives+Negatives} \\frac{True Positives + True Negatives}{Positives+Negatives} Source: wikipedia Active Learning \u00b6 [ back to top ] Active learning is a variant of the on-line learning machine learning architecture where feedback about the ground truth class labels of unseen data can be requested if the classification is uncertain. New training data that was labeled can then be used to update the model as in on-line learning . Anomaly detection \u00b6 [ back to top ] Anomaly detection describes the task of identifying points that deviate from specific patterns in a dataset -- the so-called outliers. Different types of anomaly detection methods include graph-based, statistical-based and distance-based techniques and can be used in both unsupervised and supervised learning tasks. Artificial Neural Networks (ANN) \u00b6 [ back to top ] Artificial Neural Networks (ANN) are a class of machine learning algorithms that are inspired by the neuron architecture of the human brain. Typically, a (multi-layer) ANN consists of a layer of input nodes, a layer of output nodes, and hidden layers in-between. The nodes are connected by weighted links that can be interpreted as the neuron-connections by axons of different strengths. The simplest version of an ANN is a single-layer perceptron . B \u00b6 Backtesting \u00b6 [ back to top ] Backtesting is a specific case of cross-validation in the context of finance and trading models where empirical data from previous time periods (data from the past) is used to evaluate a trading strategy. Bagging \u00b6 [ back to top ] Bagging is an ensemble method for classification (or regression analysis) in which individual models are trained by random sampling of data, and the final decision is made by voting among individual models with equal weights (or averaging for regression analysis). Bag of words \u00b6 [ back to top ] Bag of words is a model that is used to construct sparse feature vectors for text classification tasks. The bag of words is an unordered set of all words that occur in all documents that are part of the training set. Every word is then associated with a count of how often it occurs whereas the positional information is ignored. Sometimes, the bag of words is also called \"dictionary\" or \"vocabulary\" based on the training data. Batch Gradient Descent \u00b6 [ back to top ] Batch Gradient descent is a variant of a Gradient Descent algorithm to optimize a function by finding its local minimum. In contrast to Stochastic Gradient Descent the gradient is computed from the whole dataset. Bayes Theorem \u00b6 [ back to top ] Naive Bayes' classifier: posterior probability: P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} decision rule: \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} objective functions: g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) Batch Learning \u00b6 [ back to top ] Batch learning is an architecture used in machine learning tasks where the entire training dataset is available upfront to build the model. In contrast to on-line learning , the model is not updated once it was build on a training dataset. Big Data \u00b6 [ back to top ] There are many different, controversial interpretations and definitions for the term \"Big Data\". Typically, one refers to data as \"Big Data\" if its volume and complexity are of a magnitude that the data cannot be processed by \"conventional\" computing platforms anymore; storage space, processing power, and database structures are typically among the limiting factors. Binomial distribution \u00b6 [ back to top ] Probability density function: p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} Bootstrapping \u00b6 [ back to top ] A resampling technique to that is closely related to cross-validation where a training dataset is divided into random subsets. Bootstrapping -- in contrast to cross-validation -- is a random sampling with replacement. Bootstrapping is typically used for statistical estimation of bias and standard error, and a common application in machine learning is to estimate the generalization error of a predictor. Bregman divergence \u00b6 [ back to top ] Bregman divergence describes are family of proximity functions (or distance measures) that share common properties and are often used in clustering algorithms. A popular example is the squared Euclidean distance. C \u00b6 Central Limit Theorem \u00b6 [ back to top ] The Central Limit Theorem is a theorem in the field of probability theory that expresses the idea that the distribution of sample means (from independent random variables) converges to a normal distribution when the sample size approaches infinity. Co-Variance \u00b6 [ back to top ] S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) example covariance matrix: \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} Confusion Matrix \u00b6 [ back to top ] The confusion matrix is used as a way to represent the performance of a classifier and is sometimes also called \"error matrix\". This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios. Contingency Table \u00b6 [ back to top ] A contingency table is used in clustering analysis to compare the overlap between two different clustering (grouping) results. The partitions from the two clustering results are represented as rows and columns in the table, and the individual elements of the table represent the number of elements that are shared between two partitions from each clustering result. Correlation analysis \u00b6 [ back to top ] Correlation analysis describes and quantifies the relationship between two independent variables. Typically, in case of a positive correlation both variables have a tendency to increase, and in the case of negative correlation, one variable increases while the other variable increases. It is important to mention the famous quotation \"correlation does not imply causation\". Correlation analysis, Canonical \u00b6 Let x and y be two vectors, the goal of canonical correlation analysis is to maximize the correlation between linear transformations of those original vectors. With applications in dimensionality reduction and feature selection, CCA tries to find common dimensions between two vectors. Correlation analysis - Matthews Correlation Coefficient (MCC) \u00b6 [ back to top ] MCC is an assessment metric for clustering or binary classification analyses that represents the correlation between the observed (ground truth) and predicted labels. MCC can be directly computed from the confusion matrix and returns a value between -1 and 1. Correlation analysis - Kendall \u00b6 [ back to top ] Similar to the Pearson correlation coefficient , Kendall's tau measures the degree of a monotone relationship between variables, and like Spearman's rho , it calculates the dependence between ranked variables, which makes it feasible for non-normal distributed data. Kendall tau can be calculated for continuous as well as ordinal data. Roughly speaking, Kendall's tau distinguishes itself from Spearman's rho by stronger penalization of non-sequential (in context of the ranked variables) dislocations. \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} where: c = the number of concordant pairs d = the number of discordant pairs [ Source ] If ties are present among the 2 ranked variables, the following equation shall be used instead: \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ where: t = number of observations of variable x that are tied u = number of observations of variable y that are tied Correlation analysis - Pearson \u00b6 [ back to top ] The Pearson correlation coefficient is probably the most widely used measure for linear relationships between two normal distributed variables and thus often just called \"correlation coefficient\". Usually, the Pearson coefficient is obtained via a Least-Squares fit and a value of 1 represents a perfect positive relation-ship, -1 a perfect negative relationship, and 0 indicates the absence of a relationship between variables. \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} And the estimate r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} Correlation analysis - Spearman \u00b6 [ back to top ] Related to the Pearson correlation coefficient , the Spearman correlation coefficient (rho) measures the relationship between two variables. Spearman's rho can be understood as a rank-based version of Pearson's correlation coefficient , which can be used for variables that are not normal-distributed and have a non-linear relationship. Also, its use is not only restricted to continuous data, but can also be used in analyses of ordinal attributes. \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} where: d = the pairwise distances of the ranks of the variables x i and y i . n = the number of samples. Cosine Similarity \u00b6 [ back to top ] Cosine similarity measures the orientation of two n -dimensional sample vectors irrespective to their magnitude. It is calculated by the dot product of two numeric vectors, and it is normalized by the product of the vector lengths, so that output values close to 1 indicate high similarity. cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} Cost function \u00b6 [ back to top ] A cost function (synonymous to loss function) is a special case of an objective function , i.e., a function that is used for solving optimization problems. A cost function can take one or more input variables and the output variable is to be minimized. A typical use case for cost functions is parameter optimization. Cross-validation \u00b6 [ back to top ] Cross-validation is a statistical technique to estimate the prediction error rate by splitting the data into training, cross-validation, and test datasets. A prediction model is obtained using the training set, and model parameters are optimized by the cross-validation set, while the test set is held primarily for empirical error estimation. Cross-validation, K-fold \u00b6 [ back to top ] K-fold cross-validation is a variant of cross validation where contiguous segments of samples are selected from the training dataset to build two new subsets for every iteration (without replacement): a new training and test dataset (while the original test dataset is retained for the final evaluation of the predictor). Cross-validation, Leave-One-Out \u00b6 [ back to top ] Leave-One-Out cross-validation a variant of cross validation one sample is removed for every iteration (without replacement). The model is trained on the remaining N-1 samples and evaluated via the removed sample (while the original test dataset is retained for the final evaluation of the predictor). Cross-validation, Random Sampling \u00b6 [ back to top ] Cross-validation via random sampling is a variant of cross validation where random chunks of samples are extracted from the training dataset to build two new subsets for every iteration (with or without replacement): a new training and test dataset for every iteration (while the original test dataset is retained for the final evaluation of the predictor). Curse of dimensionality \u00b6 [ back to top ] For a fixed number of training samples, the curse of dimensionality describes the increased error rate for a large number of dimensions (or features) due to imprecise parameter estimations. D \u00b6 Data mining \u00b6 [ back to top ] A field that is closely related to machine learning and pattern classification. The focus of data mining does not lie in merely the collection of data, but the extraction of useful information: Discovery of patterns, and making inferences and predictions. Common techniques in data mining include predictive modeling, clustering, association rules, and anomaly detection. DBSCAN \u00b6 [ back to top ] DBSCAN is a variant of a density-based clustering algorithm that identifies core points as regions of high-densities based on their number of neighbors (> MinPts ) in a specified radius (\u03b5). Points that are below MinPts but within \u03b5 are specified as border points; the remaining points are classified as noise points. Decision rule \u00b6 [ back to top ] A function in pattern classification tasks of making an \"action\", e.g., assigning a certain class label to an observation or pattern. Decision tree classifier \u00b6 [ back to top ] Decision tree classifiers are tree like graphs, where nodes in the graph test certain conditions on a particular set of features, and branches split the decision towards the leaf nodes. Leaves represent lowest level in the graph and determine the class labels. Optimal tree are trained by minimizing Gini impurity, or maximizing information gain. Density-based clustering \u00b6 [ back to top ] In density-based clustering, regions of high density in n-dimensional space are identified as clusters. The best advantage of this class of clustering algorithms is that they do not require apriori knowledge of number of clusters (as opposed to k-means algorithm). Descriptive modeling \u00b6 [ back to top ] Descriptive modeling is a common task in the field of data mining where a model is build in order to distinguish between objects and categorize them into classes - a form of data summary. In contrast to predictive modeling , the scope of descriptive modeling does not extend to making prediction for unseen objects. Dimensionality reduction \u00b6 [ back to top ] Dimensionality reduction is a data pre-processing step in machine learning applications that aims to avoid the curse of dimensionality and reduce the effect of overfitting. Dimensionality reduction is related to feature selection , but instead of selecting a feature subset, dimensionality reduction takes as projection-based approach (e.g, linear transformation) in order to create a new feature subspace. Distance Metric Learning \u00b6 [ back to top ] Distance metrics are fundamental for many machine learning algorithms. Distance metric learning - instead of learning a model - incorporates estimated relevances of features to obtain a distance metric for potentially optimal separation of classes and clusters: Large distances for objects from different classes, and small distances for objects of the same class, respectively. Distance, Euclidean \u00b6 [ back to top ] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras' theorem. The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension. \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} Distance, Manhattan \u00b6 [ back to top ] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance, but instead of calculating the shortest diagonal path (\"beeline\") between two points, it calculates the distance based on gridlines. The Manhattan distance was named after the block-like layout of the streets in Manhattan. \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} Distance, Minkowski \u00b6 [ back to top ] The Minkowski distance is a generalized form of the Euclidean distance (if p=2 ) and the Manhattan distance (if p=1 ). \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} E \u00b6 Eager learners \u00b6 [ back to top ] Eager learners (in contrast to lazy learners ) describe machine learning algorithms that learn a model for mapping attributes to class labels as soon as the data becomes available (e.g., Decision tree classifiers or naive Bayes classifiers ) and do not require the training data for making predictions on unseen samples once the model was built. The most computationally expensive step is the creation of a prediction model from the training data, and the actual prediction is considered as relatively inexpensive. Eigenvectors and Eigenvalues \u00b6 [ back to top ] Both eigenvectors and eigenvalues fundamental in many applications involve linear systems and are related via A\u00b7v = \u03bb\u00b7v (where A is a square matrix, v the eigenvector, and \u03bb the eigenvalue). Eigenvectors are describing the direction of the axes of a linear transformation, whereas eigenvalues are describing the scale or magnitude. \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ where: \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} Ensemble methods \u00b6 [ back to top ] Ensemble methods combine multiple classifiers which may differ in algorithms, input features, or input samples. Statistical analyses showed that ensemble methods yield better classification performances and are also less prone to overfitting. Different methods, e.g., bagging or boosting, are used to construct the final classification decision based on weighted votes. Evolutionary algorithms \u00b6 [ back to top ] Evolutionary algorithms are a class of algorithms that are based on heuristic search methods inspired by biological evolution in order to solve optimization problems. Exhaustive search \u00b6 [ back to top ] Exhaustive search (synonymous to brute-force search) is a problem-solving approach where all possible combinations are sequentially evaluated to find the optimal solution. Exhaustive search guarantees to find the optimal solution whereas other approaches (e.g., heuristic searches ) are regarded as sub-optimal. A downside of exhaustive searches is that computational costs increase proportional to the number of combinations to be evaluated. Expectation Maximization algorithm - EM \u00b6 [ back to top ] The Expectation Maximization algorithm (EM) is a technique to estimate parameters of a distribution based on the Maximum Likelihood Estimate (MLE) that is often used for the imputation of missing values in incomplete datasets. After the EM algorithm is initialized with a starting value, alternating iterations between expectation and maximization steps are repeated until convergence. In the expectation step, parameters are estimated based on the current model to impute missing values. In the maximization step, the log-likelihood function of the statistical model is to be maximized by re-estimating the parameters based on the imputed values from the expectation step. F \u00b6 Feature Selection \u00b6 [ back to top ] Feature selection is an important pre-processing step in many machine learning applications in order to avoid the curse of dimensionality and overfitting . A subset of features is typically selected by evaluating different combinations of features and eventually retain the subset that minimizes a specified cost function . Commonly used algorithms for feature selection as alternative to exhaustive search algorithms include sequential selection algorithms and genetic algorithms. Feature Space \u00b6 [ back to top ] A feature space describes the descriptive variables that are available for samples in a dataset as a d -dimensional Euclidean space. E.g., sepal length and width, and petal length and width for each flower sample in the popular Iris dataset. Fuzzy C-Means Clustering \u00b6 [ back to top ] Fuzzy C-Means is a soft clustering algorithm in which each sample point has a membership degree to each cluster; in hard (crisp) clustering, membership of each point to each cluster is either 0 or 1. Fuzzy C-Means considers a weight matrix for cluster memberships, and minimizes sum squared error (SSE) of weighted distances of sample points to the cluster centroids. G \u00b6 Generalization error \u00b6 [ back to top ] The generalization error describes how well new data can be classified and is a useful metric to assess the performance of a classifier. Typically, the generalization error is computed via cross-validation or simply the absolute difference between the error rate on the training and test dataset. Genetic algorithm \u00b6 [ back to top ] The Genetic algorithm is a subclass of evolutionary algorithms that takes a heuristic approach inspired by Charles Darwin's theory of \"natural selection\" in order to solve optimization problems. Gradient Descent \u00b6 [ back to top ] Gradient descent is an algorithm that optimizes a function by finding its local minimum. After the algorithm was initialized with an initial guess, it takes the derivative of the function to make a step towards the direction of deepest descent. This step-wise process is repeated until convergence. Greedy Algorithm \u00b6 [ back to top ] Greedy Algorithms are a family of algorithms that are used in optimization problems. A greedy algorithm makes locally optimal choices in order to find a local optimum (suboptimal solution, also see ( heuristic problem solving ). H \u00b6 Heuristic search \u00b6 [ back to top ] Heuristic search is a problem-solving approach that is focussed on efficiency rather than completeness in order to find a suboptimal solution to a problem. Heuristic search is often used as alternative approach where exhaustive search is too computationally intensive and where solutions need to be approximated. Hyperparameters \u00b6 [ back to top ] Hyperparameters are the parameters of a classifier or estimator that are not directly learned in the machine learning step from the training data but are optimized separately (e.g., via Grid Search ). The goal of hyperparameter optimization is to achieve good generalization of a learning algorithm and to avoid overfitting to the training data. I \u00b6 iid \u00b6 [ back to top ] The abbreviation \"iid\" stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another variable (e.g., time series and network graphs are not independent). One popular example of iid would be the tossing of a coin: One coin toss does not affect the outcome of another coin toss, and the probability of the coin landing on either \"heads\" or \"tails\" is the same for every coin toss. Imputation \u00b6 [ back to top ] Imputations algorithms are designed to replace the missing data (NAs) with certain statistics rather than discarding them for downstream analysis. Commonly used imputation methods include mean imputation (replacement by the sample mean of an attribute), kNN imputation, and regression imputation. Independent Component Analysis \u00b6 [ back to top ] Independent Component Analysis (ICA) is a statistical signal-processing technique that decomposes a multivariate dataset of mixed, non-gaussian distributed source signals into independent components. A popular example is the separation of overlapping voice samples -- the so-called \"cocktail party problem\". J \u00b6 Jaccard coefficient \u00b6 [ back to top ] The Jaccard coefficient (bounded at [0, 1)) is used as similarity measure for asymmetric binary data and calculated by taking the number of matching attributes and divide it by the number of all attributes except those where both variables have a value 0 in contrast to a simple matching coefficient . A popular application is the identification of near-duplicate documents for which the Jaccard coefficient can be calculated by the dividing the intersection of the set of words by the union of the set words in both documents. Jackknifing \u00b6 [ back to top ] Jackknifing is a resampling technique that predates the related cross-validation and bootstrapping techniques and is mostly used for bias and variance estimations. In jackknifing, a dataset is split into N subsets where exactly one sample is removed from every subset so that every subset is of size N-1. Jittering \u00b6 [ back to top ] Jittering is a sampling technique that can be used to measure the stability of a given statistical model (classifiction/regression/clustering). In jittering, some noise is added to sample data points, and then a new model is drawn and compared to the original model. K \u00b6 k-D Trees \u00b6 [ back to top ] k-D trees are a data structures (recursive space partitioning trees) that result from the binary partitioning of multi-dimensional feature spaces. A typical application of k-D trees is to increase the search efficiency for nearest-neighbor searches. A k-D tree construction can be described as a iterating process with the following steps: Select the dimension of largest variance, draw a cutting plane based at the median along the dimension to split the data into 2 halves, choose the next dimension. Kernel Density Estimation \u00b6 [ back to top ] Non-parametric techniques to estimate probability densities from the available data without requiring prior knowledge of the underlying model of the probability distribution. Kernel (in statistics) \u00b6 [ back to top ] In the context of [kernel methods](#kernel-methods the term \u201ckernel\u201d describes a function that calculates the dot product of the images of the samples x under the kernel function \u03c6 (see kernel methods). Roughly speaking, a kernel can be understood as a similarity measure in higher-dimensional space. Kernel Methods \u00b6 [ back to top ] Kernel methods are algorithms that map the sample vectors of a dataset onto a higher-dimensional feature space via a so-called kernel function (\u03c6(x)). The goal is to identify and simplify general relationships between data, which is especially useful for linearly non-separable datasets. Kernel Trick \u00b6 [ back to top ] Since the explicit computation of the kernel is increasingly computationally expensive for large sample sizes and high numbers of dimensions, the kernel trick uses approximations to calculate the kernel implicitly. The most popular kernels used for the kernel trick are Gaussian Radius Basis Function (RBF) kernels, sigmoidal kernels, and polynomial kernels. k-fold Cross-validation \u00b6 [ back to top ] In k-fold cross-validation the data is split into k subsets, then a prediction/classification model is trained k times, each time holding one subset as test set, training the model parameters using the remaining k -1. Finally, cross-validation error is evaluated as the average error out of all k training models. K-Means Clustering \u00b6 [ back to top ] A method of partitioning a dataset into k clusters by picking k random initial points (where k < n , the number or total points - modified by S.R. ), assigning clusters, averaging, reassigning, and repeating until stability is achieved. The number k must be chosen beforehand. K-Means++ Clustering \u00b6 [ back to top ] A variant of k-means where instead of choosing all initial centers randomly, the first is chosen randomly, the second chosen with probability proportional to the squared distance from the first, the third chosen with probability proportional to the square distance from the first two, etc. See this paper . K-Medoids Clustering \u00b6 [ back to top ] K-Medoids clustering is a variant of k-means algorithm in which cluster centroids are picked among the sample points rather than the mean point of each cluster. K-Medoids can overcome some of the limitations of k-means algorithm by avoiding empty clusters, being more robust to outliers, and being more easily applicable to non-numeric data types. K-nearest neighbors algorithms \u00b6 [ back to top ] K-nearest neighbors algorithms find the k-points that are closest to a point of interest based on their attributes using a certain distance measure (e.g., Euclidean distance). K-nearest neighbors algorithms are being used in many different contexts: Non-parametric density estimation, missing value imputation, dimensionality reduction, and classifiers in supervised and unsupervised pattern classification and regression problems. Knowledge Discovery in Databases (KDD) \u00b6 [ back to top ] Knowledge Discovery in Databases (KDD) describes a popular workflow including data mining for extracting useful and meaningful information out of data. Typically, the individual steps are feature selection, pre-processing, transformation, data mining , and post-processing (evaluation and interpretation). L \u00b6 LASSO Regression \u00b6 [ back to top ] LASSO (Least Absolute Shrinkage and Selection Operator) is a regression model that uses the L1-norm (sum of absolute values) of model coefficients to penalize the model complexity. LASSO has the advantage that some coefficients can become zero, as opposed to ridge regression that uses the squared sum of model coefficients. Latent Semantic Indexing \u00b6 [ back to top ] Latent Semantic Indexing (LSI) is a data mining technique to characterize documents by topics, word usage, or other contexts. The structures of the documents are compared by applying singular value decomposition to an input term-document matrix (e.g., a data table of word counts with terms as row labels and document numbers as column labels) in order to obtain the singular values and vectors. Law of Large Numbers \u00b6 [ back to top ] The Law of Large Numbers is a theorem in the field of probability theory that expresses the idea that the actual value of a random sampling process approaches the expected value for growing sample sizes. A common example is that the observed ratio of \"heads\" in an unbiased coin-flip experiment will approach 0.5 for large sample sizes. Lazy learners \u00b6 [ back to top ] Lazy learners (in contrast to eager learners ) are memorizing training data in order to make predictions for unseen samples. While there is no expensive learning step involved, the prediction step is generally considered to be more expensive compared to eager learners since it involves the evaluation of training data. One example of lazy learners are k-nearest neighbor algorithms where the class label of a unseen sample is estimated by e.g., the majority of class labels of its neighbors in the training data. Least Squares fit \u00b6 [ back to top ] A linear regression technique that fits a straight line to a data set (or overdetermined system) by minimizing the sum of the squared residuals, which can be the minimized vertical or perpendicular offsets from the fitted line. Linear equation f(x) = a\\cdot x + b f(x) = a\\cdot x + b f(x) = a\\cdot x + b Slope: a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad Y-axis intercept: b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad where: S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} Matrix equation \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y Lennard-Jones Potential \u00b6 [ back to top ] The Lennard-Jones potential describes the energy potential between two non-bonded atoms based on their distance to each other. V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = intermolecular potential \u03c3 = distance where V is 0 r = distance between atoms, measured from one center to the other \u03b5 = interaction strength Linear Discriminant Analysis \u00b6 [ back to top ] In-between class scatter matrix S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i Where: S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ and \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} Between class scatter matrix S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T Linear Discriminant Analysis (LDA) \u00b6 [ back to top ] A linear transformation technique (related to Principal Component Analysis) that is commonly used to project a dataset onto a new feature space or feature subspace, where the new component axes maximize the spread between multiple classes, or for classification of data. Local Outlier Factor (LOF) \u00b6 [ back to top ] LOF is a density-based anomaly detection technique for outlier identification. The LOF for a point p refers to the average \"reachability distance\" towards its nearest neighbors. Eventually, the points with the largest LOF values (given a particular threshold) are identified as outliers. Locality-sensitive hashing (LSH) \u00b6 [ back to top ] Locality-sensitive hashing (LSH) is a dimensionality reduction technique that groups objects that are likely similar (based on a similarity signature such as MinHash ) into the same buckets in order to reduce the search space for pair-wise similarity comparisons. One application of LSH could be a combination with other dimensionality reduction techniques, e.g., MinHash , in order to reduce the computational costs of finding near-duplicate document pairs. Logistic Regression \u00b6 [ back to top ] Logistic regression is a statistical model used for binary classification (binomial logistic regression) where class labels are mapped to \"0\" or \"1\" outputs. Logistic regression uses the logistic function (a general form of sigmoid function), where its output ranges from (0-1). M \u00b6 Machine learning \u00b6 [ back to top ] A set of algorithmic instructions for discovering and learning patterns from data e.g., to train a classifier for a pattern classification task. Mahalanobis distance \u00b6 [ back to top ] The Mahalanobis distance measure accounts for the covariance among variables by calculating the distance between a sample x and the sample mean \u03bc in units of the standard deviation. The Mahalanobis distance becomes equal to the Euclidean distance for uncorrelated with same variances. MapReduce \u00b6 [ back to top ] MapRedcue is a programming model for analyzing large datasets on distributed computer clusters, in which the task is divided into two steps, a map step and a reducer step. In the map step, the data are filtered by some factors on each compute node, then filtered data are shuffled and passed to the reducer function which performs further analysis on each portion of filtered data separately. Markov chains \u00b6 [ back to top ] Markov chains (names after Andrey Markov) are mathematical systems that describe the transitioning between different states in a model. The transitioning from one state to the other (or back to itself) is a stochastic process. Monte Carlo simulation \u00b6 [ back to top ] A Monte Carlo simulation is an iterative sampling method for solving deterministic models. Random numbers or variables from a particular probability distribution are used as input variables for uncertain parameters to compute the response variables. Maximum Likelihood Estimates (MLE) \u00b6 [ back to top ] A technique to estimate the parameters that have been fit to a model by maximizing a known likelihood function. One common application is the estimation of \"mean\" and \"variance\" for a Gaussian distribution. D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} can be pictured as probability to observe a particular sequence of patterns, where the probability of observing a particular patterns depends on \u03b8 , the parameters the underlying (class-conditional) distribution. In order to apply MLE, we have to make the assumption that the samples are i.i.d. (independent and identically distributed). p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) Where \u03b8 is the parameter vector, that contains the parameters for a particular distribution that we want to estimate and p(D | \u03b8 ) is also called the likelihood of \u03b8 . log-likelihood p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) Differentiation \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} parameter vector \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] Min-Max scaling \u00b6 [ back to top ] X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} MinHash \u00b6 [ back to top ] MinHash is a commonly used technique for dimensionality reduction in document similarity comparisons. The idea behind MinHash is to create a signature of reduced dimensionality while preserving the Jaccard similarity coefficient . A common implementation of MinHash is to generate k random permutations of the columns in a m*x*n -document matrix (rows represent the sparse vectors of words for each document as binary data) and generate a new matrix of size m*x*k . The cells of the new matrix now contain the position labels of the first non-zero value for every document (1 column for each round of random permutation). Based on similarities of the position labels, the Jaccard coefficient for the pairs of documents can be calculated. N \u00b6 Naive Bayes Classifier \u00b6 [ back to top ] A classifier based on a statistical model (i.e., Bayes theorem: calculating posterior probabilities based on the prior probability and the so-called likelihood) in the field of pattern classification. Naive Bayes assumes that all attributes are conditionally independent, thereby, computing the likelihood is simplified to the product of the conditional probabilities of observing individual attributes given a particular class label. N-grams \u00b6 [ back to top ] In context of natural language processing (NLP), a text is typically broken down into individual elements (see tokenization ). N-grams describe the length of the individual elements where n refers to the number of words or symbols in every token. E.g., a unigram (or 1-gram) can represent a single word, and a bigram (or 2-gram) describes a token that consists of 2 words etc. Non-parametric statistics \u00b6 [ back to top ] In contrast to parametric approaches, non-parametric statistics or approaches do not make prior assumptions about the underlying probability distribution of a particular variable or attribute. Normal distribution (multivariate) \u00b6 [ back to top ] Probability density function p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] Normal distribution (univariate) \u00b6 [ back to top ] Probability density function p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } Normal Modes \u00b6 [ back to top ] Normal modes are the harmonic oscillations of a system of masses connected by springs, or roughly speaking \"concerted motions,\" and all normal modes of a system are independent from each other. A classic example describes two masses connected by a middle spring, and each mass is connected to a fixed outer edge ( | m1~~m2 |). The oscillation of this system where the middle spring does not move is defined as its normal mode. Normalization - Min-Max scaling \u00b6 [ back to top ] A data pre-processing step (also often referred to as \"Feature Scaling\") for fitting features from different measurements within a certain range, typically the unit range from 0 to 1. Normalization - Standard Scores \u00b6 [ back to top ] A data pre-processing step (also often just called \"Standardization\") for re-scaling features from different measurements to match proportions of a standard normal distribution (unit variance centered at mean=0). O \u00b6 Objective function \u00b6 [ back to top ] Objective functions are mathematical function that are used for problem-solving and optimization tasks. Depending on the task, the objective function can be omtpimized through minimization ( cost or loss functions ) or maximization (reward function). A typical application of an objective function in pattern classification tasks is to minimize the error rate of a classifier. On-Line Learning \u00b6 [ back to top ] On-line learning is a machine learning architecture where the model is being updated consecutively as new training data arrives in contrast to batch-learning , which requires the entire training dataset to be available upfront. On-line has the advantage that a model can be updated and refined over time to account for changes in the population of training samples. A popular example where on-line learning is beneficial is the task of spam detection. On-Line Analytical Processing (OLAP) \u00b6 [ back to top ] On-Line Analytical Processing (OLAP) describes the general process of working with multidimensional arrays for exploratory analysis and information retrieval; often, OLAP is used to create summary data, e.g., via data aggregation across multiple dimensions or columns. P \u00b6 Parzen-Rosenblatt Window technique \u00b6 [ back to top ] A non-parametric kernel density estimation technique for probability densities of random variables if the underlying distribution/model is unknown. A so-called window function is used to count samples within hypercubes or Gaussian kernels of a specified volume to estimate the probability density. \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} for a hypercube of unit length 1 centered at the coordinate system's origin. What this function basically does is assigning a value 1 to a sample point if it lies within \u00bd of the edges of the hypercube, and 0 if lies outside (note that the evaluation is done for all dimensions of the sample point). If we extend on this concept, we can define a more general equation that applies to hypercubes of any length h n that are centered at x : k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ where: \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) probability density estimation with hypercube kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] where: h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k probability density estimation with Gaussian kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] Pattern classification \u00b6 [ back to top ] The usage of patterns in datasets to discriminate between classes, i.e., to assign a class label to a new observation based on inference. Perceptron \u00b6 [ back to top ] A (single-layer) perceptron is a simple Artificial Neural Network algorithm that consists of only two types of nodes: Input nodes and output nodes connected by weighted links. Perceptrons are being used as linear classifiers in supervised machine learning tasks. Permissive transformations \u00b6 [ back to top ] Permissive transformations are transformations of data that that do not change the \"meaning\" of the attributes, such as scaling or mapping. For example, the transformation of temperature measurements from a Celsius to a Kelvin scale would be a permissive transformation of a numerical attribute. Poisson distribution (univariate) \u00b6 [ back to top ] Probability density function p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} Population mean \u00b6 [ back to top ] \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i example mean vector: \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} Power transform \u00b6 [ back to top ] Power transforms form a category of statistical transformation techniques that are used to transform non-normal distributed data to normality. Principal Component Analysis (PCA) \u00b6 [ back to top ] A linear transformation technique that is commonly used to project a dataset (without utilizing class labels) onto a new feature space or feature subspace (for dimensionality reduction) where the new component axes are the directions that maximize the variance/spread of the data. Scatter matrix S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T where: \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} Precision and Recall \u00b6 [ back to top ] Precision (synonymous to specificity ) and recall (synonymous to sensitivity ) are two measures to assess performance of a classifier if class label distributions are skewed. Precision is defined as the ratio of number of relevant items out of total retrieved items, whereas recall is the fraction of relevant items which are retrieved. Predictive Modeling \u00b6 [ back to top ] Predictive modeling a data mining task for predicting outcomes based on a statistical model that was build on previous observations (in contrast to descriptive modeling ). Predictive modeling can be further divided into the three sub-tasks: Regression, classification, and ranking. Proportion of Variance Explained \u00b6 [ back to top ] In the context of dimensionality reduction, the proportion of variance explained (PVE) describes how much of the total variance is captured by the new selected axes, for example, principal components or discriminant axes. It is computed by the sum of variance of new component axes divided by the total variance. Purity Measure \u00b6 [ back to top ] In a cluster analysis with given truth cluster memberships (or classes), \"purity\" is used to assess the effectiveness of clustering. Purity is measured by assigning each cluster to the class that is maximally represented and computed via the weighted average of maximum number of samples from the same class in each cluster. Q \u00b6 Quantitative and qualitative attributes \u00b6 [ back to top ] Quantitative attributes are also often called \"numeric\"; those are attributes for which calculations and comparisons like ratios and intervals make sense (e.g., temperature in Celsius). Qualitative, or \"categorical\", attributes can be grouped into to subclasses: nominal and ordinal. Where ordinal attributes (e.g., street numbers) can be ordered, nominal attributes can only distinguished by their category names (e.g., colors). R \u00b6 R-factor \u00b6 [ back to top ] The R-factor is one of several measures to assess the quality of a protein crystal structure. After building and refining an atomistic model of the crystal structure, the R-factor measures how well this model can describe the experimental diffraction patterns via the equation: R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} Random forest \u00b6 [ back to top ] Random forest is an ensemble classifier where multiple decision tree classifiers are learned and combined via the bagging technique. Unseen/test objects are then classified by taking the majority of votes from individual decision trees. Rayleigh distribution (univariate) \u00b6 [ back to top ] Probability density function p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} Receiver Operating Characteristic (ROC) \u00b6 [ back to top ] The Receiver Operating Characteristic (ROC, or ROC curve) is a quality measure for binary prediction algorithms by plotting the \"False positive rate\" vs. the \"True positive rate\" ( sensitivity ). Regularization \u00b6 [ back to top ] Regularization is a technique to overcome overfitting by introducing a penalty term for model complexity. Usually, the penalty term is the squared sum of the model parameters, thereby promoting less complex models during training. Regularization may increase the training error but can potentially reduce the classification error on the test dataset. Reinforcement learning \u00b6 [ back to top ] Reinforcement learning is a machine learning algorithm that learns from a series of actions by maximizing a \"reward function\". The reward function can either be maximized by penalizing \"bad actions\" and/or rewarding \"good actions\". Rejection sampling \u00b6 [ back to top ] Rejection sampling is similar to the popular Monte Carlo sampling with the difference of an additional bound. The goal of rejection sampling is to simplify the task of drawing random samples from a complex probability distribution by using a uniform distribution instead; random samples drawn from the uniform distribution that lie outside certain boundary criteria are rejected, and all samples within the boundary are accepted, respectively. Resubstitution error \u00b6 [ back to top ] The resubstitution error represents the classification error rate on the training dataset (the dataset that was used to train the classifier). The performance of a classifier cannot be directly deduced from resubstitution error alone, but it becomes a useful measure for calculating the generalization error . Ridge Regression \u00b6 [ back to top ] Ridge regression is a regularized regression technique in which the squared sum of the model coefficients is used to penalize model complexity. Rule-based classifier \u00b6 [ back to top ] Rule-based classifiers are classifiers that are based on one or more \"IF ... THEN ...\" rules. Rule-based classifiers are related to decision trees and can be extracted from the latter. If the requirements for a rule-based classifier (mutually exclusive: at most one rule per sample; mutuallyy exhaustive: at least one rule per sample) are violated, possible remedies include the addition of rules or the ordering of rules. S \u00b6 Sampling \u00b6 [ back to top ] Sampling is data pre-processing procedure that is used to reduce the overall size of a dataset and to reduce computational costs by selecting a representative subset from the whole input dataset. Sensitivity \u00b6 [ back to top ] Sensitivity (synonymous to precision ), which is related to specificity -- in the context of error rate evaluation -- describes the \"True Positive Rate\" for a binary classification problem: The probability to make a correct prediction for a \"positive/true\" case (e.g., in an attempt to predict a disease, the disease is correctly predicted for a patient who truly has this disease). Sensitivity is calculated as (TP)/(TP+FN), where TP=True Positives, FN=False Negatives. Sharding \u00b6 [ back to top ] Sharding is the non-redundant partitioning of a database into smaller databases; this process can also be understood as horizontal splitting. The rationale behind sharing is to divide a database among separate machines to avoid storage or performance issues that are related to growing database sizes. Silhouette Measure (clustering) \u00b6 [ back to top ] Silhouette measure provides a metric to evaluate the performance of a clustering analysis. For each data point i , it measures the average distance of point i to all other points in the same cluster (a(i)) , and the minimum distance to points from other clusters (b(i)) . The average silhouette measures for each cluster can provide a visual way to pick the proper number of clusters. Simple Matching Coefficient \u00b6 [ back to top ] The simple matching coefficient is a similarity measure for binary data and calculated by dividing the total number of matches by the total number of attributes. For asymmetric binary data, the related Jaccard coefficient is to be preferred in order to avoid highly similar scores. Singular Value Decomposition (SVD) \u00b6 [ back to top ] Singular value decomposition (SVD) is linear algebra technique that decomposes matrix X into U D V T where U (left-singular vectors) and V (right-singular vector) are both column-orthogonal, and D is a diagonal matrix that contains singular values. PCA is closely related to the right0singular vectors of SVD. Soft classification \u00b6 [ back to top ] The general goal of a pattern classification is to assign a pre-defined class labels to particular observations. Typically, in (hard) classification, only one class label is assigned to every instance whereas in soft classification, an instance can have multiple class labels. The degree to which an instance belongs to different classes is then defined by a so-called membership function. Specificity \u00b6 [ back to top ] Specificity (synonymous to recall ), which is related to sensitivity -- in the context of error rate evaluation -- describes the \"True Negative Rate\" for a binary classification problem: The probability to make a correct prediction for a \"false/negative\" case (e.g., in an attempt to predict a disease, no disease is predicted for a healthy patient). Specificity is calculated as (TN)/(FP+TN), where TN=True Negatives, FP=False Positives. Standard deviation \u00b6 [ back to top ] \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} Stochastic Gradient Descent (SGD) \u00b6 [ back to top ] Stochastic Gradient Descent (SGD) (also see Gradient Descent ) is a machine learning algorithm that seeks to minimize an objective (or cost) function and can be grouped into the category of linear classifiers for supervised learning tasks. In contrast to Batch Gradient Descent , the gradient is computed from a single sample. Supervised Learning \u00b6 [ back to top ] The problem of inferring a mapping between the input space X and a target variable y when given labelled training data (i.e. (X,y) pairs). Encompasses the problems of classification (categorical y) and regression (continuous y). Support Vector Machine \u00b6 [ back to top ] SMV is a classification method that tries to find the hyperplane which separates classes with highest margin. The margin is defined as the minimum distance from sample points to the hyperplane. The sample point(s) that form margin are called support vectors and eventually establish the SVM model. T \u00b6 Term frequency and document frequency \u00b6 [ back to top ] Term frequency and document frequency are commonly used measures in context of text classification tasks. Term frequency is the count of how often a particular word occurs in a particular document. In contrast, document frequency measures the presence or absence of a particular word in a document as a binary value. Thus, for a single document, the document frequency is either 1 or 0. Term frequency - inverse document frequency, Tf-idf \u00b6 [ back to top ] Term frequency - inverse document frequency (Tf-idf) is a weighting scheme for term frequencies and document frequencies in text classification tasks that favors terms that occur in relatively few documents. The Tf-idf is calculated as a simple product of term frequency and the inverse document frequency, and the latter is calculated is calculated by log(\"number of documents in total\" / \"number of documents that contain a particular term\"). Tokenization \u00b6 [ back to top ] Tokenization, in the context of natural language processing (NLP) is the process of breaking down a text into individual elements which can consist of words or symbols. Tokenization is usually accompanied by other processing procedures such as stemming, the removal of stop words, or the creation of n-grams. U \u00b6 Unsupervised Learning \u00b6 [ back to top ] The problem of inferring latent structure in data when not given any training cases. Encompasses the problems of clustering, dimensionality reduction and density estimation. V \u00b6 Variance \u00b6 [ back to top ] \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad W \u00b6 White noise \u00b6 [ back to top ] White noise is a source that produces random, statistically independent variables following a particular distribution. In the field of sound processing, white noise is also often referred to as a mixture of tones or sounds of different frequencies. Whitening transformation \u00b6 [ back to top ] Whitening transformation is a normalization procedure to de-correlate samples in a dataset if the covariance matrix is not a diagonal matrix. Features are uncorrelated after \"whitening\" and their variances are equal unity, thus the covariance matrix becomes an identity matrix. Z \u00b6 Z-score \u00b6 [ back to top ] z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma}","title":"Dictionary"},{"location":"glossary/dictionary/#glossary","text":"Last updated: 12 / May / 2020","title":"Glossary"},{"location":"glossary/dictionary/#table-of-contents","text":"Accuracy Active Learning Anomaly detection Artificial Neural Networks (ANN) Backtesting Bagging Bag of words Batch Gradient Descent Bayes Theorem Batch Learning Big Data Binomial distribution Bootstrapping Bregman divergence Central Limit Theorem Co-Variance Confusion Matrix Contingency Table Correlation analysis Correlation analysis, Canonical Correlation analysis - Matthews Correlation Coefficient (MCC) Correlation analysis - Kendall Correlation analysis - Pearson Correlation analysis - Spearman Cosine Similarity Cost function Cross-validation Cross-validation, K-fold Cross-validation, Leave-One-Out Cross-validation, Random Sampling Curse of dimensionality Data mining DBSCAN Decision rule Decision tree classifier Density-based clustering Descriptive modeling Dimensionality reduction Distance Metric Learning Distance, Euclidean Distance, Manhattan Distance, Minkowski Eager learners Eigenvectors and Eigenvalues Ensemble methods Evolutionary algorithms Exhaustive search Expectation Maximization algorithm - EM Feature Selection Feature Space Fuzzy C-Means Clustering Generalization error Genetic algorithm Gradient Descent Greedy Algorithm Heuristic search Hyperparameters iid Imputation Independent Component Analysis Jaccard coefficient Jackknifing Jittering k-D Trees Kernel Density Estimation Kernel (in statistics) Kernel Methods Kernel Trick k-fold Cross-validation K-Means Clustering K-Means++ Clustering K-Medoids Clustering K-nearest neighbors algorithms Knowledge Discovery in Databases (KDD) LASSO Regression Latent Semantic Indexing Law of Large Numbers Lazy learners Least Squares fit Lennard-Jones Potential Linear Discriminant Analysis Linear Discriminant Analysis (LDA) Local Outlier Factor (LOF) Locality-sensitive hashing (LSH) Logistic Regression Machine learning Mahalanobis distance MapReduce Markov chains Monte Carlo simulation Maximum Likelihood Estimates (MLE) Min-Max scaling MinHash Naive Bayes Classifier N-grams Non-parametric statistics Normal distribution (multivariate) Normal distribution (univariate) Normal Modes Normalization - Min-Max scaling Normalization - Standard Scores Objective function On-Line Learning On-Line Analytical Processing (OLAP) Parzen-Rosenblatt Window technique Pattern classification Perceptron Permissive transformations Poisson distribution (univariate) Population mean Power transform Principal Component Analysis (PCA) Precision and Recall Predictive Modeling Proportion of Variance Explained Purity Measure Quantitative and qualitative attributes R-factor Random forest Rayleigh distribution (univariate) Receiver Operating Characteristic (ROC) Regularization Reinforcement learning Rejection sampling Resubstitution error Ridge Regression Rule-based classifier Sampling Sensitivity Sharding Silhouette Measure (clustering) Simple Matching Coefficient Singular Value Decomposition (SVD) Soft classification Specificity Standard deviation Stochastic Gradient Descent (SGD) Supervised Learning Support Vector Machine Term frequency and document frequency Term frequency - inverse document frequency, Tf-idf Tokenization Unsupervised Learning Variance White noise Whitening transformation Z-score","title":"Table of Contents"},{"location":"glossary/dictionary/#a","text":"","title":"A"},{"location":"glossary/dictionary/#accuracy","text":"[ back to top ] Accuracy is defined as the fraction of correct classifications out of the total number of samples; it resembles one way to assess the performance of a predictor and is often used synonymous to specificity / precision although it is calculated differently. Accuracy is calculated as: \\frac{True Positives + True Negatives}{Positives+Negatives} \\frac{True Positives + True Negatives}{Positives+Negatives} Source: wikipedia","title":"Accuracy"},{"location":"glossary/dictionary/#active-learning","text":"[ back to top ] Active learning is a variant of the on-line learning machine learning architecture where feedback about the ground truth class labels of unseen data can be requested if the classification is uncertain. New training data that was labeled can then be used to update the model as in on-line learning .","title":"Active Learning"},{"location":"glossary/dictionary/#anomaly-detection","text":"[ back to top ] Anomaly detection describes the task of identifying points that deviate from specific patterns in a dataset -- the so-called outliers. Different types of anomaly detection methods include graph-based, statistical-based and distance-based techniques and can be used in both unsupervised and supervised learning tasks.","title":"Anomaly detection"},{"location":"glossary/dictionary/#artificial-neural-networks-ann","text":"[ back to top ] Artificial Neural Networks (ANN) are a class of machine learning algorithms that are inspired by the neuron architecture of the human brain. Typically, a (multi-layer) ANN consists of a layer of input nodes, a layer of output nodes, and hidden layers in-between. The nodes are connected by weighted links that can be interpreted as the neuron-connections by axons of different strengths. The simplest version of an ANN is a single-layer perceptron .","title":"Artificial Neural Networks (ANN)"},{"location":"glossary/dictionary/#b","text":"","title":"B"},{"location":"glossary/dictionary/#backtesting","text":"[ back to top ] Backtesting is a specific case of cross-validation in the context of finance and trading models where empirical data from previous time periods (data from the past) is used to evaluate a trading strategy.","title":"Backtesting"},{"location":"glossary/dictionary/#bagging","text":"[ back to top ] Bagging is an ensemble method for classification (or regression analysis) in which individual models are trained by random sampling of data, and the final decision is made by voting among individual models with equal weights (or averaging for regression analysis).","title":"Bagging"},{"location":"glossary/dictionary/#bag-of-words","text":"[ back to top ] Bag of words is a model that is used to construct sparse feature vectors for text classification tasks. The bag of words is an unordered set of all words that occur in all documents that are part of the training set. Every word is then associated with a count of how often it occurs whereas the positional information is ignored. Sometimes, the bag of words is also called \"dictionary\" or \"vocabulary\" based on the training data.","title":"Bag of words"},{"location":"glossary/dictionary/#batch-gradient-descent","text":"[ back to top ] Batch Gradient descent is a variant of a Gradient Descent algorithm to optimize a function by finding its local minimum. In contrast to Stochastic Gradient Descent the gradient is computed from the whole dataset.","title":"Batch Gradient Descent"},{"location":"glossary/dictionary/#bayes-theorem","text":"[ back to top ] Naive Bayes' classifier: posterior probability: P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} P(\\omega_j|x) = \\frac{p(x|\\omega_j) \\cdot P(\\omega_j)}{p(x)} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} \\Rightarrow \\text{posterior probability} = \\frac{ \\text{likelihood} \\cdot \\text{prior probability}}{\\text{evidence}} decision rule: \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\text{Decide } \\omega_1 \\text{ if } P(\\omega_1|x) > P(\\omega_2|x) \\text{ else decide } \\omega_2 . \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} \\frac{p(x|\\omega_1) \\cdot P(\\omega_1)}{p(x)} > \\frac{p(x|\\omega_2) \\cdot P(\\omega_2)}{p(x)} objective functions: g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) g_1(\\pmb x) = P(\\omega_1 | \\; \\pmb{x}), \\quad g_2(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}), \\quad g_3(\\pmb{x}) = P(\\omega_2 | \\; \\pmb{x}) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg) \\quad g_i(\\pmb{x}) = \\pmb{x}^{\\,t} \\bigg( - \\frac{1}{2} \\Sigma_i^{-1} \\bigg) \\pmb{x} + \\bigg( \\Sigma_i^{-1} \\pmb{\\mu}_{\\,i}\\bigg)^t \\pmb x + \\bigg( -\\frac{1}{2} \\pmb{\\mu}_{\\,i}^{\\,t} \\Sigma_{i}^{-1} \\pmb{\\mu}_{\\,i} -\\frac{1}{2} ln(|\\Sigma_i|)\\bigg)","title":"Bayes Theorem"},{"location":"glossary/dictionary/#batch-learning","text":"[ back to top ] Batch learning is an architecture used in machine learning tasks where the entire training dataset is available upfront to build the model. In contrast to on-line learning , the model is not updated once it was build on a training dataset.","title":"Batch Learning"},{"location":"glossary/dictionary/#big-data","text":"[ back to top ] There are many different, controversial interpretations and definitions for the term \"Big Data\". Typically, one refers to data as \"Big Data\" if its volume and complexity are of a magnitude that the data cannot be processed by \"conventional\" computing platforms anymore; storage space, processing power, and database structures are typically among the limiting factors.","title":"Big Data"},{"location":"glossary/dictionary/#binomial-distribution","text":"[ back to top ] Probability density function: p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k} p_k = {n \\choose x} \\cdot p^k \\cdot (1-p)^{n-k}","title":"Binomial distribution"},{"location":"glossary/dictionary/#bootstrapping","text":"[ back to top ] A resampling technique to that is closely related to cross-validation where a training dataset is divided into random subsets. Bootstrapping -- in contrast to cross-validation -- is a random sampling with replacement. Bootstrapping is typically used for statistical estimation of bias and standard error, and a common application in machine learning is to estimate the generalization error of a predictor.","title":"Bootstrapping"},{"location":"glossary/dictionary/#bregman-divergence","text":"[ back to top ] Bregman divergence describes are family of proximity functions (or distance measures) that share common properties and are often used in clustering algorithms. A popular example is the squared Euclidean distance.","title":"Bregman divergence"},{"location":"glossary/dictionary/#c","text":"","title":"C"},{"location":"glossary/dictionary/#central-limit-theorem","text":"[ back to top ] The Central Limit Theorem is a theorem in the field of probability theory that expresses the idea that the distribution of sample means (from independent random variables) converges to a normal distribution when the sample size approaches infinity.","title":"Central Limit Theorem"},{"location":"glossary/dictionary/#co-variance","text":"[ back to top ] S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) example covariance matrix: \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\pmb{\\Sigma_1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}","title":"Co-Variance"},{"location":"glossary/dictionary/#confusion-matrix","text":"[ back to top ] The confusion matrix is used as a way to represent the performance of a classifier and is sometimes also called \"error matrix\". This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.","title":"Confusion Matrix"},{"location":"glossary/dictionary/#contingency-table","text":"[ back to top ] A contingency table is used in clustering analysis to compare the overlap between two different clustering (grouping) results. The partitions from the two clustering results are represented as rows and columns in the table, and the individual elements of the table represent the number of elements that are shared between two partitions from each clustering result.","title":"Contingency Table"},{"location":"glossary/dictionary/#correlation-analysis","text":"[ back to top ] Correlation analysis describes and quantifies the relationship between two independent variables. Typically, in case of a positive correlation both variables have a tendency to increase, and in the case of negative correlation, one variable increases while the other variable increases. It is important to mention the famous quotation \"correlation does not imply causation\".","title":"Correlation analysis"},{"location":"glossary/dictionary/#correlation-analysis-canonical","text":"Let x and y be two vectors, the goal of canonical correlation analysis is to maximize the correlation between linear transformations of those original vectors. With applications in dimensionality reduction and feature selection, CCA tries to find common dimensions between two vectors.","title":"Correlation analysis, Canonical"},{"location":"glossary/dictionary/#correlation-analysis-matthews-correlation-coefficient-mcc","text":"[ back to top ] MCC is an assessment metric for clustering or binary classification analyses that represents the correlation between the observed (ground truth) and predicted labels. MCC can be directly computed from the confusion matrix and returns a value between -1 and 1.","title":"Correlation analysis - Matthews Correlation Coefficient (MCC)"},{"location":"glossary/dictionary/#correlation-analysis-kendall","text":"[ back to top ] Similar to the Pearson correlation coefficient , Kendall's tau measures the degree of a monotone relationship between variables, and like Spearman's rho , it calculates the dependence between ranked variables, which makes it feasible for non-normal distributed data. Kendall tau can be calculated for continuous as well as ordinal data. Roughly speaking, Kendall's tau distinguishes itself from Spearman's rho by stronger penalization of non-sequential (in context of the ranked variables) dislocations. \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} \\tau = \\frac{c-d}{c+d} = \\frac{S}{ \\left( \\begin{matrix} n \\\\ 2 \\end{matrix} \\right)} = \\frac{2S}{n(n-1)} where: c = the number of concordant pairs d = the number of discordant pairs [ Source ] If ties are present among the 2 ranked variables, the following equation shall be used instead: \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} \\tau = \\frac{S}{\\sqrt{n(n-1)/2-T}\\sqrt{n(n-1)/2-U}} T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ T = \\sum_t t(t-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ U = \\sum_u u(u-1)/2 \\\\ where: t = number of observations of variable x that are tied u = number of observations of variable y that are tied","title":"Correlation analysis - Kendall"},{"location":"glossary/dictionary/#correlation-analysis-pearson","text":"[ back to top ] The Pearson correlation coefficient is probably the most widely used measure for linear relationships between two normal distributed variables and thus often just called \"correlation coefficient\". Usually, the Pearson coefficient is obtained via a Least-Squares fit and a value of 1 represents a perfect positive relation-ship, -1 a perfect negative relationship, and 0 indicates the absence of a relationship between variables. \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} \\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y} And the estimate r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}}","title":"Correlation analysis - Pearson"},{"location":"glossary/dictionary/#correlation-analysis-spearman","text":"[ back to top ] Related to the Pearson correlation coefficient , the Spearman correlation coefficient (rho) measures the relationship between two variables. Spearman's rho can be understood as a rank-based version of Pearson's correlation coefficient , which can be used for variables that are not normal-distributed and have a non-linear relationship. Also, its use is not only restricted to continuous data, but can also be used in analyses of ordinal attributes. \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} where: d = the pairwise distances of the ranks of the variables x i and y i . n = the number of samples.","title":"Correlation analysis - Spearman"},{"location":"glossary/dictionary/#cosine-similarity","text":"[ back to top ] Cosine similarity measures the orientation of two n -dimensional sample vectors irrespective to their magnitude. It is calculated by the dot product of two numeric vectors, and it is normalized by the product of the vector lengths, so that output values close to 1 indicate high similarity. cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}","title":"Cosine Similarity"},{"location":"glossary/dictionary/#cost-function","text":"[ back to top ] A cost function (synonymous to loss function) is a special case of an objective function , i.e., a function that is used for solving optimization problems. A cost function can take one or more input variables and the output variable is to be minimized. A typical use case for cost functions is parameter optimization.","title":"Cost function"},{"location":"glossary/dictionary/#cross-validation","text":"[ back to top ] Cross-validation is a statistical technique to estimate the prediction error rate by splitting the data into training, cross-validation, and test datasets. A prediction model is obtained using the training set, and model parameters are optimized by the cross-validation set, while the test set is held primarily for empirical error estimation.","title":"Cross-validation"},{"location":"glossary/dictionary/#cross-validation-k-fold","text":"[ back to top ] K-fold cross-validation is a variant of cross validation where contiguous segments of samples are selected from the training dataset to build two new subsets for every iteration (without replacement): a new training and test dataset (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, K-fold"},{"location":"glossary/dictionary/#cross-validation-leave-one-out","text":"[ back to top ] Leave-One-Out cross-validation a variant of cross validation one sample is removed for every iteration (without replacement). The model is trained on the remaining N-1 samples and evaluated via the removed sample (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, Leave-One-Out"},{"location":"glossary/dictionary/#cross-validation-random-sampling","text":"[ back to top ] Cross-validation via random sampling is a variant of cross validation where random chunks of samples are extracted from the training dataset to build two new subsets for every iteration (with or without replacement): a new training and test dataset for every iteration (while the original test dataset is retained for the final evaluation of the predictor).","title":"Cross-validation, Random Sampling"},{"location":"glossary/dictionary/#curse-of-dimensionality","text":"[ back to top ] For a fixed number of training samples, the curse of dimensionality describes the increased error rate for a large number of dimensions (or features) due to imprecise parameter estimations.","title":"Curse of dimensionality"},{"location":"glossary/dictionary/#d","text":"","title":"D"},{"location":"glossary/dictionary/#data-mining","text":"[ back to top ] A field that is closely related to machine learning and pattern classification. The focus of data mining does not lie in merely the collection of data, but the extraction of useful information: Discovery of patterns, and making inferences and predictions. Common techniques in data mining include predictive modeling, clustering, association rules, and anomaly detection.","title":"Data mining"},{"location":"glossary/dictionary/#dbscan","text":"[ back to top ] DBSCAN is a variant of a density-based clustering algorithm that identifies core points as regions of high-densities based on their number of neighbors (> MinPts ) in a specified radius (\u03b5). Points that are below MinPts but within \u03b5 are specified as border points; the remaining points are classified as noise points.","title":"DBSCAN"},{"location":"glossary/dictionary/#decision-rule","text":"[ back to top ] A function in pattern classification tasks of making an \"action\", e.g., assigning a certain class label to an observation or pattern.","title":"Decision rule"},{"location":"glossary/dictionary/#decision-tree-classifier","text":"[ back to top ] Decision tree classifiers are tree like graphs, where nodes in the graph test certain conditions on a particular set of features, and branches split the decision towards the leaf nodes. Leaves represent lowest level in the graph and determine the class labels. Optimal tree are trained by minimizing Gini impurity, or maximizing information gain.","title":"Decision tree classifier"},{"location":"glossary/dictionary/#density-based-clustering","text":"[ back to top ] In density-based clustering, regions of high density in n-dimensional space are identified as clusters. The best advantage of this class of clustering algorithms is that they do not require apriori knowledge of number of clusters (as opposed to k-means algorithm).","title":"Density-based clustering"},{"location":"glossary/dictionary/#descriptive-modeling","text":"[ back to top ] Descriptive modeling is a common task in the field of data mining where a model is build in order to distinguish between objects and categorize them into classes - a form of data summary. In contrast to predictive modeling , the scope of descriptive modeling does not extend to making prediction for unseen objects.","title":"Descriptive modeling"},{"location":"glossary/dictionary/#dimensionality-reduction","text":"[ back to top ] Dimensionality reduction is a data pre-processing step in machine learning applications that aims to avoid the curse of dimensionality and reduce the effect of overfitting. Dimensionality reduction is related to feature selection , but instead of selecting a feature subset, dimensionality reduction takes as projection-based approach (e.g, linear transformation) in order to create a new feature subspace.","title":"Dimensionality reduction"},{"location":"glossary/dictionary/#distance-metric-learning","text":"[ back to top ] Distance metrics are fundamental for many machine learning algorithms. Distance metric learning - instead of learning a model - incorporates estimated relevances of features to obtain a distance metric for potentially optimal separation of classes and clusters: Large distances for objects from different classes, and small distances for objects of the same class, respectively.","title":"Distance Metric Learning"},{"location":"glossary/dictionary/#distance-euclidean","text":"[ back to top ] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras' theorem. The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension. \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}","title":"Distance, Euclidean"},{"location":"glossary/dictionary/#distance-manhattan","text":"[ back to top ] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance, but instead of calculating the shortest diagonal path (\"beeline\") between two points, it calculates the distance based on gridlines. The Manhattan distance was named after the block-like layout of the streets in Manhattan. \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}","title":"Distance, Manhattan"},{"location":"glossary/dictionary/#distance-minkowski","text":"[ back to top ] The Minkowski distance is a generalized form of the Euclidean distance (if p=2 ) and the Manhattan distance (if p=1 ). \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}","title":"Distance, Minkowski"},{"location":"glossary/dictionary/#e","text":"","title":"E"},{"location":"glossary/dictionary/#eager-learners","text":"[ back to top ] Eager learners (in contrast to lazy learners ) describe machine learning algorithms that learn a model for mapping attributes to class labels as soon as the data becomes available (e.g., Decision tree classifiers or naive Bayes classifiers ) and do not require the training data for making predictions on unseen samples once the model was built. The most computationally expensive step is the creation of a prediction model from the training data, and the actual prediction is considered as relatively inexpensive.","title":"Eager learners"},{"location":"glossary/dictionary/#eigenvectors-and-eigenvalues","text":"[ back to top ] Both eigenvectors and eigenvalues fundamental in many applications involve linear systems and are related via A\u00b7v = \u03bb\u00b7v (where A is a square matrix, v the eigenvector, and \u03bb the eigenvalue). Eigenvectors are describing the direction of the axes of a linear transformation, whereas eigenvalues are describing the scale or magnitude. \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ \\pmb A\\pmb{v} = \\lambda\\pmb{v}\\\\ where: \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue} \\pmb A = S_{W}^{-1}S_B\\\\ \\pmb{v} = \\text{Eigenvector}\\\\ \\lambda = \\text{Eigenvalue}","title":"Eigenvectors and Eigenvalues"},{"location":"glossary/dictionary/#ensemble-methods","text":"[ back to top ] Ensemble methods combine multiple classifiers which may differ in algorithms, input features, or input samples. Statistical analyses showed that ensemble methods yield better classification performances and are also less prone to overfitting. Different methods, e.g., bagging or boosting, are used to construct the final classification decision based on weighted votes.","title":"Ensemble methods"},{"location":"glossary/dictionary/#evolutionary-algorithms","text":"[ back to top ] Evolutionary algorithms are a class of algorithms that are based on heuristic search methods inspired by biological evolution in order to solve optimization problems.","title":"Evolutionary algorithms"},{"location":"glossary/dictionary/#exhaustive-search","text":"[ back to top ] Exhaustive search (synonymous to brute-force search) is a problem-solving approach where all possible combinations are sequentially evaluated to find the optimal solution. Exhaustive search guarantees to find the optimal solution whereas other approaches (e.g., heuristic searches ) are regarded as sub-optimal. A downside of exhaustive searches is that computational costs increase proportional to the number of combinations to be evaluated.","title":"Exhaustive search"},{"location":"glossary/dictionary/#expectation-maximization-algorithm-em","text":"[ back to top ] The Expectation Maximization algorithm (EM) is a technique to estimate parameters of a distribution based on the Maximum Likelihood Estimate (MLE) that is often used for the imputation of missing values in incomplete datasets. After the EM algorithm is initialized with a starting value, alternating iterations between expectation and maximization steps are repeated until convergence. In the expectation step, parameters are estimated based on the current model to impute missing values. In the maximization step, the log-likelihood function of the statistical model is to be maximized by re-estimating the parameters based on the imputed values from the expectation step.","title":"Expectation Maximization algorithm - EM"},{"location":"glossary/dictionary/#f","text":"","title":"F"},{"location":"glossary/dictionary/#feature-selection","text":"[ back to top ] Feature selection is an important pre-processing step in many machine learning applications in order to avoid the curse of dimensionality and overfitting . A subset of features is typically selected by evaluating different combinations of features and eventually retain the subset that minimizes a specified cost function . Commonly used algorithms for feature selection as alternative to exhaustive search algorithms include sequential selection algorithms and genetic algorithms.","title":"Feature Selection"},{"location":"glossary/dictionary/#feature-space","text":"[ back to top ] A feature space describes the descriptive variables that are available for samples in a dataset as a d -dimensional Euclidean space. E.g., sepal length and width, and petal length and width for each flower sample in the popular Iris dataset.","title":"Feature Space"},{"location":"glossary/dictionary/#fuzzy-c-means-clustering","text":"[ back to top ] Fuzzy C-Means is a soft clustering algorithm in which each sample point has a membership degree to each cluster; in hard (crisp) clustering, membership of each point to each cluster is either 0 or 1. Fuzzy C-Means considers a weight matrix for cluster memberships, and minimizes sum squared error (SSE) of weighted distances of sample points to the cluster centroids.","title":"Fuzzy C-Means Clustering"},{"location":"glossary/dictionary/#g","text":"","title":"G"},{"location":"glossary/dictionary/#generalization-error","text":"[ back to top ] The generalization error describes how well new data can be classified and is a useful metric to assess the performance of a classifier. Typically, the generalization error is computed via cross-validation or simply the absolute difference between the error rate on the training and test dataset.","title":"Generalization error"},{"location":"glossary/dictionary/#genetic-algorithm","text":"[ back to top ] The Genetic algorithm is a subclass of evolutionary algorithms that takes a heuristic approach inspired by Charles Darwin's theory of \"natural selection\" in order to solve optimization problems.","title":"Genetic algorithm"},{"location":"glossary/dictionary/#gradient-descent","text":"[ back to top ] Gradient descent is an algorithm that optimizes a function by finding its local minimum. After the algorithm was initialized with an initial guess, it takes the derivative of the function to make a step towards the direction of deepest descent. This step-wise process is repeated until convergence.","title":"Gradient Descent"},{"location":"glossary/dictionary/#greedy-algorithm","text":"[ back to top ] Greedy Algorithms are a family of algorithms that are used in optimization problems. A greedy algorithm makes locally optimal choices in order to find a local optimum (suboptimal solution, also see ( heuristic problem solving ).","title":"Greedy Algorithm"},{"location":"glossary/dictionary/#h","text":"","title":"H"},{"location":"glossary/dictionary/#heuristic-search","text":"[ back to top ] Heuristic search is a problem-solving approach that is focussed on efficiency rather than completeness in order to find a suboptimal solution to a problem. Heuristic search is often used as alternative approach where exhaustive search is too computationally intensive and where solutions need to be approximated.","title":"Heuristic search"},{"location":"glossary/dictionary/#hyperparameters","text":"[ back to top ] Hyperparameters are the parameters of a classifier or estimator that are not directly learned in the machine learning step from the training data but are optimized separately (e.g., via Grid Search ). The goal of hyperparameter optimization is to achieve good generalization of a learning algorithm and to avoid overfitting to the training data.","title":"Hyperparameters"},{"location":"glossary/dictionary/#i","text":"","title":"I"},{"location":"glossary/dictionary/#iid","text":"[ back to top ] The abbreviation \"iid\" stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another variable (e.g., time series and network graphs are not independent). One popular example of iid would be the tossing of a coin: One coin toss does not affect the outcome of another coin toss, and the probability of the coin landing on either \"heads\" or \"tails\" is the same for every coin toss.","title":"iid"},{"location":"glossary/dictionary/#imputation","text":"[ back to top ] Imputations algorithms are designed to replace the missing data (NAs) with certain statistics rather than discarding them for downstream analysis. Commonly used imputation methods include mean imputation (replacement by the sample mean of an attribute), kNN imputation, and regression imputation.","title":"Imputation"},{"location":"glossary/dictionary/#independent-component-analysis","text":"[ back to top ] Independent Component Analysis (ICA) is a statistical signal-processing technique that decomposes a multivariate dataset of mixed, non-gaussian distributed source signals into independent components. A popular example is the separation of overlapping voice samples -- the so-called \"cocktail party problem\".","title":"Independent Component Analysis"},{"location":"glossary/dictionary/#j","text":"","title":"J"},{"location":"glossary/dictionary/#jaccard-coefficient","text":"[ back to top ] The Jaccard coefficient (bounded at [0, 1)) is used as similarity measure for asymmetric binary data and calculated by taking the number of matching attributes and divide it by the number of all attributes except those where both variables have a value 0 in contrast to a simple matching coefficient . A popular application is the identification of near-duplicate documents for which the Jaccard coefficient can be calculated by the dividing the intersection of the set of words by the union of the set words in both documents.","title":"Jaccard coefficient"},{"location":"glossary/dictionary/#jackknifing","text":"[ back to top ] Jackknifing is a resampling technique that predates the related cross-validation and bootstrapping techniques and is mostly used for bias and variance estimations. In jackknifing, a dataset is split into N subsets where exactly one sample is removed from every subset so that every subset is of size N-1.","title":"Jackknifing"},{"location":"glossary/dictionary/#jittering","text":"[ back to top ] Jittering is a sampling technique that can be used to measure the stability of a given statistical model (classifiction/regression/clustering). In jittering, some noise is added to sample data points, and then a new model is drawn and compared to the original model.","title":"Jittering"},{"location":"glossary/dictionary/#k","text":"","title":"K"},{"location":"glossary/dictionary/#k-d-trees","text":"[ back to top ] k-D trees are a data structures (recursive space partitioning trees) that result from the binary partitioning of multi-dimensional feature spaces. A typical application of k-D trees is to increase the search efficiency for nearest-neighbor searches. A k-D tree construction can be described as a iterating process with the following steps: Select the dimension of largest variance, draw a cutting plane based at the median along the dimension to split the data into 2 halves, choose the next dimension.","title":"k-D Trees"},{"location":"glossary/dictionary/#kernel-density-estimation","text":"[ back to top ] Non-parametric techniques to estimate probability densities from the available data without requiring prior knowledge of the underlying model of the probability distribution.","title":"Kernel Density Estimation"},{"location":"glossary/dictionary/#kernel-in-statistics","text":"[ back to top ] In the context of [kernel methods](#kernel-methods the term \u201ckernel\u201d describes a function that calculates the dot product of the images of the samples x under the kernel function \u03c6 (see kernel methods). Roughly speaking, a kernel can be understood as a similarity measure in higher-dimensional space.","title":"Kernel (in statistics)"},{"location":"glossary/dictionary/#kernel-methods","text":"[ back to top ] Kernel methods are algorithms that map the sample vectors of a dataset onto a higher-dimensional feature space via a so-called kernel function (\u03c6(x)). The goal is to identify and simplify general relationships between data, which is especially useful for linearly non-separable datasets.","title":"Kernel Methods"},{"location":"glossary/dictionary/#kernel-trick","text":"[ back to top ] Since the explicit computation of the kernel is increasingly computationally expensive for large sample sizes and high numbers of dimensions, the kernel trick uses approximations to calculate the kernel implicitly. The most popular kernels used for the kernel trick are Gaussian Radius Basis Function (RBF) kernels, sigmoidal kernels, and polynomial kernels.","title":"Kernel Trick"},{"location":"glossary/dictionary/#k-fold-cross-validation","text":"[ back to top ] In k-fold cross-validation the data is split into k subsets, then a prediction/classification model is trained k times, each time holding one subset as test set, training the model parameters using the remaining k -1. Finally, cross-validation error is evaluated as the average error out of all k training models.","title":"k-fold Cross-validation"},{"location":"glossary/dictionary/#k-means-clustering","text":"[ back to top ] A method of partitioning a dataset into k clusters by picking k random initial points (where k < n , the number or total points - modified by S.R. ), assigning clusters, averaging, reassigning, and repeating until stability is achieved. The number k must be chosen beforehand.","title":"K-Means Clustering"},{"location":"glossary/dictionary/#k-means-clustering_1","text":"[ back to top ] A variant of k-means where instead of choosing all initial centers randomly, the first is chosen randomly, the second chosen with probability proportional to the squared distance from the first, the third chosen with probability proportional to the square distance from the first two, etc. See this paper .","title":"K-Means++ Clustering"},{"location":"glossary/dictionary/#k-medoids-clustering","text":"[ back to top ] K-Medoids clustering is a variant of k-means algorithm in which cluster centroids are picked among the sample points rather than the mean point of each cluster. K-Medoids can overcome some of the limitations of k-means algorithm by avoiding empty clusters, being more robust to outliers, and being more easily applicable to non-numeric data types.","title":"K-Medoids Clustering"},{"location":"glossary/dictionary/#k-nearest-neighbors-algorithms","text":"[ back to top ] K-nearest neighbors algorithms find the k-points that are closest to a point of interest based on their attributes using a certain distance measure (e.g., Euclidean distance). K-nearest neighbors algorithms are being used in many different contexts: Non-parametric density estimation, missing value imputation, dimensionality reduction, and classifiers in supervised and unsupervised pattern classification and regression problems.","title":"K-nearest neighbors algorithms"},{"location":"glossary/dictionary/#knowledge-discovery-in-databases-kdd","text":"[ back to top ] Knowledge Discovery in Databases (KDD) describes a popular workflow including data mining for extracting useful and meaningful information out of data. Typically, the individual steps are feature selection, pre-processing, transformation, data mining , and post-processing (evaluation and interpretation).","title":"Knowledge Discovery in Databases (KDD)"},{"location":"glossary/dictionary/#l","text":"","title":"L"},{"location":"glossary/dictionary/#lasso-regression","text":"[ back to top ] LASSO (Least Absolute Shrinkage and Selection Operator) is a regression model that uses the L1-norm (sum of absolute values) of model coefficients to penalize the model complexity. LASSO has the advantage that some coefficients can become zero, as opposed to ridge regression that uses the squared sum of model coefficients.","title":"LASSO Regression"},{"location":"glossary/dictionary/#latent-semantic-indexing","text":"[ back to top ] Latent Semantic Indexing (LSI) is a data mining technique to characterize documents by topics, word usage, or other contexts. The structures of the documents are compared by applying singular value decomposition to an input term-document matrix (e.g., a data table of word counts with terms as row labels and document numbers as column labels) in order to obtain the singular values and vectors.","title":"Latent Semantic Indexing"},{"location":"glossary/dictionary/#law-of-large-numbers","text":"[ back to top ] The Law of Large Numbers is a theorem in the field of probability theory that expresses the idea that the actual value of a random sampling process approaches the expected value for growing sample sizes. A common example is that the observed ratio of \"heads\" in an unbiased coin-flip experiment will approach 0.5 for large sample sizes.","title":"Law of Large Numbers"},{"location":"glossary/dictionary/#lazy-learners","text":"[ back to top ] Lazy learners (in contrast to eager learners ) are memorizing training data in order to make predictions for unseen samples. While there is no expensive learning step involved, the prediction step is generally considered to be more expensive compared to eager learners since it involves the evaluation of training data. One example of lazy learners are k-nearest neighbor algorithms where the class label of a unseen sample is estimated by e.g., the majority of class labels of its neighbors in the training data.","title":"Lazy learners"},{"location":"glossary/dictionary/#least-squares-fit","text":"[ back to top ] A linear regression technique that fits a straight line to a data set (or overdetermined system) by minimizing the sum of the squared residuals, which can be the minimized vertical or perpendicular offsets from the fitted line. Linear equation f(x) = a\\cdot x + b f(x) = a\\cdot x + b f(x) = a\\cdot x + b Slope: a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad a = \\frac{S_{x,y}}{\\sigma_{x}^{2}}\\quad Y-axis intercept: b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad b = \\bar{y} - a\\bar{x}\\quad where: S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\quad \\text{(covariance)} \\\\ \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\text{(variance)} Matrix equation \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\pmb X \\ . \\pmb a = \\pmb y \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\Bigg[ \\begin{array}{cc} x_1 & 1 \\\\ ... & 1 \\\\ x_n & 1 \\end{array} \\Bigg] \\bigg[ \\begin{array}{c} a \\\\ b \\end{array} \\bigg] =\\Bigg[ \\begin{array}{c} y_1 \\\\ ... \\\\ y_n \\end{array} \\Bigg] \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y \\pmb a = (\\pmb X^T \\; \\pmb X)^{-1} \\pmb X^T \\; \\pmb y","title":"Least Squares fit"},{"location":"glossary/dictionary/#lennard-jones-potential","text":"[ back to top ] The Lennard-Jones potential describes the energy potential between two non-bonded atoms based on their distance to each other. V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = 4 \\epsilon \\bigg[ \\bigg(\\frac{\\sigma}{r}\\bigg)^{12} - \\bigg(\\frac{\\sigma}{r}\\bigg)^{6} \\bigg] V = intermolecular potential \u03c3 = distance where V is 0 r = distance between atoms, measured from one center to the other \u03b5 = interaction strength","title":"Lennard-Jones Potential"},{"location":"glossary/dictionary/#linear-discriminant-analysis","text":"[ back to top ] In-between class scatter matrix S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i S_W = \\sum\\limits_{i=1}^{c} S_i Where: S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ S_i = \\sum\\limits_{\\pmb x \\in D_i}^n (\\pmb x - \\pmb m_i)\\;(\\pmb x - \\pmb m_i)^T \\text{ (scatter matrix for every class)} \\\\\\\\ and \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m_i = \\frac{1}{n_i} \\sum\\limits_{\\pmb x \\in D_i}^n \\; \\pmb x_k \\text{ (mean vector)} Between class scatter matrix S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T S_B = \\sum\\limits_{i=1}^{c} (\\pmb m_i - \\pmb m) (\\pmb m_i - \\pmb m)^T","title":"Linear Discriminant Analysis"},{"location":"glossary/dictionary/#linear-discriminant-analysis-lda","text":"[ back to top ] A linear transformation technique (related to Principal Component Analysis) that is commonly used to project a dataset onto a new feature space or feature subspace, where the new component axes maximize the spread between multiple classes, or for classification of data.","title":"Linear Discriminant Analysis (LDA)"},{"location":"glossary/dictionary/#local-outlier-factor-lof","text":"[ back to top ] LOF is a density-based anomaly detection technique for outlier identification. The LOF for a point p refers to the average \"reachability distance\" towards its nearest neighbors. Eventually, the points with the largest LOF values (given a particular threshold) are identified as outliers.","title":"Local Outlier Factor (LOF)"},{"location":"glossary/dictionary/#locality-sensitive-hashing-lsh","text":"[ back to top ] Locality-sensitive hashing (LSH) is a dimensionality reduction technique that groups objects that are likely similar (based on a similarity signature such as MinHash ) into the same buckets in order to reduce the search space for pair-wise similarity comparisons. One application of LSH could be a combination with other dimensionality reduction techniques, e.g., MinHash , in order to reduce the computational costs of finding near-duplicate document pairs.","title":"Locality-sensitive hashing (LSH)"},{"location":"glossary/dictionary/#logistic-regression","text":"[ back to top ] Logistic regression is a statistical model used for binary classification (binomial logistic regression) where class labels are mapped to \"0\" or \"1\" outputs. Logistic regression uses the logistic function (a general form of sigmoid function), where its output ranges from (0-1).","title":"Logistic Regression"},{"location":"glossary/dictionary/#m","text":"","title":"M"},{"location":"glossary/dictionary/#machine-learning","text":"[ back to top ] A set of algorithmic instructions for discovering and learning patterns from data e.g., to train a classifier for a pattern classification task.","title":"Machine learning"},{"location":"glossary/dictionary/#mahalanobis-distance","text":"[ back to top ] The Mahalanobis distance measure accounts for the covariance among variables by calculating the distance between a sample x and the sample mean \u03bc in units of the standard deviation. The Mahalanobis distance becomes equal to the Euclidean distance for uncorrelated with same variances.","title":"Mahalanobis distance"},{"location":"glossary/dictionary/#mapreduce","text":"[ back to top ] MapRedcue is a programming model for analyzing large datasets on distributed computer clusters, in which the task is divided into two steps, a map step and a reducer step. In the map step, the data are filtered by some factors on each compute node, then filtered data are shuffled and passed to the reducer function which performs further analysis on each portion of filtered data separately.","title":"MapReduce"},{"location":"glossary/dictionary/#markov-chains","text":"[ back to top ] Markov chains (names after Andrey Markov) are mathematical systems that describe the transitioning between different states in a model. The transitioning from one state to the other (or back to itself) is a stochastic process.","title":"Markov chains"},{"location":"glossary/dictionary/#monte-carlo-simulation","text":"[ back to top ] A Monte Carlo simulation is an iterative sampling method for solving deterministic models. Random numbers or variables from a particular probability distribution are used as input variables for uncertain parameters to compute the response variables.","title":"Monte Carlo simulation"},{"location":"glossary/dictionary/#maximum-likelihood-estimates-mle","text":"[ back to top ] A technique to estimate the parameters that have been fit to a model by maximizing a known likelihood function. One common application is the estimation of \"mean\" and \"variance\" for a Gaussian distribution. D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} D = \\left\\{ \\pmb x_1, \\pmb x_2,..., \\pmb x_n \\right\\} can be pictured as probability to observe a particular sequence of patterns, where the probability of observing a particular patterns depends on \u03b8 , the parameters the underlying (class-conditional) distribution. In order to apply MLE, we have to make the assumption that the samples are i.i.d. (independent and identically distributed). p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) p(D\\; | \\; \\pmb \\theta\\;) = p(\\pmb x_1 \\; | \\; \\pmb \\theta\\;)\\; \\cdot \\; p(\\pmb x_2 \\; | \\;\\pmb \\theta\\;) \\; \\cdot \\;... \\; p(\\pmb x_n \\; | \\; \\pmb \\theta\\;) = \\prod_{k=1}^{n} \\; p(\\pmb x_k \\pmb \\; | \\; \\pmb \\theta \\;) Where \u03b8 is the parameter vector, that contains the parameters for a particular distribution that we want to estimate and p(D | \u03b8 ) is also called the likelihood of \u03b8 . log-likelihood p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) p(D|\\theta) = \\prod_{k=1}^{n} p(x_k|\\theta) \\\\ \\Rightarrow l(\\theta) = \\sum_{k=1}^{n} ln \\; p(x_k|\\theta) Differentiation \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} \\equiv \\begin{bmatrix} \\frac{\\partial \\; }{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; }{\\partial \\; \\theta_p}\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} \\nabla_{\\pmb \\theta} l(\\pmb\\theta) \\equiv \\begin{bmatrix} \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_1} \\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_2} \\\\ ...\\\\ \\frac{\\partial \\; L(\\pmb\\theta)}{\\partial \\; \\theta_p}\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ...\\\\ 0\\end{bmatrix} parameter vector \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg] \\pmb \\theta_i = \\bigg[ \\begin{array}{c} \\ \\theta_{i1} \\\\ \\ \\theta_{i2} \\\\ \\end{array} \\bigg]= \\bigg[ \\begin{array}{c} \\pmb \\mu_i \\\\ \\pmb \\Sigma_i \\\\ \\end{array} \\bigg]","title":"Maximum Likelihood Estimates (MLE)"},{"location":"glossary/dictionary/#min-max-scaling","text":"[ back to top ] X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}","title":"Min-Max scaling"},{"location":"glossary/dictionary/#minhash","text":"[ back to top ] MinHash is a commonly used technique for dimensionality reduction in document similarity comparisons. The idea behind MinHash is to create a signature of reduced dimensionality while preserving the Jaccard similarity coefficient . A common implementation of MinHash is to generate k random permutations of the columns in a m*x*n -document matrix (rows represent the sparse vectors of words for each document as binary data) and generate a new matrix of size m*x*k . The cells of the new matrix now contain the position labels of the first non-zero value for every document (1 column for each round of random permutation). Based on similarities of the position labels, the Jaccard coefficient for the pairs of documents can be calculated.","title":"MinHash"},{"location":"glossary/dictionary/#n","text":"","title":"N"},{"location":"glossary/dictionary/#naive-bayes-classifier","text":"[ back to top ] A classifier based on a statistical model (i.e., Bayes theorem: calculating posterior probabilities based on the prior probability and the so-called likelihood) in the field of pattern classification. Naive Bayes assumes that all attributes are conditionally independent, thereby, computing the likelihood is simplified to the product of the conditional probabilities of observing individual attributes given a particular class label.","title":"Naive Bayes Classifier"},{"location":"glossary/dictionary/#n-grams","text":"[ back to top ] In context of natural language processing (NLP), a text is typically broken down into individual elements (see tokenization ). N-grams describe the length of the individual elements where n refers to the number of words or symbols in every token. E.g., a unigram (or 1-gram) can represent a single word, and a bigram (or 2-gram) describes a token that consists of 2 words etc.","title":"N-grams"},{"location":"glossary/dictionary/#non-parametric-statistics","text":"[ back to top ] In contrast to parametric approaches, non-parametric statistics or approaches do not make prior assumptions about the underlying probability distribution of a particular variable or attribute.","title":"Non-parametric statistics"},{"location":"glossary/dictionary/#normal-distribution-multivariate","text":"[ back to top ] Probability density function p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim N(\\pmb \\mu|\\Sigma) p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg] p(\\pmb x) \\sim \\frac{1}{(2\\pi)^{d/2} \\; |\\Sigma|^{1/2}} \\exp \\bigg[ -\\frac{1}{2}(\\pmb x - \\pmb \\mu)^t \\Sigma^{-1}(\\pmb x - \\pmb \\mu) \\bigg]","title":"Normal distribution (multivariate)"},{"location":"glossary/dictionary/#normal-distribution-univariate","text":"[ back to top ] Probability density function p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim N(\\mu|\\sigma^2) p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] } p(x) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\bigg[-\\frac{1}{2}\\bigg( \\frac{x-\\mu}{\\sigma}\\bigg)^2 \\bigg] }","title":"Normal distribution (univariate)"},{"location":"glossary/dictionary/#normal-modes","text":"[ back to top ] Normal modes are the harmonic oscillations of a system of masses connected by springs, or roughly speaking \"concerted motions,\" and all normal modes of a system are independent from each other. A classic example describes two masses connected by a middle spring, and each mass is connected to a fixed outer edge ( | m1~~m2 |). The oscillation of this system where the middle spring does not move is defined as its normal mode.","title":"Normal Modes"},{"location":"glossary/dictionary/#normalization-min-max-scaling","text":"[ back to top ] A data pre-processing step (also often referred to as \"Feature Scaling\") for fitting features from different measurements within a certain range, typically the unit range from 0 to 1.","title":"Normalization - Min-Max scaling"},{"location":"glossary/dictionary/#normalization-standard-scores","text":"[ back to top ] A data pre-processing step (also often just called \"Standardization\") for re-scaling features from different measurements to match proportions of a standard normal distribution (unit variance centered at mean=0).","title":"Normalization - Standard Scores"},{"location":"glossary/dictionary/#o","text":"","title":"O"},{"location":"glossary/dictionary/#objective-function","text":"[ back to top ] Objective functions are mathematical function that are used for problem-solving and optimization tasks. Depending on the task, the objective function can be omtpimized through minimization ( cost or loss functions ) or maximization (reward function). A typical application of an objective function in pattern classification tasks is to minimize the error rate of a classifier.","title":"Objective function"},{"location":"glossary/dictionary/#on-line-learning","text":"[ back to top ] On-line learning is a machine learning architecture where the model is being updated consecutively as new training data arrives in contrast to batch-learning , which requires the entire training dataset to be available upfront. On-line has the advantage that a model can be updated and refined over time to account for changes in the population of training samples. A popular example where on-line learning is beneficial is the task of spam detection.","title":"On-Line Learning"},{"location":"glossary/dictionary/#on-line-analytical-processing-olap","text":"[ back to top ] On-Line Analytical Processing (OLAP) describes the general process of working with multidimensional arrays for exploratory analysis and information retrieval; often, OLAP is used to create summary data, e.g., via data aggregation across multiple dimensions or columns.","title":"On-Line Analytical Processing (OLAP)"},{"location":"glossary/dictionary/#p","text":"","title":"P"},{"location":"glossary/dictionary/#parzen-rosenblatt-window-technique","text":"[ back to top ] A non-parametric kernel density estimation technique for probability densities of random variables if the underlying distribution/model is unknown. A so-called window function is used to count samples within hypercubes or Gaussian kernels of a specified volume to estimate the probability density. \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} \\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\ 0 & \\quad \\text{otherwise} \\end{array} for a hypercube of unit length 1 centered at the coordinate system's origin. What this function basically does is assigning a value 1 to a sample point if it lies within \u00bd of the edges of the hypercube, and 0 if lies outside (note that the evaluation is done for all dimensions of the sample point). If we extend on this concept, we can define a more general equation that applies to hypercubes of any length h n that are centered at x : k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ k_n = \\sum\\limits_{i=1}^{n} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)\\\\\\\\ where: \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) \\pmb u = \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg) probability density estimation with hypercube kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] where: h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k h^d = V_n\\quad \\text{and} \\quad\\phi \\bigg[ \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg] = k probability density estimation with Gaussian kernel p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg] p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\Bigg[ \\frac{1}{(\\sqrt {2 \\pi})^d h_{n}^{d}} \\exp \\; \\bigg[ -\\frac{1}{2} \\bigg(\\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)^2 \\bigg] \\Bigg]","title":"Parzen-Rosenblatt Window technique"},{"location":"glossary/dictionary/#pattern-classification","text":"[ back to top ] The usage of patterns in datasets to discriminate between classes, i.e., to assign a class label to a new observation based on inference.","title":"Pattern classification"},{"location":"glossary/dictionary/#perceptron","text":"[ back to top ] A (single-layer) perceptron is a simple Artificial Neural Network algorithm that consists of only two types of nodes: Input nodes and output nodes connected by weighted links. Perceptrons are being used as linear classifiers in supervised machine learning tasks.","title":"Perceptron"},{"location":"glossary/dictionary/#permissive-transformations","text":"[ back to top ] Permissive transformations are transformations of data that that do not change the \"meaning\" of the attributes, such as scaling or mapping. For example, the transformation of temperature measurements from a Celsius to a Kelvin scale would be a permissive transformation of a numerical attribute.","title":"Permissive transformations"},{"location":"glossary/dictionary/#poisson-distribution-univariate","text":"[ back to top ] Probability density function p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!} p(x|\\theta) = \\frac{e^{-\\theta}\\theta^{xk}}{x_k!}","title":"Poisson distribution (univariate)"},{"location":"glossary/dictionary/#population-mean","text":"[ back to top ] \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i example mean vector: \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\pmb{\\mu_1} = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}","title":"Population mean"},{"location":"glossary/dictionary/#power-transform","text":"[ back to top ] Power transforms form a category of statistical transformation techniques that are used to transform non-normal distributed data to normality.","title":"Power transform"},{"location":"glossary/dictionary/#principal-component-analysis-pca","text":"[ back to top ] A linear transformation technique that is commonly used to project a dataset (without utilizing class labels) onto a new feature space or feature subspace (for dimensionality reduction) where the new component axes are the directions that maximize the variance/spread of the data. Scatter matrix S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T S = \\sum\\limits_{k=1}^n (\\pmb x_k - \\pmb m)\\;(\\pmb x_k - \\pmb m)^T where: \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)} \\pmb m = \\frac{1}{n} \\sum\\limits_{k=1}^n \\; \\pmb x_k \\text{ (mean vector)}","title":"Principal Component Analysis (PCA)"},{"location":"glossary/dictionary/#precision-and-recall","text":"[ back to top ] Precision (synonymous to specificity ) and recall (synonymous to sensitivity ) are two measures to assess performance of a classifier if class label distributions are skewed. Precision is defined as the ratio of number of relevant items out of total retrieved items, whereas recall is the fraction of relevant items which are retrieved.","title":"Precision and Recall"},{"location":"glossary/dictionary/#predictive-modeling","text":"[ back to top ] Predictive modeling a data mining task for predicting outcomes based on a statistical model that was build on previous observations (in contrast to descriptive modeling ). Predictive modeling can be further divided into the three sub-tasks: Regression, classification, and ranking.","title":"Predictive Modeling"},{"location":"glossary/dictionary/#proportion-of-variance-explained","text":"[ back to top ] In the context of dimensionality reduction, the proportion of variance explained (PVE) describes how much of the total variance is captured by the new selected axes, for example, principal components or discriminant axes. It is computed by the sum of variance of new component axes divided by the total variance.","title":"Proportion of Variance Explained"},{"location":"glossary/dictionary/#purity-measure","text":"[ back to top ] In a cluster analysis with given truth cluster memberships (or classes), \"purity\" is used to assess the effectiveness of clustering. Purity is measured by assigning each cluster to the class that is maximally represented and computed via the weighted average of maximum number of samples from the same class in each cluster.","title":"Purity Measure"},{"location":"glossary/dictionary/#q","text":"","title":"Q"},{"location":"glossary/dictionary/#quantitative-and-qualitative-attributes","text":"[ back to top ] Quantitative attributes are also often called \"numeric\"; those are attributes for which calculations and comparisons like ratios and intervals make sense (e.g., temperature in Celsius). Qualitative, or \"categorical\", attributes can be grouped into to subclasses: nominal and ordinal. Where ordinal attributes (e.g., street numbers) can be ordered, nominal attributes can only distinguished by their category names (e.g., colors).","title":"Quantitative and qualitative attributes"},{"location":"glossary/dictionary/#r","text":"","title":"R"},{"location":"glossary/dictionary/#r-factor","text":"[ back to top ] The R-factor is one of several measures to assess the quality of a protein crystal structure. After building and refining an atomistic model of the crystal structure, the R-factor measures how well this model can describe the experimental diffraction patterns via the equation: R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|} R = \\frac{\\sum ||F_{obs}| - |F_{calc}||}{\\sum|F_{obs}|}","title":"R-factor"},{"location":"glossary/dictionary/#random-forest","text":"[ back to top ] Random forest is an ensemble classifier where multiple decision tree classifiers are learned and combined via the bagging technique. Unseen/test objects are then classified by taking the majority of votes from individual decision trees.","title":"Random forest"},{"location":"glossary/dictionary/#rayleigh-distribution-univariate","text":"[ back to top ] Probability density function p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array} p(x|\\theta) = \\Bigg\\{ \\begin{array}{c} 2\\theta xe^{- \\theta x^2},\\quad \\quad x \\geq0, \\\\ 0,\\quad \\text{otherwise.} \\\\ \\end{array}","title":"Rayleigh distribution (univariate)"},{"location":"glossary/dictionary/#receiver-operating-characteristic-roc","text":"[ back to top ] The Receiver Operating Characteristic (ROC, or ROC curve) is a quality measure for binary prediction algorithms by plotting the \"False positive rate\" vs. the \"True positive rate\" ( sensitivity ).","title":"Receiver Operating Characteristic (ROC)"},{"location":"glossary/dictionary/#regularization","text":"[ back to top ] Regularization is a technique to overcome overfitting by introducing a penalty term for model complexity. Usually, the penalty term is the squared sum of the model parameters, thereby promoting less complex models during training. Regularization may increase the training error but can potentially reduce the classification error on the test dataset.","title":"Regularization"},{"location":"glossary/dictionary/#reinforcement-learning","text":"[ back to top ] Reinforcement learning is a machine learning algorithm that learns from a series of actions by maximizing a \"reward function\". The reward function can either be maximized by penalizing \"bad actions\" and/or rewarding \"good actions\".","title":"Reinforcement learning"},{"location":"glossary/dictionary/#rejection-sampling","text":"[ back to top ] Rejection sampling is similar to the popular Monte Carlo sampling with the difference of an additional bound. The goal of rejection sampling is to simplify the task of drawing random samples from a complex probability distribution by using a uniform distribution instead; random samples drawn from the uniform distribution that lie outside certain boundary criteria are rejected, and all samples within the boundary are accepted, respectively.","title":"Rejection sampling"},{"location":"glossary/dictionary/#resubstitution-error","text":"[ back to top ] The resubstitution error represents the classification error rate on the training dataset (the dataset that was used to train the classifier). The performance of a classifier cannot be directly deduced from resubstitution error alone, but it becomes a useful measure for calculating the generalization error .","title":"Resubstitution error"},{"location":"glossary/dictionary/#ridge-regression","text":"[ back to top ] Ridge regression is a regularized regression technique in which the squared sum of the model coefficients is used to penalize model complexity.","title":"Ridge Regression"},{"location":"glossary/dictionary/#rule-based-classifier","text":"[ back to top ] Rule-based classifiers are classifiers that are based on one or more \"IF ... THEN ...\" rules. Rule-based classifiers are related to decision trees and can be extracted from the latter. If the requirements for a rule-based classifier (mutually exclusive: at most one rule per sample; mutuallyy exhaustive: at least one rule per sample) are violated, possible remedies include the addition of rules or the ordering of rules.","title":"Rule-based classifier"},{"location":"glossary/dictionary/#s","text":"","title":"S"},{"location":"glossary/dictionary/#sampling","text":"[ back to top ] Sampling is data pre-processing procedure that is used to reduce the overall size of a dataset and to reduce computational costs by selecting a representative subset from the whole input dataset.","title":"Sampling"},{"location":"glossary/dictionary/#sensitivity","text":"[ back to top ] Sensitivity (synonymous to precision ), which is related to specificity -- in the context of error rate evaluation -- describes the \"True Positive Rate\" for a binary classification problem: The probability to make a correct prediction for a \"positive/true\" case (e.g., in an attempt to predict a disease, the disease is correctly predicted for a patient who truly has this disease). Sensitivity is calculated as (TP)/(TP+FN), where TP=True Positives, FN=False Negatives.","title":"Sensitivity"},{"location":"glossary/dictionary/#sharding","text":"[ back to top ] Sharding is the non-redundant partitioning of a database into smaller databases; this process can also be understood as horizontal splitting. The rationale behind sharing is to divide a database among separate machines to avoid storage or performance issues that are related to growing database sizes.","title":"Sharding"},{"location":"glossary/dictionary/#silhouette-measure-clustering","text":"[ back to top ] Silhouette measure provides a metric to evaluate the performance of a clustering analysis. For each data point i , it measures the average distance of point i to all other points in the same cluster (a(i)) , and the minimum distance to points from other clusters (b(i)) . The average silhouette measures for each cluster can provide a visual way to pick the proper number of clusters.","title":"Silhouette Measure (clustering)"},{"location":"glossary/dictionary/#simple-matching-coefficient","text":"[ back to top ] The simple matching coefficient is a similarity measure for binary data and calculated by dividing the total number of matches by the total number of attributes. For asymmetric binary data, the related Jaccard coefficient is to be preferred in order to avoid highly similar scores.","title":"Simple Matching Coefficient"},{"location":"glossary/dictionary/#singular-value-decomposition-svd","text":"[ back to top ] Singular value decomposition (SVD) is linear algebra technique that decomposes matrix X into U D V T where U (left-singular vectors) and V (right-singular vector) are both column-orthogonal, and D is a diagonal matrix that contains singular values. PCA is closely related to the right0singular vectors of SVD.","title":"Singular Value Decomposition (SVD)"},{"location":"glossary/dictionary/#soft-classification","text":"[ back to top ] The general goal of a pattern classification is to assign a pre-defined class labels to particular observations. Typically, in (hard) classification, only one class label is assigned to every instance whereas in soft classification, an instance can have multiple class labels. The degree to which an instance belongs to different classes is then defined by a so-called membership function.","title":"Soft classification"},{"location":"glossary/dictionary/#specificity","text":"[ back to top ] Specificity (synonymous to recall ), which is related to sensitivity -- in the context of error rate evaluation -- describes the \"True Negative Rate\" for a binary classification problem: The probability to make a correct prediction for a \"false/negative\" case (e.g., in an attempt to predict a disease, no disease is predicted for a healthy patient). Specificity is calculated as (TN)/(FP+TN), where TN=True Negatives, FP=False Positives.","title":"Specificity"},{"location":"glossary/dictionary/#standard-deviation","text":"[ back to top ] \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}","title":"Standard deviation"},{"location":"glossary/dictionary/#stochastic-gradient-descent-sgd","text":"[ back to top ] Stochastic Gradient Descent (SGD) (also see Gradient Descent ) is a machine learning algorithm that seeks to minimize an objective (or cost) function and can be grouped into the category of linear classifiers for supervised learning tasks. In contrast to Batch Gradient Descent , the gradient is computed from a single sample.","title":"Stochastic Gradient Descent (SGD)"},{"location":"glossary/dictionary/#supervised-learning","text":"[ back to top ] The problem of inferring a mapping between the input space X and a target variable y when given labelled training data (i.e. (X,y) pairs). Encompasses the problems of classification (categorical y) and regression (continuous y).","title":"Supervised Learning"},{"location":"glossary/dictionary/#support-vector-machine","text":"[ back to top ] SMV is a classification method that tries to find the hyperplane which separates classes with highest margin. The margin is defined as the minimum distance from sample points to the hyperplane. The sample point(s) that form margin are called support vectors and eventually establish the SVM model.","title":"Support Vector Machine"},{"location":"glossary/dictionary/#t","text":"","title":"T"},{"location":"glossary/dictionary/#term-frequency-and-document-frequency","text":"[ back to top ] Term frequency and document frequency are commonly used measures in context of text classification tasks. Term frequency is the count of how often a particular word occurs in a particular document. In contrast, document frequency measures the presence or absence of a particular word in a document as a binary value. Thus, for a single document, the document frequency is either 1 or 0.","title":"Term frequency and document frequency"},{"location":"glossary/dictionary/#term-frequency-inverse-document-frequency-tf-idf","text":"[ back to top ] Term frequency - inverse document frequency (Tf-idf) is a weighting scheme for term frequencies and document frequencies in text classification tasks that favors terms that occur in relatively few documents. The Tf-idf is calculated as a simple product of term frequency and the inverse document frequency, and the latter is calculated is calculated by log(\"number of documents in total\" / \"number of documents that contain a particular term\").","title":"Term frequency - inverse document frequency, Tf-idf"},{"location":"glossary/dictionary/#tokenization","text":"[ back to top ] Tokenization, in the context of natural language processing (NLP) is the process of breaking down a text into individual elements which can consist of words or symbols. Tokenization is usually accompanied by other processing procedures such as stemming, the removal of stop words, or the creation of n-grams.","title":"Tokenization"},{"location":"glossary/dictionary/#u","text":"","title":"U"},{"location":"glossary/dictionary/#unsupervised-learning","text":"[ back to top ] The problem of inferring latent structure in data when not given any training cases. Encompasses the problems of clustering, dimensionality reduction and density estimation.","title":"Unsupervised Learning"},{"location":"glossary/dictionary/#v","text":"","title":"V"},{"location":"glossary/dictionary/#variance","text":"[ back to top ] \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad \\sigma{_x}^{2} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\quad","title":"Variance"},{"location":"glossary/dictionary/#w","text":"","title":"W"},{"location":"glossary/dictionary/#white-noise","text":"[ back to top ] White noise is a source that produces random, statistically independent variables following a particular distribution. In the field of sound processing, white noise is also often referred to as a mixture of tones or sounds of different frequencies.","title":"White noise"},{"location":"glossary/dictionary/#whitening-transformation","text":"[ back to top ] Whitening transformation is a normalization procedure to de-correlate samples in a dataset if the covariance matrix is not a diagonal matrix. Features are uncorrelated after \"whitening\" and their variances are equal unity, thus the covariance matrix becomes an identity matrix.","title":"Whitening transformation"},{"location":"glossary/dictionary/#z","text":"","title":"Z"},{"location":"glossary/dictionary/#z-score","text":"[ back to top ] z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma}","title":"Z-score"},{"location":"methodology/Customer-Acceptance/","text":"Veniam elit enim eiusmod exercitation id officia aute eiusmod velit in eiusmod quis. Consectetur sint nulla consectetur ex id laborum do nisi. Velit id enim ut ipsum consequat elit deserunt aute sunt. Laboris quis fugiat est sint. Quis proident voluptate ut proident irure qui nostrud fugiat. Sit anim veniam proident ad cupidatat consectetur velit. Ullamco incididunt dolor velit quis ipsum qui sunt. Officia ea deserunt amet ut dolore enim voluptate nisi qui quis qui dolore. Adipisicing duis deserunt eiusmod do culpa in magna labore laboris sunt fugiat amet dolore. Dolor eu irure dolor qui sint eiusmod. Labore Lorem cupidatat laboris nisi non enim irure do. In aliqua sint non adipisicing nulla id non. Quis nulla aliquip tempor do. Mollit sint officia est dolor aute anim consectetur laborum labore minim ut enim in in. Dolore veniam consequat commodo duis enim dolore Lorem sit. Aliqua esse veniam cupidatat ex reprehenderit qui minim. Esse labore voluptate quis aute ea. Dolor excepteur adipisicing consectetur deserunt esse. Esse labore duis aliquip ut ut exercitation nostrud proident. Amet adipisicing qui exercitation occaecat eiusmod fugiat. Laboris do cillum fugiat est enim laboris pariatur nostrud labore eiusmod qui dolor. Deserunt adipisicing mollit ad veniam excepteur elit nisi laborum. Sit laborum amet qui esse duis nulla duis. Voluptate exercitation minim qui laboris velit consequat veniam laborum quis Lorem proident. Esse ut sit aliqua commodo irure consequat duis nisi voluptate pariatur nostrud sunt. Mollit consequat exercitation ut enim duis duis minim aute amet. Proident et et ea officia mollit consequat mollit aliqua cillum sunt eiusmod. Eu ut do nostrud aliquip culpa duis adipisicing et non ex aute aliqua. Proident aliqua culpa nostrud consequat ea laboris aliqua ipsum eiusmod proident enim non. Incididunt aliqua id amet non excepteur exercitation. Cillum Lorem fugiat aliquip esse officia cillum cillum incididunt. Non Lorem eiusmod qui amet nisi. Ut eu ea deserunt sint non dolore ad proident. Id fugiat duis aute proident magna ullamco magna Lorem voluptate magna fugiat qui. Culpa aute aliquip excepteur reprehenderit deserunt culpa non duis ut. Ut occaecat labore dolor labore fugiat proident commodo excepteur ea sunt veniam esse irure. Cillum incididunt consequat proident quis veniam fugiat consequat est eu adipisicing id laborum. Exercitation cupidatat dolore aliquip magna in officia minim magna. Consequat tempor consequat pariatur voluptate elit deserunt amet do. Sit voluptate Lorem ullamco dolor nisi irure. Reprehenderit sunt adipisicing consequat velit qui esse id deserunt officia laboris dolore cupidatat elit. Anim ipsum irure nostrud culpa culpa nostrud. Reprehenderit non consequat Lorem nisi magna. Aliquip tempor id exercitation elit non tempor ex. Qui pariatur nisi reprehenderit ullamco consectetur amet ea sunt. Labore magna occaecat excepteur adipisicing ea dolore deserunt. Veniam nostrud in tempor ex do duis ad magna aliquip do cupidatat. Ipsum esse irure in in adipisicing incididunt et Lorem dolore ad nostrud culpa Lorem enim. Reprehenderit aliqua commodo velit nisi dolore adipisicing reprehenderit dolore culpa. Officia consequat laborum eiusmod ea proident eu veniam cillum ad enim ullamco irure. Est pariatur pariatur ad nulla laboris eiusmod. Et nostrud et magna elit quis aliquip. Elit quis magna deserunt aliquip in tempor commodo fugiat id ut minim non enim. Occaecat ea et consequat anim sit cillum.","title":"Customer Acceptance"},{"location":"methodology/Data-Science-Roles-%26-Tasks/","text":"https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/roles-tasks.md Team Data Science Process: Roles and tasks \u00b6 This document outlines the key personnel roles and their associated tasks. Definition of Four TDSP Roles Group Manager (Leader of Digital Factory) - Is the manager of our entire DS unit. Team Lead (DS coordinator) - Is the manager of a team in a DS unit. DS Teams - Multiple Data Scientists. Project Lead (Scrum Master) - Manages the daily activities of individual DS on a specific DS project. Project Individual Contributor - Is our team members, compose by: Data Scientists, Data Engineering, Designers, Scrum Masters, etc. OBS : The team lead and Goup Manager can be the same person. Definition of the tasks by TDSP Roles Group Manager - Create a Group Account on code hosting platform, like GitHub - Create a Project Template Repository on the group account. - Create a Utility Repository , utilities to make the work of DS more efficient. - Create the Security Control Policy of these repositories on our group account. Team Lead - Create the Team Project Template Repository about the team project. - Create the Team Utility Repository* and add some team-specific utilities to the repository. - Create the Azure File Storage & Data Science Virtual Machine to store useful data assets. - Set up the Security Control by adding team members and configure their privileges. Project Lead: - Create a Project Repository under the team project, seed it from the Team project template repository. - Create a Azure File Storage & DSVM to store data assets of the project. - Set up the Security Control by adding project members and configure their privileges. Project Individual Contributor - Clone the Project Repository set up by the Project Lead . - Mount the shared Azure File Storage and DSVM* . - Execute the project. DS Project Execution - All members, excluding the manager, can create work items to track all tasks and stages of the project. - Essential Version Control in these Artifacts. - Sprint Planning (Project Lead - Scrum Master) - Developing artifacts on Git Branches to address work items (DS) - Code Review and merging branches with master (Data Engineer) Ref: Group Manager Detail Link : https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/group-manager-tasks.md Team Lead Tasks Detail Link: https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/team-lead-tasks.md Project Lead Tasks Detail Link: https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-lead-tasks.md Project Individual Contributor Detail Link https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-ic-tasks.md Execution https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-execution.md","title":"Data Science Roles & Tasks"},{"location":"methodology/Data-Science-Roles-%26-Tasks/#team-data-science-process-roles-and-tasks","text":"This document outlines the key personnel roles and their associated tasks. Definition of Four TDSP Roles Group Manager (Leader of Digital Factory) - Is the manager of our entire DS unit. Team Lead (DS coordinator) - Is the manager of a team in a DS unit. DS Teams - Multiple Data Scientists. Project Lead (Scrum Master) - Manages the daily activities of individual DS on a specific DS project. Project Individual Contributor - Is our team members, compose by: Data Scientists, Data Engineering, Designers, Scrum Masters, etc. OBS : The team lead and Goup Manager can be the same person. Definition of the tasks by TDSP Roles Group Manager - Create a Group Account on code hosting platform, like GitHub - Create a Project Template Repository on the group account. - Create a Utility Repository , utilities to make the work of DS more efficient. - Create the Security Control Policy of these repositories on our group account. Team Lead - Create the Team Project Template Repository about the team project. - Create the Team Utility Repository* and add some team-specific utilities to the repository. - Create the Azure File Storage & Data Science Virtual Machine to store useful data assets. - Set up the Security Control by adding team members and configure their privileges. Project Lead: - Create a Project Repository under the team project, seed it from the Team project template repository. - Create a Azure File Storage & DSVM to store data assets of the project. - Set up the Security Control by adding project members and configure their privileges. Project Individual Contributor - Clone the Project Repository set up by the Project Lead . - Mount the shared Azure File Storage and DSVM* . - Execute the project. DS Project Execution - All members, excluding the manager, can create work items to track all tasks and stages of the project. - Essential Version Control in these Artifacts. - Sprint Planning (Project Lead - Scrum Master) - Developing artifacts on Git Branches to address work items (DS) - Code Review and merging branches with master (Data Engineer) Ref: Group Manager Detail Link : https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/group-manager-tasks.md Team Lead Tasks Detail Link: https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/team-lead-tasks.md Project Lead Tasks Detail Link: https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-lead-tasks.md Project Individual Contributor Detail Link https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-ic-tasks.md Execution https://github.com/Azure/Microsoft-TDSP/blob/master/Docs/project-execution.md","title":"Team Data Science Process: Roles and tasks"},{"location":"methodology/Data-Science-Utilities/","text":"https://github.com/Azure/Azure-TDSP-Utilities Data Science Utilities \u00b6 Using the IDEAR format in Python \u00b6 The **I**teractive **D**ata **E**xploratory **A**nalysis and **R**eporting is a utility developed for DS to visualize, analyze and get good insights about the data sets.","title":"Data Science Utilities"},{"location":"methodology/Data-Science-Utilities/#data-science-utilities","text":"","title":"Data Science Utilities"},{"location":"methodology/Data-Science-Utilities/#using-the-idear-format-in-python","text":"The **I**teractive **D**ata **E**xploratory **A**nalysis and **R**eporting is a utility developed for DS to visualize, analyze and get good insights about the data sets.","title":"Using the IDEAR format in Python"},{"location":"methodology/Deployment/","text":"Goal \u00b6 Deploy models with a data pipeline to a production or production-like environment for final user acceptance. \u00b6 Operationalize a model \u00b6 After you have a set of models that perform well, you can operationalize them for other applications to consume. Depending on the business requirements, predictions are made either in real time or on a batch basis. To deploy models, you expose them with an open API interface. The interface enables the model to be easily consumed from various applications, such as: Online websites Spreadsheets Dashboards Line-of-business applications Back-end applications For examples of model operationalization with an Azure Machine Learning web service, see Deploy an Azure Machine Learning web service. It is a best practice to build telemetry and monitoring into the production model and the data pipeline that you deploy. This practice helps with subsequent system status reporting and troubleshooting.","title":"Goal"},{"location":"methodology/Deployment/#goal","text":"","title":"Goal"},{"location":"methodology/Deployment/#deploy-models-with-a-data-pipeline-to-a-production-or-production-like-environment-for-final-user-acceptance","text":"","title":"Deploy models with a data pipeline to a production or production-like environment for final user acceptance."},{"location":"methodology/Deployment/#operationalize-a-model","text":"After you have a set of models that perform well, you can operationalize them for other applications to consume. Depending on the business requirements, predictions are made either in real time or on a batch basis. To deploy models, you expose them with an open API interface. The interface enables the model to be easily consumed from various applications, such as: Online websites Spreadsheets Dashboards Line-of-business applications Back-end applications For examples of model operationalization with an Azure Machine Learning web service, see Deploy an Azure Machine Learning web service. It is a best practice to build telemetry and monitoring into the production model and the data pipeline that you deploy. This practice helps with subsequent system status reporting and troubleshooting.","title":"Operationalize a model"},{"location":"methodology/Documents-and-Artifacts-templates/","text":"https://github.com/Azure/Azure-TDSP-ProjectTemplate Veniam elit enim eiusmod exercitation id officia aute eiusmod velit in eiusmod quis. Consectetur sint nulla consectetur ex id laborum do nisi. Velit id enim ut ipsum consequat elit deserunt aute sunt. Laboris quis fugiat est sint. Quis proident voluptate ut proident irure qui nostrud fugiat. Sit anim veniam proident ad cupidatat consectetur velit. Ullamco incididunt dolor velit quis ipsum qui sunt. Officia ea deserunt amet ut dolore enim voluptate nisi qui quis qui dolore. Adipisicing duis deserunt eiusmod do culpa in magna labore laboris sunt fugiat amet dolore. Dolor eu irure dolor qui sint eiusmod. Labore Lorem cupidatat laboris nisi non enim irure do. In aliqua sint non adipisicing nulla id non. Quis nulla aliquip tempor do. Mollit sint officia est dolor aute anim consectetur laborum labore minim ut enim in in. Dolore veniam consequat commodo duis enim dolore Lorem sit. Aliqua esse veniam cupidatat ex reprehenderit qui minim. Esse labore voluptate quis aute ea. Dolor excepteur adipisicing consectetur deserunt esse. Esse labore duis aliquip ut ut exercitation nostrud proident. Amet adipisicing qui exercitation occaecat eiusmod fugiat. Laboris do cillum fugiat est enim laboris pariatur nostrud labore eiusmod qui dolor. Deserunt adipisicing mollit ad veniam excepteur elit nisi laborum. Sit laborum amet qui esse duis nulla duis. Voluptate exercitation minim qui laboris velit consequat veniam laborum quis Lorem proident. Esse ut sit aliqua commodo irure consequat duis nisi voluptate pariatur nostrud sunt. Mollit consequat exercitation ut enim duis duis minim aute amet. Proident et et ea officia mollit consequat mollit aliqua cillum sunt eiusmod. Eu ut do nostrud aliquip culpa duis adipisicing et non ex aute aliqua. Proident aliqua culpa nostrud consequat ea laboris aliqua ipsum eiusmod proident enim non. Incididunt aliqua id amet non excepteur exercitation. Cillum Lorem fugiat aliquip esse officia cillum cillum incididunt. Non Lorem eiusmod qui amet nisi. Ut eu ea deserunt sint non dolore ad proident. Id fugiat duis aute proident magna ullamco magna Lorem voluptate magna fugiat qui. Culpa aute aliquip excepteur reprehenderit deserunt culpa non duis ut. Ut occaecat labore dolor labore fugiat proident commodo excepteur ea sunt veniam esse irure. Cillum incididunt consequat proident quis veniam fugiat consequat est eu adipisicing id laborum. Exercitation cupidatat dolore aliquip magna in officia minim magna. Consequat tempor consequat pariatur voluptate elit deserunt amet do. Sit voluptate Lorem ullamco dolor nisi irure. Reprehenderit sunt adipisicing consequat velit qui esse id deserunt officia laboris dolore cupidatat elit. Anim ipsum irure nostrud culpa culpa nostrud. Reprehenderit non consequat Lorem nisi magna. Aliquip tempor id exercitation elit non tempor ex. Qui pariatur nisi reprehenderit ullamco consectetur amet ea sunt. Labore magna occaecat excepteur adipisicing ea dolore deserunt. Veniam nostrud in tempor ex do duis ad magna aliquip do cupidatat. Ipsum esse irure in in adipisicing incididunt et Lorem dolore ad nostrud culpa Lorem enim. Reprehenderit aliqua commodo velit nisi dolore adipisicing reprehenderit dolore culpa. Officia consequat laborum eiusmod ea proident eu veniam cillum ad enim ullamco irure. Est pariatur pariatur ad nulla laboris eiusmod. Et nostrud et magna elit quis aliquip. Elit quis magna deserunt aliquip in tempor commodo fugiat id ut minim non enim. Occaecat ea et consequat anim sit cillum.","title":"Documents and Artifacts templates"},{"location":"methodology/FAQ/","text":"Veniam elit enim eiusmod exercitation id officia aute eiusmod velit in eiusmod quis. Consectetur sint nulla consectetur ex id laborum do nisi. Velit id enim ut ipsum consequat elit deserunt aute sunt. Laboris quis fugiat est sint. Quis proident voluptate ut proident irure qui nostrud fugiat. Sit anim veniam proident ad cupidatat consectetur velit. Ullamco incididunt dolor velit quis ipsum qui sunt. Officia ea deserunt amet ut dolore enim voluptate nisi qui quis qui dolore. Adipisicing duis deserunt eiusmod do culpa in magna labore laboris sunt fugiat amet dolore. Dolor eu irure dolor qui sint eiusmod. Labore Lorem cupidatat laboris nisi non enim irure do. In aliqua sint non adipisicing nulla id non. Quis nulla aliquip tempor do. Mollit sint officia est dolor aute anim consectetur laborum labore minim ut enim in in. Dolore veniam consequat commodo duis enim dolore Lorem sit. Aliqua esse veniam cupidatat ex reprehenderit qui minim. Esse labore voluptate quis aute ea. Dolor excepteur adipisicing consectetur deserunt esse. Esse labore duis aliquip ut ut exercitation nostrud proident. Amet adipisicing qui exercitation occaecat eiusmod fugiat. Laboris do cillum fugiat est enim laboris pariatur nostrud labore eiusmod qui dolor. Deserunt adipisicing mollit ad veniam excepteur elit nisi laborum. Sit laborum amet qui esse duis nulla duis. Voluptate exercitation minim qui laboris velit consequat veniam laborum quis Lorem proident. Esse ut sit aliqua commodo irure consequat duis nisi voluptate pariatur nostrud sunt. Mollit consequat exercitation ut enim duis duis minim aute amet. Proident et et ea officia mollit consequat mollit aliqua cillum sunt eiusmod. Eu ut do nostrud aliquip culpa duis adipisicing et non ex aute aliqua. Proident aliqua culpa nostrud consequat ea laboris aliqua ipsum eiusmod proident enim non. Incididunt aliqua id amet non excepteur exercitation. Cillum Lorem fugiat aliquip esse officia cillum cillum incididunt. Non Lorem eiusmod qui amet nisi. Ut eu ea deserunt sint non dolore ad proident. Id fugiat duis aute proident magna ullamco magna Lorem voluptate magna fugiat qui. Culpa aute aliquip excepteur reprehenderit deserunt culpa non duis ut. Ut occaecat labore dolor labore fugiat proident commodo excepteur ea sunt veniam esse irure. Cillum incididunt consequat proident quis veniam fugiat consequat est eu adipisicing id laborum. Exercitation cupidatat dolore aliquip magna in officia minim magna. Consequat tempor consequat pariatur voluptate elit deserunt amet do. Sit voluptate Lorem ullamco dolor nisi irure. Reprehenderit sunt adipisicing consequat velit qui esse id deserunt officia laboris dolore cupidatat elit. Anim ipsum irure nostrud culpa culpa nostrud. Reprehenderit non consequat Lorem nisi magna. Aliquip tempor id exercitation elit non tempor ex. Qui pariatur nisi reprehenderit ullamco consectetur amet ea sunt. Labore magna occaecat excepteur adipisicing ea dolore deserunt. Veniam nostrud in tempor ex do duis ad magna aliquip do cupidatat. Ipsum esse irure in in adipisicing incididunt et Lorem dolore ad nostrud culpa Lorem enim. Reprehenderit aliqua commodo velit nisi dolore adipisicing reprehenderit dolore culpa. Officia consequat laborum eiusmod ea proident eu veniam cillum ad enim ullamco irure. Est pariatur pariatur ad nulla laboris eiusmod. Et nostrud et magna elit quis aliquip. Elit quis magna deserunt aliquip in tempor commodo fugiat id ut minim non enim. Occaecat ea et consequat anim sit cillum.","title":"FAQ"},{"location":"methodology/Framing/","text":"Veniam elit enim eiusmod exercitation id officia aute eiusmod velit in eiusmod quis. Consectetur sint nulla consectetur ex id laborum do nisi. Velit id enim ut ipsum consequat elit deserunt aute sunt. Laboris quis fugiat est sint. Quis proident voluptate ut proident irure qui nostrud fugiat. Sit anim veniam proident ad cupidatat consectetur velit. Ullamco incididunt dolor velit quis ipsum qui sunt. Officia ea deserunt amet ut dolore enim voluptate nisi qui quis qui dolore. Adipisicing duis deserunt eiusmod do culpa in magna labore laboris sunt fugiat amet dolore. Dolor eu irure dolor qui sint eiusmod. Labore Lorem cupidatat laboris nisi non enim irure do. In aliqua sint non adipisicing nulla id non. Quis nulla aliquip tempor do. Mollit sint officia est dolor aute anim consectetur laborum labore minim ut enim in in. Dolore veniam consequat commodo duis enim dolore Lorem sit. Aliqua esse veniam cupidatat ex reprehenderit qui minim. Esse labore voluptate quis aute ea. Dolor excepteur adipisicing consectetur deserunt esse. Esse labore duis aliquip ut ut exercitation nostrud proident. Amet adipisicing qui exercitation occaecat eiusmod fugiat. Laboris do cillum fugiat est enim laboris pariatur nostrud labore eiusmod qui dolor. Deserunt adipisicing mollit ad veniam excepteur elit nisi laborum. Sit laborum amet qui esse duis nulla duis. Voluptate exercitation minim qui laboris velit consequat veniam laborum quis Lorem proident. Esse ut sit aliqua commodo irure consequat duis nisi voluptate pariatur nostrud sunt. Mollit consequat exercitation ut enim duis duis minim aute amet. Proident et et ea officia mollit consequat mollit aliqua cillum sunt eiusmod. Eu ut do nostrud aliquip culpa duis adipisicing et non ex aute aliqua. Proident aliqua culpa nostrud consequat ea laboris aliqua ipsum eiusmod proident enim non. Incididunt aliqua id amet non excepteur exercitation. Cillum Lorem fugiat aliquip esse officia cillum cillum incididunt. Non Lorem eiusmod qui amet nisi. Ut eu ea deserunt sint non dolore ad proident. Id fugiat duis aute proident magna ullamco magna Lorem voluptate magna fugiat qui. Culpa aute aliquip excepteur reprehenderit deserunt culpa non duis ut. Ut occaecat labore dolor labore fugiat proident commodo excepteur ea sunt veniam esse irure. Cillum incididunt consequat proident quis veniam fugiat consequat est eu adipisicing id laborum. Exercitation cupidatat dolore aliquip magna in officia minim magna. Consequat tempor consequat pariatur voluptate elit deserunt amet do. Sit voluptate Lorem ullamco dolor nisi irure. Reprehenderit sunt adipisicing consequat velit qui esse id deserunt officia laboris dolore cupidatat elit. Anim ipsum irure nostrud culpa culpa nostrud. Reprehenderit non consequat Lorem nisi magna. Aliquip tempor id exercitation elit non tempor ex. Qui pariatur nisi reprehenderit ullamco consectetur amet ea sunt. Labore magna occaecat excepteur adipisicing ea dolore deserunt. Veniam nostrud in tempor ex do duis ad magna aliquip do cupidatat. Ipsum esse irure in in adipisicing incididunt et Lorem dolore ad nostrud culpa Lorem enim. Reprehenderit aliqua commodo velit nisi dolore adipisicing reprehenderit dolore culpa. Officia consequat laborum eiusmod ea proident eu veniam cillum ad enim ullamco irure. Est pariatur pariatur ad nulla laboris eiusmod. Et nostrud et magna elit quis aliquip. Elit quis magna deserunt aliquip in tempor commodo fugiat id ut minim non enim. Occaecat ea et consequat anim sit cillum.","title":"Framing"},{"location":"methodology/MVP/","text":"Veniam elit enim eiusmod exercitation id officia aute eiusmod velit in eiusmod quis. Consectetur sint nulla consectetur ex id laborum do nisi. Velit id enim ut ipsum consequat elit deserunt aute sunt. Laboris quis fugiat est sint. Quis proident voluptate ut proident irure qui nostrud fugiat. Sit anim veniam proident ad cupidatat consectetur velit. Ullamco incididunt dolor velit quis ipsum qui sunt. Officia ea deserunt amet ut dolore enim voluptate nisi qui quis qui dolore. Adipisicing duis deserunt eiusmod do culpa in magna labore laboris sunt fugiat amet dolore. Dolor eu irure dolor qui sint eiusmod. Labore Lorem cupidatat laboris nisi non enim irure do. In aliqua sint non adipisicing nulla id non. Quis nulla aliquip tempor do. Mollit sint officia est dolor aute anim consectetur laborum labore minim ut enim in in. Dolore veniam consequat commodo duis enim dolore Lorem sit. Aliqua esse veniam cupidatat ex reprehenderit qui minim. Esse labore voluptate quis aute ea. Dolor excepteur adipisicing consectetur deserunt esse. Esse labore duis aliquip ut ut exercitation nostrud proident. Amet adipisicing qui exercitation occaecat eiusmod fugiat. Laboris do cillum fugiat est enim laboris pariatur nostrud labore eiusmod qui dolor. Deserunt adipisicing mollit ad veniam excepteur elit nisi laborum. Sit laborum amet qui esse duis nulla duis. Voluptate exercitation minim qui laboris velit consequat veniam laborum quis Lorem proident. Esse ut sit aliqua commodo irure consequat duis nisi voluptate pariatur nostrud sunt. Mollit consequat exercitation ut enim duis duis minim aute amet. Proident et et ea officia mollit consequat mollit aliqua cillum sunt eiusmod. Eu ut do nostrud aliquip culpa duis adipisicing et non ex aute aliqua. Proident aliqua culpa nostrud consequat ea laboris aliqua ipsum eiusmod proident enim non. Incididunt aliqua id amet non excepteur exercitation. Cillum Lorem fugiat aliquip esse officia cillum cillum incididunt. Non Lorem eiusmod qui amet nisi. Ut eu ea deserunt sint non dolore ad proident. Id fugiat duis aute proident magna ullamco magna Lorem voluptate magna fugiat qui. Culpa aute aliquip excepteur reprehenderit deserunt culpa non duis ut. Ut occaecat labore dolor labore fugiat proident commodo excepteur ea sunt veniam esse irure. Cillum incididunt consequat proident quis veniam fugiat consequat est eu adipisicing id laborum. Exercitation cupidatat dolore aliquip magna in officia minim magna. Consequat tempor consequat pariatur voluptate elit deserunt amet do. Sit voluptate Lorem ullamco dolor nisi irure. Reprehenderit sunt adipisicing consequat velit qui esse id deserunt officia laboris dolore cupidatat elit. Anim ipsum irure nostrud culpa culpa nostrud. Reprehenderit non consequat Lorem nisi magna. Aliquip tempor id exercitation elit non tempor ex. Qui pariatur nisi reprehenderit ullamco consectetur amet ea sunt. Labore magna occaecat excepteur adipisicing ea dolore deserunt. Veniam nostrud in tempor ex do duis ad magna aliquip do cupidatat. Ipsum esse irure in in adipisicing incididunt et Lorem dolore ad nostrud culpa Lorem enim. Reprehenderit aliqua commodo velit nisi dolore adipisicing reprehenderit dolore culpa. Officia consequat laborum eiusmod ea proident eu veniam cillum ad enim ullamco irure. Est pariatur pariatur ad nulla laboris eiusmod. Et nostrud et magna elit quis aliquip. Elit quis magna deserunt aliquip in tempor commodo fugiat id ut minim non enim. Occaecat ea et consequat anim sit cillum.","title":"MVP"},{"location":"methodology/Overview/","text":"Data Science Process Lifecycle Is an iterative agile methodology, the main goal is to deliver predictive analytics and intelligent applications efficiently. Helps improve team collaboration and it's good to understand how team works best together. This Document has the goal to create a solid Data Science Licycle definition , with a Standardized Project Structure , well defined Infraestructure and Resources for DS projects and Tools and Utilities to manage the project execution. In this Lifecyle we have four phases : - Business Understanding - Data Acquisition and Understanding - Modeling - Deployment - Customer Acceptance We will define the Goals, Tasks and Documentation Artifacts for each role in this DS lifecycle, the roles will be define in another topic. Roles in DS Lifecycle: - Project manager - Project lead - Solution architect - Data scientist Visual Example about roles, tasks and acceptance criteria Project Structure All Projects must be standardized, in a directory with templates available, so it will be easy to team members find crucial information about any project. Codes and Documents will be storage in a Version Control System (VCS), we use the Azure DevOps environment (Repos). This will allow teams to closer-control codes for individual features. It's good to create separate repositories for each project in the Version Control System, so you can have more information security and collaboration. Infrastructure and Resources Our Azure infrastructure provide resources to store our raw data and to process datasets, this structure enable reproducible analysis, avoids duplication and unnecessary infrastructure costs. Tools and Utilities This tools and utilities are designed to provide a fast pace in common tasks in the data science lifecycle, such as data exploration, modeling and acceptance criteria reports. Ref: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/","title":"Overview"},{"location":"methodology/Source-control-and-CI%252DCD/","text":"Source Control \u00b6 This is and essential tool for Data Science Team and allow developers collaborate on code and track changes that are made to the code base. Ability to track|audit changes Ability to revert changes that introduced bugs Continuos Integration and Continuos Delivery/Deployment (CI/CD) \u00b6 Continuous integration is the practice of testing each change made to your codebase automatically and as early as possible. Continuous delivery follows the testing that happens during continuous integration and pushes changes to a staging or production system. In Azure Data Factory, CI/CD means to moving Data Factory pipelines from one environment (development, test, production) to another. How is the CI/CD Lifecycle? \u00b6 Data Facrtory is created and configured with Azure Repos Git. All members should have permision to access Data Factory resources. When developers made changes in their feature branches, they should debug their pipelines with the recent changes. When changes satisfy the developers, they create the Pull-Request from their feature branch to the master branch . The changes will be reviewed by peers. After the pull is approved and the changes are merged in the master, the changes are published in the development factory. When the team is ready to deploy the changes to the test factory and then to the production factory, the team exports the Resource Manager template from the master branch. The exported Resource Manager template is deployed with different parameter files to the test factory and the production factory. Best Practices for Git Integration \u00b6 All members should have read permission to the Data Factory Only a few members should be allow to publish to the Data Factory environment. Ref: https://docs.microsoft.com/en-us/azure/data-factory/source-control https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment","title":"Source control and CI%2DCD"},{"location":"methodology/Source-control-and-CI%252DCD/#source-control","text":"This is and essential tool for Data Science Team and allow developers collaborate on code and track changes that are made to the code base. Ability to track|audit changes Ability to revert changes that introduced bugs","title":"Source Control"},{"location":"methodology/Source-control-and-CI%252DCD/#continuos-integration-and-continuos-deliverydeployment-cicd","text":"Continuous integration is the practice of testing each change made to your codebase automatically and as early as possible. Continuous delivery follows the testing that happens during continuous integration and pushes changes to a staging or production system. In Azure Data Factory, CI/CD means to moving Data Factory pipelines from one environment (development, test, production) to another.","title":"Continuos Integration and Continuos Delivery/Deployment (CI/CD)"},{"location":"methodology/Source-control-and-CI%252DCD/#how-is-the-cicd-lifecycle","text":"Data Facrtory is created and configured with Azure Repos Git. All members should have permision to access Data Factory resources. When developers made changes in their feature branches, they should debug their pipelines with the recent changes. When changes satisfy the developers, they create the Pull-Request from their feature branch to the master branch . The changes will be reviewed by peers. After the pull is approved and the changes are merged in the master, the changes are published in the development factory. When the team is ready to deploy the changes to the test factory and then to the production factory, the team exports the Resource Manager template from the master branch. The exported Resource Manager template is deployed with different parameter files to the test factory and the production factory.","title":"How is the CI/CD Lifecycle?"},{"location":"methodology/Source-control-and-CI%252DCD/#best-practices-for-git-integration","text":"All members should have read permission to the Data Factory Only a few members should be allow to publish to the Data Factory environment. Ref: https://docs.microsoft.com/en-us/azure/data-factory/source-control https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment","title":"Best Practices for Git Integration"},{"location":"methodology/Customer-Acceptance/Retrain-ML-model/","text":"","title":"Retrain ML model"},{"location":"methodology/Deployment/Final-deployment-and-architecture/","text":"Goal \u00b6 Confirm that the pipeline, the model, and their deployment in a production environment satisfy the customer's objectives & consumption. Strategy About the Infrastructure Maintenance \u00b6 Our infrastructure is based on modern warehouse format, the components are described below: - Data Factory: Data Flow - Data Lake Storage: Structure Files - Databricks: Data Processing - SQL Data Warehouse: Data Storage - Analysis Service: Data Consolidation - Automated Pipeline Version and Production control","title":"Final deployment and architecture"},{"location":"methodology/Deployment/Final-deployment-and-architecture/#goal","text":"Confirm that the pipeline, the model, and their deployment in a production environment satisfy the customer's objectives & consumption.","title":"Goal"},{"location":"methodology/Deployment/Final-deployment-and-architecture/#strategy-about-the-infrastructure-maintenance","text":"Our infrastructure is based on modern warehouse format, the components are described below: - Data Factory: Data Flow - Data Lake Storage: Structure Files - Databricks: Data Processing - SQL Data Warehouse: Data Storage - Analysis Service: Data Consolidation - Automated Pipeline Version and Production control","title":"Strategy About the Infrastructure Maintenance"},{"location":"methodology/Framing/Business-Understanding/","text":"","title":"Business Understanding"},{"location":"methodology/Framing/Business-Understanding/Artifacts/","text":"Here are the deliverables in this stage: \u00b6 Charter document: A standard template is provided in the TDSP project structure definition. The charter document is a living document. You update the template throughout the project as you make new discoveries and as business requirements change. The key is to iterate upon this document, adding more detail, as you progress through the discovery process. Keep the customer and other stakeholders involved in making the changes and clearly communicate the reasons for the changes to them. Data sources: The Raw data sources section of the Data definitions report that's found in the TDSP project Data report folder contains the data sources. This section specifies the original and destination locations for the raw data. In later stages, you fill in additional details like the scripts to move the data to your analytic environment. Data dictionaries: This document provides descriptions of the data that's provided by the client. These descriptions include information about the schema (the data types and information on the validation rules, if any) and the entity-relation diagrams, if available.","title":"Artifacts"},{"location":"methodology/Framing/Business-Understanding/Artifacts/#here-are-the-deliverables-in-this-stage","text":"Charter document: A standard template is provided in the TDSP project structure definition. The charter document is a living document. You update the template throughout the project as you make new discoveries and as business requirements change. The key is to iterate upon this document, adding more detail, as you progress through the discovery process. Keep the customer and other stakeholders involved in making the changes and clearly communicate the reasons for the changes to them. Data sources: The Raw data sources section of the Data definitions report that's found in the TDSP project Data report folder contains the data sources. This section specifies the original and destination locations for the raw data. In later stages, you fill in additional details like the scripts to move the data to your analytic environment. Data dictionaries: This document provides descriptions of the data that's provided by the client. These descriptions include information about the schema (the data types and information on the validation rules, if any) and the entity-relation diagrams, if available.","title":"Here are the deliverables in this stage:"},{"location":"methodology/Framing/Business-Understanding/Define-Product-Development-Approach/","text":"GOAL \u00b6 Work with your customer and other stakeholders to understand and identify the business problems. \u00b6 Formulate questions that define the business goals that the data science techniques can target. \u00b6 Define objectives \u00b6 1. A central objective of this step is to identify the key business variables that the analysis needs to predict. We refer to these variables as the model targets, and we use the metrics associated with them to determine the success of the project. Two examples of such targets are sales forecasts or the probability of an order being fraudulent. 2. Define the project goals by asking and refining \"sharp\" questions that are relevant, specific, and unambiguous. Data science is a process that uses names and numbers to answer such questions. You typically use data science or machine learning to answer five types of questions: How much or how many? (regression) Which category? (classification) Which group? (clustering) Is this weird? (anomaly detection) Which option should be taken? (recommendation) Determine which of these questions you're asking and how answering it achieves your business goals. 3. Define the project team by specifying the roles and responsibilities of its members. Develop a high-level milestone plan that you iterate on as you discover more information. 4. Define the success metrics. For example, you might want to achieve a customer churn prediction. You need an accuracy rate of \"x\" percent by the end of this three-month project. With this data, you can offer customer promotions to reduce churn. The metrics must be SMART: **S**pecific **M**easurable **A**chievable **R**elevant **T**ime-bound","title":"GOAL"},{"location":"methodology/Framing/Business-Understanding/Define-Product-Development-Approach/#goal","text":"","title":"GOAL"},{"location":"methodology/Framing/Business-Understanding/Define-Product-Development-Approach/#work-with-your-customer-and-other-stakeholders-to-understand-and-identify-the-business-problems","text":"","title":"Work with your customer and other stakeholders to understand and identify the business problems."},{"location":"methodology/Framing/Business-Understanding/Define-Product-Development-Approach/#formulate-questions-that-define-the-business-goals-that-the-data-science-techniques-can-target","text":"","title":"Formulate questions that define the business goals that the data science techniques can target."},{"location":"methodology/Framing/Business-Understanding/Define-Product-Development-Approach/#define-objectives","text":"1. A central objective of this step is to identify the key business variables that the analysis needs to predict. We refer to these variables as the model targets, and we use the metrics associated with them to determine the success of the project. Two examples of such targets are sales forecasts or the probability of an order being fraudulent. 2. Define the project goals by asking and refining \"sharp\" questions that are relevant, specific, and unambiguous. Data science is a process that uses names and numbers to answer such questions. You typically use data science or machine learning to answer five types of questions: How much or how many? (regression) Which category? (classification) Which group? (clustering) Is this weird? (anomaly detection) Which option should be taken? (recommendation) Determine which of these questions you're asking and how answering it achieves your business goals. 3. Define the project team by specifying the roles and responsibilities of its members. Develop a high-level milestone plan that you iterate on as you discover more information. 4. Define the success metrics. For example, you might want to achieve a customer churn prediction. You need an accuracy rate of \"x\" percent by the end of this three-month project. With this data, you can offer customer promotions to reduce churn. The metrics must be SMART: **S**pecific **M**easurable **A**chievable **R**elevant **T**ime-bound","title":"Define objectives"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/","text":"GOALS \u00b6 Specify the key variables that are to serve as the model targets and whose related metrics are used determine the success of the project. \u00b6 Identify the relevant data sources that the business has access to or needs to obtain. \u00b6 Data Accessibility \u00b6 Is the data available - points to be considered below: What we have to do in order to have it available for the model online deployment? How much will it cost? How long will it take for us to have the data available? Can it be a roadblock for the deployment? Data Consistency \u00b6 What is the profile of the data that we have available? How does it behavior? Does it present any outliers? Is it influenced by external factors that are not [yet] measured? Can the behavior/outliers/external factors potentially be roadblocks for our initiative? Data Completeness \u00b6 Is the data workable? Does it require much wrangling? Do we have enough data points enough to create a model with a reasonable confidence level (considering too, accuracy)? Data Frequency \u00b6 Are there any infrastructure barriers or developments that need to be address now? Do we need to engineer a new pipeline? What tools and resources can we explore to extract data (e.g. Dariva\u00b4s solutions, ChemTech, Central PIMS, AT team) Does the event I am trying to capture occur at a frequency that can be captured by the available sensors? Is the data online? If necessary, can I change the data collection frequency so I can capture the event? Do I store this data in my databases, or where do I store the data?","title":"GOALS"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#goals","text":"","title":"GOALS"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#specify-the-key-variables-that-are-to-serve-as-the-model-targets-and-whose-related-metrics-are-used-determine-the-success-of-the-project","text":"","title":"Specify the key variables that are to serve as the model targets and whose related metrics are used determine the success of the project."},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#identify-the-relevant-data-sources-that-the-business-has-access-to-or-needs-to-obtain","text":"","title":"Identify the relevant data sources that the business has access to or needs to obtain."},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#data-accessibility","text":"Is the data available - points to be considered below: What we have to do in order to have it available for the model online deployment? How much will it cost? How long will it take for us to have the data available? Can it be a roadblock for the deployment?","title":"Data Accessibility"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#data-consistency","text":"What is the profile of the data that we have available? How does it behavior? Does it present any outliers? Is it influenced by external factors that are not [yet] measured? Can the behavior/outliers/external factors potentially be roadblocks for our initiative?","title":"Data Consistency"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#data-completeness","text":"Is the data workable? Does it require much wrangling? Do we have enough data points enough to create a model with a reasonable confidence level (considering too, accuracy)?","title":"Data Completeness"},{"location":"methodology/Framing/Business-Understanding/Feasibility-Analysis/#data-frequency","text":"Are there any infrastructure barriers or developments that need to be address now? Do we need to engineer a new pipeline? What tools and resources can we explore to extract data (e.g. Dariva\u00b4s solutions, ChemTech, Central PIMS, AT team) Does the event I am trying to capture occur at a frequency that can be captured by the available sensors? Is the data online? If necessary, can I change the data collection frequency so I can capture the event? Do I store this data in my databases, or where do I store the data?","title":"Data Frequency"},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/","text":"GOAL \u00b6 Find the relevant data that helps you answer the questions that define the objectives of the project. \u00b6 Identify data sources that contain known examples of answers to your sharp questions. Look for the following data: Data that's relevant to the question. Do you have measures of the target and features that are related to the target? Data that's an accurate measure of your model target and the features of interest. For example, you might find that the existing systems need to collect and log additional kinds of data to address the problem and achieve the project goals. In this situation, you might want to look for external data sources or update your systems to collect new data. Find the data owners that helps you to develop the infraestructure of the project. \u00b6 Does the data available belong to Braskem domain or belong to a third party? \u00b6 Braskem Domain: - Which system? (SAP? Excel Files? Pins?) - Who is the owner of this data?? - Who has the knowledge about this Data? Who can be consulted in case of doubts? Third Party: - What is the contract we have made with third parties? - What is the (contractual) latency of this data? - Which are the restrictions about the contract? These are relevant questions that you should know to minimize the problems relate to data ingestion. Find the Relevant Features from the Data & Data Source \u00b6 The Database is automated or manual. How often we will need to retrieve it (frequency). Streaming, batch or hybrid? How are we connecting to the data source? What is the schema and format of the data? What is the best cloud (Draft) solution for this pipeline?","title":"GOAL"},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/#goal","text":"","title":"GOAL"},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/#find-the-relevant-data-that-helps-you-answer-the-questions-that-define-the-objectives-of-the-project","text":"Identify data sources that contain known examples of answers to your sharp questions. Look for the following data: Data that's relevant to the question. Do you have measures of the target and features that are related to the target? Data that's an accurate measure of your model target and the features of interest. For example, you might find that the existing systems need to collect and log additional kinds of data to address the problem and achieve the project goals. In this situation, you might want to look for external data sources or update your systems to collect new data.","title":"Find the relevant data that helps you answer the questions that define the objectives of the project."},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/#find-the-data-owners-that-helps-you-to-develop-the-infraestructure-of-the-project","text":"","title":"Find the data owners that helps you to develop the infraestructure of the project."},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/#does-the-data-available-belong-to-braskem-domain-or-belong-to-a-third-party","text":"Braskem Domain: - Which system? (SAP? Excel Files? Pins?) - Who is the owner of this data?? - Who has the knowledge about this Data? Who can be consulted in case of doubts? Third Party: - What is the contract we have made with third parties? - What is the (contractual) latency of this data? - Which are the restrictions about the contract? These are relevant questions that you should know to minimize the problems relate to data ingestion.","title":"Does the data available belong to Braskem domain or belong to a third party?"},{"location":"methodology/Framing/Business-Understanding/Identify-Data-Sources/#find-the-relevant-features-from-the-data-data-source","text":"The Database is automated or manual. How often we will need to retrieve it (frequency). Streaming, batch or hybrid? How are we connecting to the data source? What is the schema and format of the data? What is the best cloud (Draft) solution for this pipeline?","title":"Find the Relevant Features from the Data &amp; Data Source"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/","text":"Goals \u00b6 Produce a clean, high-quality data set whose relationship to the target variables is understood. Locate the data set in the appropriate analytics environment so you are ready to model. \u00b6 Develop a solution architecture of the data pipeline that refreshes and scores the data regularly. \u00b6 Ingest the data \u00b6 Set up the process to move the data from the source locations to the target locations where you run analytics operations, like training and predictions. For technical details and options on how to move the data with various Azure data services, see Load data into storage environments for analytics. Explore the data \u00b6 Before you train your models, you need to develop a sound understanding of the data. Real-world data sets are often noisy, are missing values, or have a host of other discrepancies. You can use data summarization and visualization to audit the quality of your data and provide the information you need to process the data before it's ready for modeling. This process is often iterative. TDSP provides an automated utility, called IDEAR, to help visualize the data and prepare data summary reports. We recommend that you start with IDEAR first to explore the data to help develop initial data understanding interactively with no coding. Then you can write custom code for data exploration and visualization. For guidance on cleaning the data, see Tasks to prepare data for enhanced machine learning. After you're satisfied with the quality of the cleansed data, the next step is to better understand the patterns that are inherent in the data. This data analysis helps you choose and develop an appropriate predictive model for your target. Look for evidence for how well connected the data is to the target. Then determine whether there is sufficient data to move forward with the next modeling steps. Again, this process is often iterative. You might need to find new data sources with more accurate or more relevant data to augment the data set initially identified in the previous stage. Set up a data pipeline \u00b6 In addition to the initial ingestion and cleaning of the data, you typically need to set up a process to score new data or refresh the data regularly as part of an ongoing learning process. Scoring may be completed with a data pipeline or workflow. The Move data from an on-premises SQL Server instance to Azure SQL Database with Azure Data Factory article gives an example of how to set up a pipeline with Azure Data Factory. In this stage, you develop a solution architecture of the data pipeline. You develop the pipeline in parallel with the next stage of the data science project. Depending on your business needs and the constraints of your existing systems into which this solution is being integrated, the pipeline can be one of the following options: Batch-based Streaming or real time A hybrid","title":"Goals"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#goals","text":"","title":"Goals"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#produce-a-clean-high-quality-data-set-whose-relationship-to-the-target-variables-is-understood-locate-the-data-set-in-the-appropriate-analytics-environment-so-you-are-ready-to-model","text":"","title":"Produce a clean, high-quality data set whose relationship to the target variables is understood. Locate the data set in the appropriate analytics environment so you are ready to model."},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#develop-a-solution-architecture-of-the-data-pipeline-that-refreshes-and-scores-the-data-regularly","text":"","title":"Develop a solution architecture of the data pipeline that refreshes and scores the data regularly."},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#ingest-the-data","text":"Set up the process to move the data from the source locations to the target locations where you run analytics operations, like training and predictions. For technical details and options on how to move the data with various Azure data services, see Load data into storage environments for analytics.","title":"Ingest the data"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#explore-the-data","text":"Before you train your models, you need to develop a sound understanding of the data. Real-world data sets are often noisy, are missing values, or have a host of other discrepancies. You can use data summarization and visualization to audit the quality of your data and provide the information you need to process the data before it's ready for modeling. This process is often iterative. TDSP provides an automated utility, called IDEAR, to help visualize the data and prepare data summary reports. We recommend that you start with IDEAR first to explore the data to help develop initial data understanding interactively with no coding. Then you can write custom code for data exploration and visualization. For guidance on cleaning the data, see Tasks to prepare data for enhanced machine learning. After you're satisfied with the quality of the cleansed data, the next step is to better understand the patterns that are inherent in the data. This data analysis helps you choose and develop an appropriate predictive model for your target. Look for evidence for how well connected the data is to the target. Then determine whether there is sufficient data to move forward with the next modeling steps. Again, this process is often iterative. You might need to find new data sources with more accurate or more relevant data to augment the data set initially identified in the previous stage.","title":"Explore the data"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/#set-up-a-data-pipeline","text":"In addition to the initial ingestion and cleaning of the data, you typically need to set up a process to score new data or refresh the data regularly as part of an ongoing learning process. Scoring may be completed with a data pipeline or workflow. The Move data from an on-premises SQL Server instance to Azure SQL Database with Azure Data Factory article gives an example of how to set up a pipeline with Azure Data Factory. In this stage, you develop a solution architecture of the data pipeline. You develop the pipeline in parallel with the next stage of the data science project. Depending on your business needs and the constraints of your existing systems into which this solution is being integrated, the pipeline can be one of the following options: Batch-based Streaming or real time A hybrid","title":"Set up a data pipeline"},{"location":"methodology/MVP/Data-Visualization/","text":"","title":"Data Visualization"},{"location":"methodology/MVP/Inception/","text":"","title":"Inception"},{"location":"methodology/MVP/Modeling/","text":"Goals \u00b6 Determine the optimal data features for the machine-learning model. \u00b6 Create an informative machine-learning model that predicts the target most accurately. \u00b6 Create a machine-learning model that's suitable for production. \u00b6 Feature engineering \u00b6 Feature engineering involves the inclusion, aggregation, and transformation of raw variables to create the features used in the analysis. If you want insight into what is driving a model, then you need to understand how the features relate to each other and how the machine-learning algorithms are to use those features. This step requires a creative combination of domain expertise and the insights obtained from the data exploration step. Feature engineering is a balancing act of finding and including informative variables, but at the same time trying to avoid too many unrelated variables. Informative variables improve your result; unrelated variables introduce unnecessary noise into the model. You also need to generate these features for any new data obtained during scoring. As a result, the generation of these features can only depend on data that's available at the time of scoring. For technical guidance on feature engineering when make use of various Azure data technologies, see Feature engineering in the data science process. Model training \u00b6 Depending on the type of question that you're trying to answer, there are many modeling algorithms available. For guidance on choosing the algorithms, see How to choose algorithms for Microsoft Azure Machine Learning. Although this article uses Azure Machine Learning, the guidance it provides is useful for any machine-learning projects. The process for model training includes the following steps: Split the input data randomly for modeling into a training data set and a test data set. Build the models by using the training data set. Evaluate the training and the test data set. Use a series of competing machine-learning algorithms along with the various associated tuning parameters (known as a parameter sweep) that are geared toward answering the question of interest with the current data. Determine the \u201cbest\u201d solution to answer the question by comparing the success metrics between alternative methods. Note Avoid leakage: You can cause data leakage if you include data from outside the training data set that allows a model or machine-learning algorithm to make unrealistically good predictions. Leakage is a common reason why data scientists get nervous when they get predictive results that seem too good to be true. These dependencies can be hard to detect. To avoid leakage often requires iterating between building an analysis data set, creating a model, and evaluating the accuracy of the results. We provide an automated modeling and reporting tool with TDSP that's able to run through multiple algorithms and parameter sweeps to produce a baseline model. It also produces a baseline modeling report that summarizes the performance of each model and parameter combination including variable importance. This process is also iterative as it can drive further feature engineering.","title":"Goals"},{"location":"methodology/MVP/Modeling/#goals","text":"","title":"Goals"},{"location":"methodology/MVP/Modeling/#determine-the-optimal-data-features-for-the-machine-learning-model","text":"","title":"Determine the optimal data features for the machine-learning model."},{"location":"methodology/MVP/Modeling/#create-an-informative-machine-learning-model-that-predicts-the-target-most-accurately","text":"","title":"Create an informative machine-learning model that predicts the target most accurately."},{"location":"methodology/MVP/Modeling/#create-a-machine-learning-model-thats-suitable-for-production","text":"","title":"Create a machine-learning model that's suitable for production."},{"location":"methodology/MVP/Modeling/#feature-engineering","text":"Feature engineering involves the inclusion, aggregation, and transformation of raw variables to create the features used in the analysis. If you want insight into what is driving a model, then you need to understand how the features relate to each other and how the machine-learning algorithms are to use those features. This step requires a creative combination of domain expertise and the insights obtained from the data exploration step. Feature engineering is a balancing act of finding and including informative variables, but at the same time trying to avoid too many unrelated variables. Informative variables improve your result; unrelated variables introduce unnecessary noise into the model. You also need to generate these features for any new data obtained during scoring. As a result, the generation of these features can only depend on data that's available at the time of scoring. For technical guidance on feature engineering when make use of various Azure data technologies, see Feature engineering in the data science process.","title":"Feature engineering"},{"location":"methodology/MVP/Modeling/#model-training","text":"Depending on the type of question that you're trying to answer, there are many modeling algorithms available. For guidance on choosing the algorithms, see How to choose algorithms for Microsoft Azure Machine Learning. Although this article uses Azure Machine Learning, the guidance it provides is useful for any machine-learning projects. The process for model training includes the following steps: Split the input data randomly for modeling into a training data set and a test data set. Build the models by using the training data set. Evaluate the training and the test data set. Use a series of competing machine-learning algorithms along with the various associated tuning parameters (known as a parameter sweep) that are geared toward answering the question of interest with the current data. Determine the \u201cbest\u201d solution to answer the question by comparing the success metrics between alternative methods. Note Avoid leakage: You can cause data leakage if you include data from outside the training data set that allows a model or machine-learning algorithm to make unrealistically good predictions. Leakage is a common reason why data scientists get nervous when they get predictive results that seem too good to be true. These dependencies can be hard to detect. To avoid leakage often requires iterating between building an analysis data set, creating a model, and evaluating the accuracy of the results. We provide an automated modeling and reporting tool with TDSP that's able to run through multiple algorithms and parameter sweeps to produce a baseline model. It also produces a baseline modeling report that summarizes the performance of each model and parameter combination including variable importance. This process is also iterative as it can drive further feature engineering.","title":"Model training"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/Artifacts/","text":"The following are the deliverables in this stage: \u00b6 Data quality report: This report includes data summaries, the relationships between each attribute and target, variable ranking, and more. The IDEAR tool provided as part of TDSP can quickly generate this report on any tabular data set, such as a CSV file or a relational table. Solution architecture: The solution architecture can be a diagram or description of your data pipeline that you use to run scoring or predictions on new data after you have built a model. It also contains the pipeline to retrain your model based on new data. Store the document in the Project directory when you use the TDSP directory structure template. Checkpoint decision: Before you begin full-feature engineering and model building, you can reevaluate the project to determine whether the value expected is sufficient to continue pursuing it. You might, for example, be ready to proceed, need to collect more data, or abandon the project as the data does not exist to answer the question.","title":"Artifacts"},{"location":"methodology/MVP/Data-Acquisition-and-Understanding/Artifacts/#the-following-are-the-deliverables-in-this-stage","text":"Data quality report: This report includes data summaries, the relationships between each attribute and target, variable ranking, and more. The IDEAR tool provided as part of TDSP can quickly generate this report on any tabular data set, such as a CSV file or a relational table. Solution architecture: The solution architecture can be a diagram or description of your data pipeline that you use to run scoring or predictions on new data after you have built a model. It also contains the pipeline to retrain your model based on new data. Store the document in the Project directory when you use the TDSP directory structure template. Checkpoint decision: Before you begin full-feature engineering and model building, you can reevaluate the project to determine whether the value expected is sufficient to continue pursuing it. You might, for example, be ready to proceed, need to collect more data, or abandon the project as the data does not exist to answer the question.","title":"The following are the deliverables in this stage:"},{"location":"methodology/MVP/Data-Visualization/Develop-Front%252Dend/","text":"","title":"Develop Front%2Dend"},{"location":"methodology/MVP/Data-Visualization/Prototype-Front%252Dend/","text":"","title":"Prototype Front%2Dend"},{"location":"methodology/Source-control-and-CI%252DCD/DevOps/","text":"","title":"DevOps"},{"location":"projects/MRO/tutorial/","text":"MRO \u00b6 Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"MRO"},{"location":"projects/MRO/tutorial/#mro","text":"Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"MRO"},{"location":"projects/Predictive%20Maintenance/tutorial/","text":"Predictive Maintenance \u00b6 Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"Predictive Maintenance"},{"location":"projects/Predictive%20Maintenance/tutorial/#predictive-maintenance","text":"Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"Predictive Maintenance"},{"location":"projects/Quality%20Control/tutorial/","text":"Quality Control \u00b6 Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"Quality Control"},{"location":"projects/Quality%20Control/tutorial/#quality-control","text":"Non aliquip dolore id quis velit enim elit non amet ea. Ut ullamco culpa laboris enim ex anim cupidatat anim laborum non nulla aute enim esse. Enim dolor quis irure consectetur commodo pariatur et ut. Enim esse id ut velit sit. Id est sit sint sint sit pariatur. Voluptate anim est est amet ut id occaecat. Voluptate adipisicing mollit velit deserunt. Nulla velit minim ullamco proident. Tempor irure do fugiat culpa id velit laborum mollit in. Esse cillum duis consequat nostrud ad excepteur. Esse aliqua dolor nisi laborum proident veniam ipsum culpa consequat cillum commodo mollit esse enim. Duis sint esse aliquip occaecat magna exercitation incididunt Lorem. Commodo consequat tempor laboris labore consectetur. Commodo nulla dolore irure ex et reprehenderit esse dolore irure commodo Lorem ullamco sunt. Pariatur velit est proident amet nisi. Cillum ex aute deserunt id officia in aliqua ipsum cillum aliqua velit qui. https://preditivadigitalhelp.braskem.com/","title":"Quality Control"},{"location":"projects/mkdocs/4/","text":"Upgrading to 4.x \u00b6 Highlights \u00b6 Material for MkDocs 4 fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material for MkDocs with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed. How to upgrade \u00b6 Changes to mkdocs.yml \u00b6 None. Changes to *.html files \u00b6 None.","title":"Upgrading to 4.x"},{"location":"projects/mkdocs/4/#upgrading-to-4x","text":"","title":"Upgrading to 4.x"},{"location":"projects/mkdocs/4/#highlights","text":"Material for MkDocs 4 fixes incorrect layout on Chinese systems. The fix includes a mandatory change of the base font-size from 10px to 20px which means all rem values needed to be updated. Within the theme, px to rem calculation is now encapsulated in a new function called px2rem which is part of the SASS code base. If you use Material for MkDocs with custom CSS that is based on rem values, note that those values must now be divided by 2. Now, 1.0rem doesn't map to 10px , but 20px . To learn more about the problem and implications, please refer to the issue in which the problem was discovered and fixed.","title":"Highlights"},{"location":"projects/mkdocs/4/#how-to-upgrade","text":"","title":"How to upgrade"},{"location":"projects/mkdocs/4/#changes-to-mkdocsyml","text":"None.","title":"Changes to mkdocs.yml"},{"location":"projects/mkdocs/4/#changes-to-html-files","text":"None.","title":"Changes to *.html files"},{"location":"projects/mkdocs/5/","text":"Upgrading to 5.x \u00b6 Highlights \u00b6 Reactive architecture \u2013 try app.dialog$.next(\"Hi!\") in the console Instant loading \u2013 make Material behave like a Single Page Application Improved CSS customization with CSS variables \u2013 set your brand's colors Improved CSS resilience, e.g. proper sidebar locking for customized headers Improved icon integration and configuration \u2013 now including over 5k icons Added possibility to use any icon for logo, repository and social links Search UI does not freeze anymore (moved to web worker) Search index built only once when using instant loading Improved extensible keyboard handling Support for prebuilt search indexes Support for displaying stars and forks for GitLab repositories Support for scroll snapping of sidebars and search results Reduced HTML and CSS footprint due to deprecation of Internet Explorer support Slight facelifting of some UI elements (Admonitions, tables, ...) How to upgrade \u00b6 Changes to mkdocs.yml \u00b6 Following is a list of changes that need to be made to mkdocs.yml . Note that you only have to adjust the value if you defined it, so if your configuration does not contain the key, you can skip it. theme.feature \u00b6 Optional features like tabs and instant loading are now implemented as flags and can be enabled by listing them in mkdocs.yml under theme.features : 5.x theme : features : - tabs - instant 4.x theme : feature : tabs : true theme.logo.icon \u00b6 The logo icon configuration was centralized under theme.icon.logo and can now be set to any of the icons bundled with the theme : 5.x theme : icon : logo : material/cloud 4.x theme : logo : icon : cloud extra.repo_icon \u00b6 The repo icon configuration was centralized under theme.icon.repo and can now be set to any of the icons bundled with the theme : 5.x theme : icon : repo : fontawesome/brands/gitlab 4.x extra : repo_icon : gitlab extra.search.* \u00b6 Search is now configured as part of the plugin options . Note that the search languages must now be listed as an array of strings and the tokenizer was renamed to separator : 5.x plugins : - search : separator : '[\\s\\-\\.]+' lang : - en - de - ru 4.x extra : search : language : en, de, ru tokenizer : [ \\s\\-\\. ] + extra.social.* \u00b6 Social links stayed in the same place, but the type key was renamed to icon in order to match the new way of specifying which icon to be used: 5.x extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk 4.x extra : social : - type : github link : https://github.com/squidfunk Changes to *.html files \u00b6 The templates have undergone a set of changes to make them future-proof. If you've used theme extension to override a block or template, make sure that it matches the new structure: If you've overridden a block , check base.html for potential changes If you've overridden a template , check the respective *.html file for potential changes base.html \u00b6 @@ -2,7 +2,6 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -{% set feature = config.theme.feature %} {% set palette = config.theme.palette %} {% set font = config.theme.font %} <!doctype html> @@ -30,19 +29,6 @@ {% elif config.site_author %} <meta name=\"author\" content=\"{{ config.site_author }}\"> {% endif %} - {% for key in [ - \"clipboard.copy\", - \"clipboard.copied\", - \"search.language\", - \"search.pipeline.stopwords\", - \"search.pipeline.trimmer\", - \"search.result.none\", - \"search.result.one\", - \"search.result.other\", - \"search.tokenizer\" - ] %} - <meta name=\"lang:{{ key }}\" content=\"{{ lang.t(key) }}\"> - {% endfor %} <link rel=\"shortcut icon\" href=\"{{ config.theme.favicon | url }}\"> <meta name=\"generator\" content=\"mkdocs-{{ mkdocs_version }}, mkdocs-material-5.0.0\"> {% endblock %} @@ -56,9 +42,9 @@ {% endif %} {% endblock %} {% block styles %} - <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/application.********.css' | url }}\"> + <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/main.********.min.css' | url }}\"> {% if palette.primary or palette.accent %} - <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/application-palette.********.css' | url }}\"> + <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/palette.********.min.css' | url }}\"> {% endif %} {% if palette.primary %} {% import \"partials/palette.html\" as map %} @@ -69,20 +55,17 @@ {% endif %} {% endblock %} {% block libs %} - <script src=\"{{ 'assets/javascripts/modernizr.********.js' | url }}\"></script> {% endblock %} {% block fonts %} {% if font != false %} <link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" crossorigin> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family={{ font.text | replace(' ', '+') + ':300,400,400i,700%7C' + font.code | replace(' ', '+') }}&display=fallback\"> <style>body,input{font-family:\"{{ font.text }}\",\"Helvetica Neue\",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:\"{{ font.code }}\",\"Courier New\",Courier,monospace}</style> {% endif %} {% endblock %} - <link rel=\"stylesheet\" href=\"{{ 'assets/fonts/material-icons.css' | url }}\"> {% if config.extra.manifest %} <link rel=\"manifest\" href=\"{{ config.extra.manifest | url }}\" crossorigin=\"use-credentials\"> {% endif %} @@ -95,47 +77,50 @@ {% endblock %} {% block extrahead %}{% endblock %} </head> + {% set direction = config.theme.direction | default(lang.t('direction')) %} {% if palette.primary or palette.accent %} {% set primary = palette.primary | replace(\" \", \"-\") | lower %} {% set accent = palette.accent | replace(\" \", \"-\") | lower %} - <body dir=\"{{ lang.t('direction') }}\" data-md-color-primary=\"{{ primary }}\" data-md-color-accent=\"{{ accent }}\"> + <body dir=\"{{ direction }}\" data-md-color-primary=\"{{ primary }}\" data-md-color-accent=\"{{ accent }}\"> {% else %} - <body dir=\"{{ lang.t('direction') }}\"> + <body dir=\"{{ direction }}\"> {% endif %} - <svg class=\"md-svg\"> - <defs> - {% set platform = config.extra.repo_icon or config.repo_url %} - {% if \"github\" in platform %} - {% include \"assets/images/icons/github.f0b8504a.svg\" %} - {% elif \"gitlab\" in platform %} - {% include \"assets/images/icons/gitlab.6dd19c00.svg\" %} - {% elif \"bitbucket\" in platform %} - {% include \"assets/images/icons/bitbucket.1b09e088.svg\" %} - {% endif %} - </defs> - </svg> <input class=\"md-toggle\" data-md-toggle=\"drawer\" type=\"checkbox\" id=\"__drawer\" autocomplete=\"off\"> <input class=\"md-toggle\" data-md-toggle=\"search\" type=\"checkbox\" id=\"__search\" autocomplete=\"off\"> - <label class=\"md-overlay\" data-md-component=\"overlay\" for=\"__drawer\"></label> + <label class=\"md-overlay\" for=\"__drawer\"></label> + <div data-md-component=\"skip\"> + {% if page.toc | first is defined %} + {% set skip = page.toc | first %} + <a href=\"{{ skip.url | url }}\" class=\"md-skip\"> + {{ lang.t('skip.link.title') }} + </a> + {% endif %} + </div> + <div data-md-component=\"announce\"> + {% if self.announce() %} + <aside class=\"md-announce\"> + <div class=\"md-announce__inner md-grid md-typeset\"> + {% block announce %}{% endblock %} + </div> + </aside> + {% endif %} + </div> {% block header %} {% include \"partials/header.html\" %} {% endblock %} - <div class=\"md-container\"> + <div class=\"md-container\" data-md-component=\"container\"> {% block hero %} {% if page and page.meta and page.meta.hero %} {% include \"partials/hero.html\" with context %} {% endif %} {% endblock %} - {% if feature.tabs %} - {% include \"partials/tabs.html\" %} - {% endif %} + {% block tabs %} + {% if \"tabs\" in config.theme.features %} + {% include \"partials/tabs.html\" %} + {% endif %} + {% endblock %} - <main class=\"md-main\" role=\"main\"> - <div class=\"md-main__inner md-grid\" data-md-component=\"container\"> + <main class=\"md-main\" data-md-component=\"main\"> + <div class=\"md-main__inner md-grid\"> {% block site_nav %} {% if nav %} <div class=\"md-sidebar md-sidebar--primary\" data-md-component=\"navigation\"> @@ -160,41 +141,25 @@ <article class=\"md-content__inner md-typeset\"> {% block content %} {% if page.edit_url %} - <a href=\"{{ page.edit_url }}\" title=\"{{ lang.t('edit.link.title') }}\" class=\"md-icon md-content__icon\">&#xE3C9;</a> + <a href=\"{{ page.edit_url }}\" title=\"{{ lang.t('edit.link.title') }}\" class=\"md-content__button md-icon\"> + {% include \".icons/material/pencil.svg\" %} + </a> {% endif %} + {% block source %} + {% if page and page.meta and page.meta.source %} + {% include \"partials/source-link.html\" %} + {% endif %} + {% endblock %} {% if not \"\\x3ch1\" in page.content %} <h1>{{ page.title | default(config.site_name, true)}}</h1> {% endif %} {{ page.content }} - {% block source %} - {% if page and page.meta and page.meta.source %} - <h2 id=\"__source\">{{ lang.t(\"meta.source\") }}</h2> - {% set repo = config.repo_url %} - {% if repo | last == \"/\" %} - {% set repo = repo[:-1] %} - {% endif %} - {% set path = page.meta.path | default([\"\"]) %} - {% set file = page.meta.source %} - <a href=\"{{ [repo, path, file] | join('/') }}\" title=\"{{ file }}\" class=\"md-source-file\"> - {{ file }} - </a> - {% endif %} - {% endblock %} + {% if page and page.meta %} + {% if page.meta.git_revision_date_localized or + page.meta.revision_date + %} + {% include \"partials/source-date.html\" %} - {% if page and page.meta and ( - page.meta.git_revision_date_localized or - page.meta.revision_date - ) %} - {% set label = lang.t(\"source.revision.date\") %} - <hr> - <div class=\"md-source-date\"> - <small> - {% if page.meta.git_revision_date_localized %} - {{ label }}: {{ page.meta.git_revision_date_localized }} - {% elif page.meta.revision_date %} - {{ label }}: {{ page.meta.revision_date }} - {% endif %} - </small> - </div> {% endif %} {% endblock %} {% block disqus %} @@ -208,29 +174,35 @@ {% include \"partials/footer.html\" %} {% endblock %} </div> {% block scripts %} - <script src=\"{{ 'assets/javascripts/application.********.js' | url }}\"></script> - {% if lang.t(\"search.language\") != \"en\" %} - {% set languages = lang.t(\"search.language\").split(\",\") %} - {% if languages | length and languages[0] != \"\" %} - {% set path = \"assets/javascripts/lunr/\" %} - <script src=\"{{ (path ~ 'lunr.stemmer.support.js') | url }}\"></script> - {% for language in languages | map(\"trim\") %} - {% if language != \"en\" %} - {% if language == \"ja\" %} - <script src=\"{{ (path ~ 'tinyseg.js') | url }}\"></script> - {% endif %} - {% if language in (\"ar\", \"da\", \"de\", \"es\", \"fi\", \"fr\", \"hu\", \"it\", \"ja\", \"nl\", \"no\", \"pt\", \"ro\", \"ru\", \"sv\", \"th\", \"tr\", \"vi\") %} - <script src=\"{{ (path ~ 'lunr.' ~ language ~ '.js') | url }}\"></script> - {% endif %} - {% endif %} - {% endfor %} - {% if languages | length > 1 %} - <script src=\"{{ (path ~ 'lunr.multi.js') | url }}\"></script> - {% endif %} - {% endif %} - {% endif %} - <script>app.initialize({version:\"{{ mkdocs_version }}\",url:{base:\"{{ base_url }}\"}})</script> + <script src=\"{{ 'assets/javascripts/vendor.********.min.js' | url }}\"></script> + <script src=\"{{ 'assets/javascripts/bundle.********.min.js' | url }}\"></script> + {%- set translations = {} -%} + {%- for key in [ + \"clipboard.copy\", + \"clipboard.copied\", + \"search.config.lang\", + \"search.config.pipeline\", + \"search.config.separator\", + \"search.result.placeholder\", + \"search.result.none\", + \"search.result.one\", + \"search.result.other\" + ] -%} + {%- set _ = translations.update({ key: lang.t(key) }) -%} + {%- endfor -%} + <script id=\"__lang\" type=\"application/json\"> + {{- translations | tojson -}} + </script> + {% block config %}{% endblock %} + <script> + app = initialize({ + base: \"{{ base_url }}\", + features: {{ config.theme.features | tojson }}, + search: Object.assign({ + worker: \"{{ 'assets/javascripts/worker/search.********.min.js' | url }}\" + }, typeof search !== \"undefined\" && search) + }) + </script> {% for path in config[\"extra_javascript\"] %} <script src=\"{{ path | url }}\"></script> {% endfor %} partials/footer.html \u00b6 @@ -5,34 +5,34 @@ <div class=\"md-footer-nav\"> - <nav class=\"md-footer-nav__inner md-grid\"> + <nav class=\"md-footer-nav__inner md-grid\" aria-label=\"{{ lang.t('footer.title') }}\"> {% if page.previous_page %} - <a href=\"{{ page.previous_page.url | url }}\" title=\"{{ page.previous_page.title | striptags }}\" class=\"md-flex md-footer-nav__link md-footer-nav__link--prev\" rel=\"prev\"> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <i class=\"md-icon md-icon--arrow-back md-footer-nav__button\"></i> + <a href=\"{{ page.previous_page.url | url }}\" title=\"{{ page.previous_page.title | striptags }}\" class=\"md-footer-nav__link md-footer-nav__link--prev\" rel=\"prev\"> + <div class=\"md-footer-nav__button md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} </div> - <div class=\"md-flex__cell md-flex__cell--stretch md-footer-nav__title\"> - <span class=\"md-flex__ellipsis\"> + <div class=\"md-footer-nav__title\"> + <div class=\"md-ellipsis\"> <span class=\"md-footer-nav__direction\"> {{ lang.t(\"footer.previous\") }} </span> {{ page.previous_page.title }} - </span> + </div> </div> </a> {% endif %} {% if page.next_page %} - <a href=\"{{ page.next_page.url | url }}\" title=\"{{ page.next_page.title | striptags }}\" class=\"md-flex md-footer-nav__link md-footer-nav__link--next\" rel=\"next\"> - <div class=\"md-flex__cell md-flex__cell--stretch md-footer-nav__title\"> - <span class=\"md-flex__ellipsis\"> + <a href=\"{{ page.next_page.url | url }}\" title=\"{{ page.next_page.title | striptags }}\" class=\"md-footer-nav__link md-footer-nav__link--next\" rel=\"next\"> + <div class=\"md-footer-nav__title\"> + <div class=\"md-ellipsis\"> <span class=\"md-footer-nav__direction\"> {{ lang.t(\"footer.next\") }} </span> {{ page.next_page.title }} - </span> + </div> </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <i class=\"md-icon md-icon--arrow-forward md-footer-nav__button\"></i> + <div class=\"md-footer-nav__button md-icon\"> + {% include \".icons/material/arrow-right.svg\" %} </div> </a> {% endif %} partials/header.html \u00b6 @@ -2,51 +2,43 @@ This file was automatically generated - do not edit -#} <header class=\"md-header\" data-md-component=\"header\"> - <nav class=\"md-header-nav md-grid\"> - <div class=\"md-flex\"> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" aria-label=\"{{ config.site_name }}\" class=\"md-header-nav__button md-logo\"> - {% if config.theme.logo.icon %} - <i class=\"md-icon\">{{ config.theme.logo.icon }}</i> - {% else %} - <img alt=\"logo\" src=\"{{ config.theme.logo | url }}\" width=\"24\" height=\"24\"> - {% endif %} - </a> - </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <label class=\"md-icon md-icon--menu md-header-nav__button\" for=\"__drawer\"></label> - </div> - <div class=\"md-flex__cell md-flex__cell--stretch\"> - <div class=\"md-flex__ellipsis md-header-nav__title\" data-md-component=\"title\"> - {% if config.site_name == page.title %} - {{ config.site_name }} - {% else %} - <span class=\"md-header-nav__topic\"> - {{ config.site_name }} - </span> - <span class=\"md-header-nav__topic\"> - {% if page and page.meta and page.meta.title %} - {{ page.meta.title }} - {% else %} - {{ page.title }} - {% endif %} - </span> - {% endif %} + <nav class=\"md-header-nav md-grid\" aria-label=\"{{ lang.t('header.title') }}\"> + <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-header-nav__button md-logo\" aria-label=\"{{ config.site_name }}\"> + {% include \"partials/logo.html\" %} + </a> + <label class=\"md-header-nav__button md-icon\" for=\"__drawer\"> + {% include \".icons/material/menu\" ~ \".svg\" %} + </label> + <div class=\"md-header-nav__title\" data-md-component=\"header-title\"> + {% if config.site_name == page.title %} + <div class=\"md-header-nav__ellipsis md-ellipsis\"> + {{ config.site_name }} </div> - </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - {% if \"search\" in config[\"plugins\"] %} - <label class=\"md-icon md-icon--search md-header-nav__button\" for=\"__search\"></label> - {% include \"partials/search.html\" %} - {% endif %} - </div> - {% if config.repo_url %} - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <div class=\"md-header-nav__source\"> - {% include \"partials/source.html\" %} - </div> + {% else %} + <div class=\"md-header-nav__ellipsis\"> + <span class=\"md-header-nav__topic md-ellipsis\"> + {{ config.site_name }} + </span> + <span class=\"md-header-nav__topic md-ellipsis\"> + {% if page and page.meta and page.meta.title %} + {{ page.meta.title }} + {% else %} + {{ page.title }} + {% endif %} + </span> </div> {% endif %} </div> + {% if \"search\" in config[\"plugins\"] %} + <label class=\"md-header-nav__button md-icon\" for=\"__search\"> + {% include \".icons/material/magnify.svg\" %} + </label> + {% include \"partials/search.html\" %} + {% endif %} + {% if config.repo_url %} + <div class=\"md-header-nav__source\"> + {% include \"partials/source.html\" %} + </div> + {% endif %} </nav> </header> partials/hero.html \u00b6 @@ -1,9 +1,8 @@ {#- This file was automatically generated - do not edit -#} -{% set feature = config.theme.feature %} {% set class = \"md-hero\" %} -{% if not feature.tabs %} +{% if \"tabs\" not in config.theme.features %} {% set class = \"md-hero md-hero--expand\" %} {% endif %} <div class=\"{{ class }}\" data-md-component=\"hero\"> partials/language.html \u00b6 @@ -3,12 +3,4 @@ -#} {% import \"partials/language/\" + config.theme.language + \".html\" as lang %} {% import \"partials/language/en.html\" as fallback %} -{% macro t(key) %}{{ { - \"direction\": config.theme.direction, - \"search.language\": ( - config.extra.search | default({}) - ).language, - \"search.tokenizer\": ( - config.extra.search | default({}) - ).tokenizer | default(\"\", true), -}[key] or lang.t(key) or fallback.t(key) }}{% endmacro %} +{% macro t(key) %}{{ lang.t(key) | default(fallback.t(key)) }}{% endmacro %} partials/logo.html \u00b6 @@ -0,0 +1,9 @@ +{#- + This file was automatically generated - do not edit +-#} +{% if config.theme.logo %} + <img src=\"{{ config.theme.logo | url }}\" alt=\"logo\"> +{% else %} + {% set icon = config.theme.icon.logo or \"material/library\" %} + {% include \".icons/\" ~ icon ~ \".svg\" %} +{% endif %} partials/nav-item.html \u00b6 @@ -14,9 +14,15 @@ {% endif %} <label class=\"md-nav__link\" for=\"{{ path }}\"> {{ nav_item.title }} + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/chevron-right.svg\" %} + </span> </label> - <nav class=\"md-nav\" data-md-component=\"collapsible\" data-md-level=\"{{ level }}\"> + <nav class=\"md-nav\" aria-label=\"{{ nav_item.title }}\" data-md-level=\"{{ level }}\"> <label class=\"md-nav__title\" for=\"{{ path }}\"> + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} + </span> {{ nav_item.title }} </label> <ul class=\"md-nav__list\" data-md-scrollfix> @@ -39,6 +45,9 @@ {% if toc | first is defined %} <label class=\"md-nav__link md-nav__link--active\" for=\"__toc\"> {{ nav_item.title }} + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/table-of-contents.svg\" %} + </span> </label> {% endif %} <a href=\"{{ nav_item.url | url }}\" title=\"{{ nav_item.title | striptags }}\" class=\"md-nav__link md-nav__link--active\"> partials/nav.html \u00b6 @@ -1,14 +1,10 @@ {#- This file was automatically generated - do not edit -#} -<nav class=\"md-nav md-nav--primary\" data-md-level=\"0\"> - <label class=\"md-nav__title md-nav__title--site\" for=\"__drawer\"> - <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-nav__button md-logo\"> - {% if config.theme.logo.icon %} - <i class=\"md-icon\">{{ config.theme.logo.icon }}</i> - {% else %} - <img alt=\"logo\" src=\"{{ config.theme.logo | url }}\" width=\"48\" height=\"48\"> - {% endif %} +<nav class=\"md-nav md-nav--primary\" aria-label=\"{{ lang.t('nav.title') }}\" data-md-level=\"0\"> + <label class=\"md-nav__title\" for=\"__drawer\"> + <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-nav__button md-logo\" aria-label=\"{{ config.site_name }}\"> + {% include \"partials/logo.html\" %} </a> {{ config.site_name }} </label> partials/search.html \u00b6 @@ -6,15 +6,18 @@ <label class=\"md-search__overlay\" for=\"__search\"></label> <div class=\"md-search__inner\" role=\"search\"> <form class=\"md-search__form\" name=\"search\"> - <input type=\"text\" class=\"md-search__input\" name=\"query\" aria-label=\"Search\" placeholder=\"{{ lang.t('search.placeholder') }}\" autocapitalize=\"off\" autocorrect=\"off\" autocomplete=\"off\" spellcheck=\"false\" data-md-component=\"query\" data-md-state=\"active\"> + <input type=\"text\" class=\"md-search__input\" name=\"query\" aria-label=\"{{ lang.t('search.placeholder') }}\" placeholder=\"{{ lang.t('search.placeholder') }}\" autocapitalize=\"off\" autocorrect=\"off\" autocomplete=\"off\" spellcheck=\"false\" data-md-component=\"search-query\" data-md-state=\"active\"> <label class=\"md-search__icon md-icon\" for=\"__search\"> + {% include \".icons/material/magnify.svg\" %} + {% include \".icons/material/arrow-left.svg\" %} </label> - <button type=\"reset\" class=\"md-icon md-search__icon\" data-md-component=\"reset\" tabindex=\"-1\"> - &#xE5CD; + <button type=\"reset\" class=\"md-search__icon md-icon\" aria-label=\"{{ lang.t('search.reset') }}\" data-md-component=\"search-reset\" tabindex=\"-1\"> + {% include \".icons/material/close.svg\" %} </button> </form> <div class=\"md-search__output\"> <div class=\"md-search__scrollwrap\" data-md-scrollfix> - <div class=\"md-search-result\" data-md-component=\"result\"> + <div class=\"md-search-result\" data-md-component=\"search-result\"> <div class=\"md-search-result__meta\"> {{ lang.t(\"search.result.placeholder\") }} </div> partials/social.html \u00b6 @@ -3,9 +3,12 @@ -#} {% if config.extra.social %} <div class=\"md-footer-social\"> - <link rel=\"stylesheet\" href=\"{{ 'assets/fonts/font-awesome.css' | url }}\"> {% for social in config.extra.social %} - <a href=\"{{ social.link }}\" target=\"_blank\" rel=\"noopener\" title=\"{{ social.type }}\" class=\"md-footer-social__link fa fa-{{ social.type }}\"></a> + {% set _,rest = social.link.split(\"//\") %} + {% set domain = rest.split(\"/\")[0] %} + <a href=\"{{ social.link }}\" target=\"_blank\" rel=\"noopener\" title=\"{{ domain }}\" class=\"md-footer-social__link\"> + {% include \".icons/\" ~ social.icon ~ \".svg\" %} + </a> {% endfor %} </div> {% endif %} partials/source-date.html \u00b6 @@ -0,0 +1,15 @@ +{#- + This file was automatically generated - do not edit +-#} +{% import \"partials/language.html\" as lang with context %} +{% set label = lang.t(\"source.revision.date\") %} +<hr> +<div class=\"md-source-date\"> + <small> + {% if page.meta.git_revision_date_localized %} + {{ label }}: {{ page.meta.git_revision_date_localized }} + {% elif page.meta.revision_date %} + {{ label }}: {{ page.meta.revision_date }} + {% endif %} + </small> +</div> partials/source-link.html \u00b6 @@ -0,0 +1,13 @@ +{#- + This file was automatically generated - do not edit +-#} +{% import \"partials/language.html\" as lang with context %} +{% set repo = config.repo_url %} +{% if repo | last == \"/\" %} + {% set repo = repo[:-1] %} +{% endif %} +{% set path = page.meta.path | default([\"\"]) %} +<a href=\"{{ [repo, path, page.meta.source] | join('/') }}\" title=\"{{ file }}\" class=\"md-content__button md-icon\"> + {{ lang.t(\"meta.source\") }} + {% include \".icons/\" ~ config.theme.icon.repo ~ \".svg\" %} +</a> partials/source.html \u00b6 @@ -2,24 +2,11 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -{% set platform = config.extra.repo_icon or config.repo_url %} -{% if \"github\" in platform %} - {% set repo_type = \"github\" %} -{% elif \"gitlab\" in platform %} - {% set repo_type = \"gitlab\" %} -{% elif \"bitbucket\" in platform %} - {% set repo_type = \"bitbucket\" %} -{% else %} - {% set repo_type = \"\" %} -{% endif %} -<a href=\"{{ config.repo_url }}\" title=\"{{ lang.t('source.link.title') }}\" class=\"md-source\" data-md-source=\"{{ repo_type }}\"> - {% if repo_type %} - <div class=\"md-source__icon\"> - <svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"> - <use xlink:href=\"#__{{ repo_type }}\" width=\"24\" height=\"24\"></use> - </svg> - </div> - {% endif %} +<a href=\"{{ config.repo_url }}\" title=\"{{ lang.t('source.link.title') }}\" class=\"md-source\"> + <div class=\"md-source__icon md-icon\"> + {% set icon = config.theme.icon.repo or \"fontawesome/brands/git-alt\" %} + {% include \".icons/\" ~ icon ~ \".svg\" %} + </div> <div class=\"md-source__repository\"> {{ config.repo_name }} </div> partials/tabs-item.html \u00b6 @@ -1,7 +1,7 @@ {#- This file was automatically generated - do not edit -#} -{% if nav_item.is_homepage %} +{% if nav_item.is_homepage or nav_item.url == \"index.html\" %} <li class=\"md-tabs__item\"> {% if not page.ancestors | length and nav | selectattr(\"url\", page.url) %} <a href=\"{{ nav_item.url | url }}\" class=\"md-tabs__link md-tabs__link--active\"> partials/tabs.html \u00b6 @@ -5,7 +5,7 @@ {% if page.ancestors | length > 0 %} {% set class = \"md-tabs md-tabs--active\" %} {% endif %} -<nav class=\"{{ class }}\" data-md-component=\"tabs\"> +<nav class=\"{{ class }}\" aria-label=\"{{ lang.t('tabs.title') }}\" data-md-component=\"tabs\"> <div class=\"md-tabs__inner md-grid\"> <ul class=\"md-tabs__list\"> {% for nav_item in nav %} partials/toc-item.html \u00b6 @@ -6,7 +6,7 @@ {{ toc_item.title }} </a> {% if toc_item.children %} - <nav class=\"md-nav\"> + <nav class=\"md-nav\" aria-label=\"{{ toc_item.title }}\"> <ul class=\"md-nav__list\"> {% for toc_item in toc_item.children %} {% include \"partials/toc-item.html\" %} partials/toc.html \u00b6 @@ -2,35 +2,22 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -<nav class=\"md-nav md-nav--secondary\"> +<nav class=\"md-nav md-nav--secondary\" aria-label=\"{{ lang.t('toc.title') }}\"> {% endif %} {% if toc | first is defined %} <label class=\"md-nav__title\" for=\"__toc\"> + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} + </span> {{ lang.t(\"toc.title\") }} </label> <ul class=\"md-nav__list\" data-md-scrollfix> {% for toc_item in toc %} {% include \"partials/toc-item.html\" %} {% endfor %} - {% if page.meta.source and page.meta.source | length > 0 %} - <li class=\"md-nav__item\"> - <a href=\"#__source\" class=\"md-nav__link md-nav__link--active\"> - {{ lang.t(\"meta.source\") }} - </a> - </li> - {% endif %} - {% set disqus = config.extra.disqus %} - {% if page and page.meta and page.meta.disqus is string %} - {% set disqus = page.meta.disqus %} - {% endif %} - {% if not page.is_homepage and disqus %} - <li class=\"md-nav__item\"> - <a href=\"#__comments\" class=\"md-nav__link md-nav__link--active\"> - {{ lang.t(\"meta.comments\") }} - </a> - </li> - {% endif %} </ul> {% endif %} </nav>","title":"Upgrading to 5.x"},{"location":"projects/mkdocs/5/#upgrading-to-5x","text":"","title":"Upgrading to 5.x"},{"location":"projects/mkdocs/5/#highlights","text":"Reactive architecture \u2013 try app.dialog$.next(\"Hi!\") in the console Instant loading \u2013 make Material behave like a Single Page Application Improved CSS customization with CSS variables \u2013 set your brand's colors Improved CSS resilience, e.g. proper sidebar locking for customized headers Improved icon integration and configuration \u2013 now including over 5k icons Added possibility to use any icon for logo, repository and social links Search UI does not freeze anymore (moved to web worker) Search index built only once when using instant loading Improved extensible keyboard handling Support for prebuilt search indexes Support for displaying stars and forks for GitLab repositories Support for scroll snapping of sidebars and search results Reduced HTML and CSS footprint due to deprecation of Internet Explorer support Slight facelifting of some UI elements (Admonitions, tables, ...)","title":"Highlights"},{"location":"projects/mkdocs/5/#how-to-upgrade","text":"","title":"How to upgrade"},{"location":"projects/mkdocs/5/#changes-to-mkdocsyml","text":"Following is a list of changes that need to be made to mkdocs.yml . Note that you only have to adjust the value if you defined it, so if your configuration does not contain the key, you can skip it.","title":"Changes to mkdocs.yml"},{"location":"projects/mkdocs/5/#themefeature","text":"Optional features like tabs and instant loading are now implemented as flags and can be enabled by listing them in mkdocs.yml under theme.features : 5.x theme : features : - tabs - instant 4.x theme : feature : tabs : true","title":"theme.feature"},{"location":"projects/mkdocs/5/#themelogoicon","text":"The logo icon configuration was centralized under theme.icon.logo and can now be set to any of the icons bundled with the theme : 5.x theme : icon : logo : material/cloud 4.x theme : logo : icon : cloud","title":"theme.logo.icon"},{"location":"projects/mkdocs/5/#extrarepo_icon","text":"The repo icon configuration was centralized under theme.icon.repo and can now be set to any of the icons bundled with the theme : 5.x theme : icon : repo : fontawesome/brands/gitlab 4.x extra : repo_icon : gitlab","title":"extra.repo_icon"},{"location":"projects/mkdocs/5/#extrasearch","text":"Search is now configured as part of the plugin options . Note that the search languages must now be listed as an array of strings and the tokenizer was renamed to separator : 5.x plugins : - search : separator : '[\\s\\-\\.]+' lang : - en - de - ru 4.x extra : search : language : en, de, ru tokenizer : [ \\s\\-\\. ] +","title":"extra.search.*"},{"location":"projects/mkdocs/5/#extrasocial","text":"Social links stayed in the same place, but the type key was renamed to icon in order to match the new way of specifying which icon to be used: 5.x extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk 4.x extra : social : - type : github link : https://github.com/squidfunk","title":"extra.social.*"},{"location":"projects/mkdocs/5/#changes-to-html-files","text":"The templates have undergone a set of changes to make them future-proof. If you've used theme extension to override a block or template, make sure that it matches the new structure: If you've overridden a block , check base.html for potential changes If you've overridden a template , check the respective *.html file for potential changes","title":"Changes to *.html files"},{"location":"projects/mkdocs/5/#basehtml","text":"@@ -2,7 +2,6 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -{% set feature = config.theme.feature %} {% set palette = config.theme.palette %} {% set font = config.theme.font %} <!doctype html> @@ -30,19 +29,6 @@ {% elif config.site_author %} <meta name=\"author\" content=\"{{ config.site_author }}\"> {% endif %} - {% for key in [ - \"clipboard.copy\", - \"clipboard.copied\", - \"search.language\", - \"search.pipeline.stopwords\", - \"search.pipeline.trimmer\", - \"search.result.none\", - \"search.result.one\", - \"search.result.other\", - \"search.tokenizer\" - ] %} - <meta name=\"lang:{{ key }}\" content=\"{{ lang.t(key) }}\"> - {% endfor %} <link rel=\"shortcut icon\" href=\"{{ config.theme.favicon | url }}\"> <meta name=\"generator\" content=\"mkdocs-{{ mkdocs_version }}, mkdocs-material-5.0.0\"> {% endblock %} @@ -56,9 +42,9 @@ {% endif %} {% endblock %} {% block styles %} - <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/application.********.css' | url }}\"> + <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/main.********.min.css' | url }}\"> {% if palette.primary or palette.accent %} - <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/application-palette.********.css' | url }}\"> + <link rel=\"stylesheet\" href=\"{{ 'assets/stylesheets/palette.********.min.css' | url }}\"> {% endif %} {% if palette.primary %} {% import \"partials/palette.html\" as map %} @@ -69,20 +55,17 @@ {% endif %} {% endblock %} {% block libs %} - <script src=\"{{ 'assets/javascripts/modernizr.********.js' | url }}\"></script> {% endblock %} {% block fonts %} {% if font != false %} <link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" crossorigin> <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family={{ font.text | replace(' ', '+') + ':300,400,400i,700%7C' + font.code | replace(' ', '+') }}&display=fallback\"> <style>body,input{font-family:\"{{ font.text }}\",\"Helvetica Neue\",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:\"{{ font.code }}\",\"Courier New\",Courier,monospace}</style> {% endif %} {% endblock %} - <link rel=\"stylesheet\" href=\"{{ 'assets/fonts/material-icons.css' | url }}\"> {% if config.extra.manifest %} <link rel=\"manifest\" href=\"{{ config.extra.manifest | url }}\" crossorigin=\"use-credentials\"> {% endif %} @@ -95,47 +77,50 @@ {% endblock %} {% block extrahead %}{% endblock %} </head> + {% set direction = config.theme.direction | default(lang.t('direction')) %} {% if palette.primary or palette.accent %} {% set primary = palette.primary | replace(\" \", \"-\") | lower %} {% set accent = palette.accent | replace(\" \", \"-\") | lower %} - <body dir=\"{{ lang.t('direction') }}\" data-md-color-primary=\"{{ primary }}\" data-md-color-accent=\"{{ accent }}\"> + <body dir=\"{{ direction }}\" data-md-color-primary=\"{{ primary }}\" data-md-color-accent=\"{{ accent }}\"> {% else %} - <body dir=\"{{ lang.t('direction') }}\"> + <body dir=\"{{ direction }}\"> {% endif %} - <svg class=\"md-svg\"> - <defs> - {% set platform = config.extra.repo_icon or config.repo_url %} - {% if \"github\" in platform %} - {% include \"assets/images/icons/github.f0b8504a.svg\" %} - {% elif \"gitlab\" in platform %} - {% include \"assets/images/icons/gitlab.6dd19c00.svg\" %} - {% elif \"bitbucket\" in platform %} - {% include \"assets/images/icons/bitbucket.1b09e088.svg\" %} - {% endif %} - </defs> - </svg> <input class=\"md-toggle\" data-md-toggle=\"drawer\" type=\"checkbox\" id=\"__drawer\" autocomplete=\"off\"> <input class=\"md-toggle\" data-md-toggle=\"search\" type=\"checkbox\" id=\"__search\" autocomplete=\"off\"> - <label class=\"md-overlay\" data-md-component=\"overlay\" for=\"__drawer\"></label> + <label class=\"md-overlay\" for=\"__drawer\"></label> + <div data-md-component=\"skip\"> + {% if page.toc | first is defined %} + {% set skip = page.toc | first %} + <a href=\"{{ skip.url | url }}\" class=\"md-skip\"> + {{ lang.t('skip.link.title') }} + </a> + {% endif %} + </div> + <div data-md-component=\"announce\"> + {% if self.announce() %} + <aside class=\"md-announce\"> + <div class=\"md-announce__inner md-grid md-typeset\"> + {% block announce %}{% endblock %} + </div> + </aside> + {% endif %} + </div> {% block header %} {% include \"partials/header.html\" %} {% endblock %} - <div class=\"md-container\"> + <div class=\"md-container\" data-md-component=\"container\"> {% block hero %} {% if page and page.meta and page.meta.hero %} {% include \"partials/hero.html\" with context %} {% endif %} {% endblock %} - {% if feature.tabs %} - {% include \"partials/tabs.html\" %} - {% endif %} + {% block tabs %} + {% if \"tabs\" in config.theme.features %} + {% include \"partials/tabs.html\" %} + {% endif %} + {% endblock %} - <main class=\"md-main\" role=\"main\"> - <div class=\"md-main__inner md-grid\" data-md-component=\"container\"> + <main class=\"md-main\" data-md-component=\"main\"> + <div class=\"md-main__inner md-grid\"> {% block site_nav %} {% if nav %} <div class=\"md-sidebar md-sidebar--primary\" data-md-component=\"navigation\"> @@ -160,41 +141,25 @@ <article class=\"md-content__inner md-typeset\"> {% block content %} {% if page.edit_url %} - <a href=\"{{ page.edit_url }}\" title=\"{{ lang.t('edit.link.title') }}\" class=\"md-icon md-content__icon\">&#xE3C9;</a> + <a href=\"{{ page.edit_url }}\" title=\"{{ lang.t('edit.link.title') }}\" class=\"md-content__button md-icon\"> + {% include \".icons/material/pencil.svg\" %} + </a> {% endif %} + {% block source %} + {% if page and page.meta and page.meta.source %} + {% include \"partials/source-link.html\" %} + {% endif %} + {% endblock %} {% if not \"\\x3ch1\" in page.content %} <h1>{{ page.title | default(config.site_name, true)}}</h1> {% endif %} {{ page.content }} - {% block source %} - {% if page and page.meta and page.meta.source %} - <h2 id=\"__source\">{{ lang.t(\"meta.source\") }}</h2> - {% set repo = config.repo_url %} - {% if repo | last == \"/\" %} - {% set repo = repo[:-1] %} - {% endif %} - {% set path = page.meta.path | default([\"\"]) %} - {% set file = page.meta.source %} - <a href=\"{{ [repo, path, file] | join('/') }}\" title=\"{{ file }}\" class=\"md-source-file\"> - {{ file }} - </a> - {% endif %} - {% endblock %} + {% if page and page.meta %} + {% if page.meta.git_revision_date_localized or + page.meta.revision_date + %} + {% include \"partials/source-date.html\" %} - {% if page and page.meta and ( - page.meta.git_revision_date_localized or - page.meta.revision_date - ) %} - {% set label = lang.t(\"source.revision.date\") %} - <hr> - <div class=\"md-source-date\"> - <small> - {% if page.meta.git_revision_date_localized %} - {{ label }}: {{ page.meta.git_revision_date_localized }} - {% elif page.meta.revision_date %} - {{ label }}: {{ page.meta.revision_date }} - {% endif %} - </small> - </div> {% endif %} {% endblock %} {% block disqus %} @@ -208,29 +174,35 @@ {% include \"partials/footer.html\" %} {% endblock %} </div> {% block scripts %} - <script src=\"{{ 'assets/javascripts/application.********.js' | url }}\"></script> - {% if lang.t(\"search.language\") != \"en\" %} - {% set languages = lang.t(\"search.language\").split(\",\") %} - {% if languages | length and languages[0] != \"\" %} - {% set path = \"assets/javascripts/lunr/\" %} - <script src=\"{{ (path ~ 'lunr.stemmer.support.js') | url }}\"></script> - {% for language in languages | map(\"trim\") %} - {% if language != \"en\" %} - {% if language == \"ja\" %} - <script src=\"{{ (path ~ 'tinyseg.js') | url }}\"></script> - {% endif %} - {% if language in (\"ar\", \"da\", \"de\", \"es\", \"fi\", \"fr\", \"hu\", \"it\", \"ja\", \"nl\", \"no\", \"pt\", \"ro\", \"ru\", \"sv\", \"th\", \"tr\", \"vi\") %} - <script src=\"{{ (path ~ 'lunr.' ~ language ~ '.js') | url }}\"></script> - {% endif %} - {% endif %} - {% endfor %} - {% if languages | length > 1 %} - <script src=\"{{ (path ~ 'lunr.multi.js') | url }}\"></script> - {% endif %} - {% endif %} - {% endif %} - <script>app.initialize({version:\"{{ mkdocs_version }}\",url:{base:\"{{ base_url }}\"}})</script> + <script src=\"{{ 'assets/javascripts/vendor.********.min.js' | url }}\"></script> + <script src=\"{{ 'assets/javascripts/bundle.********.min.js' | url }}\"></script> + {%- set translations = {} -%} + {%- for key in [ + \"clipboard.copy\", + \"clipboard.copied\", + \"search.config.lang\", + \"search.config.pipeline\", + \"search.config.separator\", + \"search.result.placeholder\", + \"search.result.none\", + \"search.result.one\", + \"search.result.other\" + ] -%} + {%- set _ = translations.update({ key: lang.t(key) }) -%} + {%- endfor -%} + <script id=\"__lang\" type=\"application/json\"> + {{- translations | tojson -}} + </script> + {% block config %}{% endblock %} + <script> + app = initialize({ + base: \"{{ base_url }}\", + features: {{ config.theme.features | tojson }}, + search: Object.assign({ + worker: \"{{ 'assets/javascripts/worker/search.********.min.js' | url }}\" + }, typeof search !== \"undefined\" && search) + }) + </script> {% for path in config[\"extra_javascript\"] %} <script src=\"{{ path | url }}\"></script> {% endfor %}","title":"base.html"},{"location":"projects/mkdocs/5/#partialsfooterhtml","text":"@@ -5,34 +5,34 @@ <div class=\"md-footer-nav\"> - <nav class=\"md-footer-nav__inner md-grid\"> + <nav class=\"md-footer-nav__inner md-grid\" aria-label=\"{{ lang.t('footer.title') }}\"> {% if page.previous_page %} - <a href=\"{{ page.previous_page.url | url }}\" title=\"{{ page.previous_page.title | striptags }}\" class=\"md-flex md-footer-nav__link md-footer-nav__link--prev\" rel=\"prev\"> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <i class=\"md-icon md-icon--arrow-back md-footer-nav__button\"></i> + <a href=\"{{ page.previous_page.url | url }}\" title=\"{{ page.previous_page.title | striptags }}\" class=\"md-footer-nav__link md-footer-nav__link--prev\" rel=\"prev\"> + <div class=\"md-footer-nav__button md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} </div> - <div class=\"md-flex__cell md-flex__cell--stretch md-footer-nav__title\"> - <span class=\"md-flex__ellipsis\"> + <div class=\"md-footer-nav__title\"> + <div class=\"md-ellipsis\"> <span class=\"md-footer-nav__direction\"> {{ lang.t(\"footer.previous\") }} </span> {{ page.previous_page.title }} - </span> + </div> </div> </a> {% endif %} {% if page.next_page %} - <a href=\"{{ page.next_page.url | url }}\" title=\"{{ page.next_page.title | striptags }}\" class=\"md-flex md-footer-nav__link md-footer-nav__link--next\" rel=\"next\"> - <div class=\"md-flex__cell md-flex__cell--stretch md-footer-nav__title\"> - <span class=\"md-flex__ellipsis\"> + <a href=\"{{ page.next_page.url | url }}\" title=\"{{ page.next_page.title | striptags }}\" class=\"md-footer-nav__link md-footer-nav__link--next\" rel=\"next\"> + <div class=\"md-footer-nav__title\"> + <div class=\"md-ellipsis\"> <span class=\"md-footer-nav__direction\"> {{ lang.t(\"footer.next\") }} </span> {{ page.next_page.title }} - </span> + </div> </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <i class=\"md-icon md-icon--arrow-forward md-footer-nav__button\"></i> + <div class=\"md-footer-nav__button md-icon\"> + {% include \".icons/material/arrow-right.svg\" %} </div> </a> {% endif %}","title":"partials/footer.html"},{"location":"projects/mkdocs/5/#partialsheaderhtml","text":"@@ -2,51 +2,43 @@ This file was automatically generated - do not edit -#} <header class=\"md-header\" data-md-component=\"header\"> - <nav class=\"md-header-nav md-grid\"> - <div class=\"md-flex\"> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" aria-label=\"{{ config.site_name }}\" class=\"md-header-nav__button md-logo\"> - {% if config.theme.logo.icon %} - <i class=\"md-icon\">{{ config.theme.logo.icon }}</i> - {% else %} - <img alt=\"logo\" src=\"{{ config.theme.logo | url }}\" width=\"24\" height=\"24\"> - {% endif %} - </a> - </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <label class=\"md-icon md-icon--menu md-header-nav__button\" for=\"__drawer\"></label> - </div> - <div class=\"md-flex__cell md-flex__cell--stretch\"> - <div class=\"md-flex__ellipsis md-header-nav__title\" data-md-component=\"title\"> - {% if config.site_name == page.title %} - {{ config.site_name }} - {% else %} - <span class=\"md-header-nav__topic\"> - {{ config.site_name }} - </span> - <span class=\"md-header-nav__topic\"> - {% if page and page.meta and page.meta.title %} - {{ page.meta.title }} - {% else %} - {{ page.title }} - {% endif %} - </span> - {% endif %} + <nav class=\"md-header-nav md-grid\" aria-label=\"{{ lang.t('header.title') }}\"> + <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-header-nav__button md-logo\" aria-label=\"{{ config.site_name }}\"> + {% include \"partials/logo.html\" %} + </a> + <label class=\"md-header-nav__button md-icon\" for=\"__drawer\"> + {% include \".icons/material/menu\" ~ \".svg\" %} + </label> + <div class=\"md-header-nav__title\" data-md-component=\"header-title\"> + {% if config.site_name == page.title %} + <div class=\"md-header-nav__ellipsis md-ellipsis\"> + {{ config.site_name }} </div> - </div> - <div class=\"md-flex__cell md-flex__cell--shrink\"> - {% if \"search\" in config[\"plugins\"] %} - <label class=\"md-icon md-icon--search md-header-nav__button\" for=\"__search\"></label> - {% include \"partials/search.html\" %} - {% endif %} - </div> - {% if config.repo_url %} - <div class=\"md-flex__cell md-flex__cell--shrink\"> - <div class=\"md-header-nav__source\"> - {% include \"partials/source.html\" %} - </div> + {% else %} + <div class=\"md-header-nav__ellipsis\"> + <span class=\"md-header-nav__topic md-ellipsis\"> + {{ config.site_name }} + </span> + <span class=\"md-header-nav__topic md-ellipsis\"> + {% if page and page.meta and page.meta.title %} + {{ page.meta.title }} + {% else %} + {{ page.title }} + {% endif %} + </span> </div> {% endif %} </div> + {% if \"search\" in config[\"plugins\"] %} + <label class=\"md-header-nav__button md-icon\" for=\"__search\"> + {% include \".icons/material/magnify.svg\" %} + </label> + {% include \"partials/search.html\" %} + {% endif %} + {% if config.repo_url %} + <div class=\"md-header-nav__source\"> + {% include \"partials/source.html\" %} + </div> + {% endif %} </nav> </header>","title":"partials/header.html"},{"location":"projects/mkdocs/5/#partialsherohtml","text":"@@ -1,9 +1,8 @@ {#- This file was automatically generated - do not edit -#} -{% set feature = config.theme.feature %} {% set class = \"md-hero\" %} -{% if not feature.tabs %} +{% if \"tabs\" not in config.theme.features %} {% set class = \"md-hero md-hero--expand\" %} {% endif %} <div class=\"{{ class }}\" data-md-component=\"hero\">","title":"partials/hero.html"},{"location":"projects/mkdocs/5/#partialslanguagehtml","text":"@@ -3,12 +3,4 @@ -#} {% import \"partials/language/\" + config.theme.language + \".html\" as lang %} {% import \"partials/language/en.html\" as fallback %} -{% macro t(key) %}{{ { - \"direction\": config.theme.direction, - \"search.language\": ( - config.extra.search | default({}) - ).language, - \"search.tokenizer\": ( - config.extra.search | default({}) - ).tokenizer | default(\"\", true), -}[key] or lang.t(key) or fallback.t(key) }}{% endmacro %} +{% macro t(key) %}{{ lang.t(key) | default(fallback.t(key)) }}{% endmacro %}","title":"partials/language.html"},{"location":"projects/mkdocs/5/#partialslogohtml","text":"@@ -0,0 +1,9 @@ +{#- + This file was automatically generated - do not edit +-#} +{% if config.theme.logo %} + <img src=\"{{ config.theme.logo | url }}\" alt=\"logo\"> +{% else %} + {% set icon = config.theme.icon.logo or \"material/library\" %} + {% include \".icons/\" ~ icon ~ \".svg\" %} +{% endif %}","title":"partials/logo.html"},{"location":"projects/mkdocs/5/#partialsnav-itemhtml","text":"@@ -14,9 +14,15 @@ {% endif %} <label class=\"md-nav__link\" for=\"{{ path }}\"> {{ nav_item.title }} + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/chevron-right.svg\" %} + </span> </label> - <nav class=\"md-nav\" data-md-component=\"collapsible\" data-md-level=\"{{ level }}\"> + <nav class=\"md-nav\" aria-label=\"{{ nav_item.title }}\" data-md-level=\"{{ level }}\"> <label class=\"md-nav__title\" for=\"{{ path }}\"> + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} + </span> {{ nav_item.title }} </label> <ul class=\"md-nav__list\" data-md-scrollfix> @@ -39,6 +45,9 @@ {% if toc | first is defined %} <label class=\"md-nav__link md-nav__link--active\" for=\"__toc\"> {{ nav_item.title }} + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/table-of-contents.svg\" %} + </span> </label> {% endif %} <a href=\"{{ nav_item.url | url }}\" title=\"{{ nav_item.title | striptags }}\" class=\"md-nav__link md-nav__link--active\">","title":"partials/nav-item.html"},{"location":"projects/mkdocs/5/#partialsnavhtml","text":"@@ -1,14 +1,10 @@ {#- This file was automatically generated - do not edit -#} -<nav class=\"md-nav md-nav--primary\" data-md-level=\"0\"> - <label class=\"md-nav__title md-nav__title--site\" for=\"__drawer\"> - <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-nav__button md-logo\"> - {% if config.theme.logo.icon %} - <i class=\"md-icon\">{{ config.theme.logo.icon }}</i> - {% else %} - <img alt=\"logo\" src=\"{{ config.theme.logo | url }}\" width=\"48\" height=\"48\"> - {% endif %} +<nav class=\"md-nav md-nav--primary\" aria-label=\"{{ lang.t('nav.title') }}\" data-md-level=\"0\"> + <label class=\"md-nav__title\" for=\"__drawer\"> + <a href=\"{{ config.site_url | default(nav.homepage.url, true) | url }}\" title=\"{{ config.site_name }}\" class=\"md-nav__button md-logo\" aria-label=\"{{ config.site_name }}\"> + {% include \"partials/logo.html\" %} </a> {{ config.site_name }} </label>","title":"partials/nav.html"},{"location":"projects/mkdocs/5/#partialssearchhtml","text":"@@ -6,15 +6,18 @@ <label class=\"md-search__overlay\" for=\"__search\"></label> <div class=\"md-search__inner\" role=\"search\"> <form class=\"md-search__form\" name=\"search\"> - <input type=\"text\" class=\"md-search__input\" name=\"query\" aria-label=\"Search\" placeholder=\"{{ lang.t('search.placeholder') }}\" autocapitalize=\"off\" autocorrect=\"off\" autocomplete=\"off\" spellcheck=\"false\" data-md-component=\"query\" data-md-state=\"active\"> + <input type=\"text\" class=\"md-search__input\" name=\"query\" aria-label=\"{{ lang.t('search.placeholder') }}\" placeholder=\"{{ lang.t('search.placeholder') }}\" autocapitalize=\"off\" autocorrect=\"off\" autocomplete=\"off\" spellcheck=\"false\" data-md-component=\"search-query\" data-md-state=\"active\"> <label class=\"md-search__icon md-icon\" for=\"__search\"> + {% include \".icons/material/magnify.svg\" %} + {% include \".icons/material/arrow-left.svg\" %} </label> - <button type=\"reset\" class=\"md-icon md-search__icon\" data-md-component=\"reset\" tabindex=\"-1\"> - &#xE5CD; + <button type=\"reset\" class=\"md-search__icon md-icon\" aria-label=\"{{ lang.t('search.reset') }}\" data-md-component=\"search-reset\" tabindex=\"-1\"> + {% include \".icons/material/close.svg\" %} </button> </form> <div class=\"md-search__output\"> <div class=\"md-search__scrollwrap\" data-md-scrollfix> - <div class=\"md-search-result\" data-md-component=\"result\"> + <div class=\"md-search-result\" data-md-component=\"search-result\"> <div class=\"md-search-result__meta\"> {{ lang.t(\"search.result.placeholder\") }} </div>","title":"partials/search.html"},{"location":"projects/mkdocs/5/#partialssocialhtml","text":"@@ -3,9 +3,12 @@ -#} {% if config.extra.social %} <div class=\"md-footer-social\"> - <link rel=\"stylesheet\" href=\"{{ 'assets/fonts/font-awesome.css' | url }}\"> {% for social in config.extra.social %} - <a href=\"{{ social.link }}\" target=\"_blank\" rel=\"noopener\" title=\"{{ social.type }}\" class=\"md-footer-social__link fa fa-{{ social.type }}\"></a> + {% set _,rest = social.link.split(\"//\") %} + {% set domain = rest.split(\"/\")[0] %} + <a href=\"{{ social.link }}\" target=\"_blank\" rel=\"noopener\" title=\"{{ domain }}\" class=\"md-footer-social__link\"> + {% include \".icons/\" ~ social.icon ~ \".svg\" %} + </a> {% endfor %} </div> {% endif %}","title":"partials/social.html"},{"location":"projects/mkdocs/5/#partialssource-datehtml","text":"@@ -0,0 +1,15 @@ +{#- + This file was automatically generated - do not edit +-#} +{% import \"partials/language.html\" as lang with context %} +{% set label = lang.t(\"source.revision.date\") %} +<hr> +<div class=\"md-source-date\"> + <small> + {% if page.meta.git_revision_date_localized %} + {{ label }}: {{ page.meta.git_revision_date_localized }} + {% elif page.meta.revision_date %} + {{ label }}: {{ page.meta.revision_date }} + {% endif %} + </small> +</div>","title":"partials/source-date.html"},{"location":"projects/mkdocs/5/#partialssource-linkhtml","text":"@@ -0,0 +1,13 @@ +{#- + This file was automatically generated - do not edit +-#} +{% import \"partials/language.html\" as lang with context %} +{% set repo = config.repo_url %} +{% if repo | last == \"/\" %} + {% set repo = repo[:-1] %} +{% endif %} +{% set path = page.meta.path | default([\"\"]) %} +<a href=\"{{ [repo, path, page.meta.source] | join('/') }}\" title=\"{{ file }}\" class=\"md-content__button md-icon\"> + {{ lang.t(\"meta.source\") }} + {% include \".icons/\" ~ config.theme.icon.repo ~ \".svg\" %} +</a>","title":"partials/source-link.html"},{"location":"projects/mkdocs/5/#partialssourcehtml","text":"@@ -2,24 +2,11 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -{% set platform = config.extra.repo_icon or config.repo_url %} -{% if \"github\" in platform %} - {% set repo_type = \"github\" %} -{% elif \"gitlab\" in platform %} - {% set repo_type = \"gitlab\" %} -{% elif \"bitbucket\" in platform %} - {% set repo_type = \"bitbucket\" %} -{% else %} - {% set repo_type = \"\" %} -{% endif %} -<a href=\"{{ config.repo_url }}\" title=\"{{ lang.t('source.link.title') }}\" class=\"md-source\" data-md-source=\"{{ repo_type }}\"> - {% if repo_type %} - <div class=\"md-source__icon\"> - <svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"> - <use xlink:href=\"#__{{ repo_type }}\" width=\"24\" height=\"24\"></use> - </svg> - </div> - {% endif %} +<a href=\"{{ config.repo_url }}\" title=\"{{ lang.t('source.link.title') }}\" class=\"md-source\"> + <div class=\"md-source__icon md-icon\"> + {% set icon = config.theme.icon.repo or \"fontawesome/brands/git-alt\" %} + {% include \".icons/\" ~ icon ~ \".svg\" %} + </div> <div class=\"md-source__repository\"> {{ config.repo_name }} </div>","title":"partials/source.html"},{"location":"projects/mkdocs/5/#partialstabs-itemhtml","text":"@@ -1,7 +1,7 @@ {#- This file was automatically generated - do not edit -#} -{% if nav_item.is_homepage %} +{% if nav_item.is_homepage or nav_item.url == \"index.html\" %} <li class=\"md-tabs__item\"> {% if not page.ancestors | length and nav | selectattr(\"url\", page.url) %} <a href=\"{{ nav_item.url | url }}\" class=\"md-tabs__link md-tabs__link--active\">","title":"partials/tabs-item.html"},{"location":"projects/mkdocs/5/#partialstabshtml","text":"@@ -5,7 +5,7 @@ {% if page.ancestors | length > 0 %} {% set class = \"md-tabs md-tabs--active\" %} {% endif %} -<nav class=\"{{ class }}\" data-md-component=\"tabs\"> +<nav class=\"{{ class }}\" aria-label=\"{{ lang.t('tabs.title') }}\" data-md-component=\"tabs\"> <div class=\"md-tabs__inner md-grid\"> <ul class=\"md-tabs__list\"> {% for nav_item in nav %}","title":"partials/tabs.html"},{"location":"projects/mkdocs/5/#partialstoc-itemhtml","text":"@@ -6,7 +6,7 @@ {{ toc_item.title }} </a> {% if toc_item.children %} - <nav class=\"md-nav\"> + <nav class=\"md-nav\" aria-label=\"{{ toc_item.title }}\"> <ul class=\"md-nav__list\"> {% for toc_item in toc_item.children %} {% include \"partials/toc-item.html\" %}","title":"partials/toc-item.html"},{"location":"projects/mkdocs/5/#partialstochtml","text":"@@ -2,35 +2,22 @@ This file was automatically generated - do not edit -#} {% import \"partials/language.html\" as lang with context %} -<nav class=\"md-nav md-nav--secondary\"> +<nav class=\"md-nav md-nav--secondary\" aria-label=\"{{ lang.t('toc.title') }}\"> {% endif %} {% if toc | first is defined %} <label class=\"md-nav__title\" for=\"__toc\"> + <span class=\"md-nav__icon md-icon\"> + {% include \".icons/material/arrow-left.svg\" %} + </span> {{ lang.t(\"toc.title\") }} </label> <ul class=\"md-nav__list\" data-md-scrollfix> {% for toc_item in toc %} {% include \"partials/toc-item.html\" %} {% endfor %} - {% if page.meta.source and page.meta.source | length > 0 %} - <li class=\"md-nav__item\"> - <a href=\"#__source\" class=\"md-nav__link md-nav__link--active\"> - {{ lang.t(\"meta.source\") }} - </a> - </li> - {% endif %} - {% set disqus = config.extra.disqus %} - {% if page and page.meta and page.meta.disqus is string %} - {% set disqus = page.meta.disqus %} - {% endif %} - {% if not page.is_homepage and disqus %} - <li class=\"md-nav__item\"> - <a href=\"#__comments\" class=\"md-nav__link md-nav__link--active\"> - {{ lang.t(\"meta.comments\") }} - </a> - </li> - {% endif %} </ul> {% endif %} </nav>","title":"partials/toc.html"},{"location":"projects/mkdocs/admonition/","text":"Admonition \u00b6 Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, e.g. summaries, notes, hints or warnings. Configuration \u00b6 Add the following lines to mkdocs.yml : markdown_extensions : - admonition Usage \u00b6 Admonitions follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Changing the title \u00b6 By default, the Admonition title will equal the type qualifier in titlecase. However, it can be changed by adding a quoted string after the type qualifier. Example: !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Removing the title \u00b6 Similar to changing the title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Embedded content \u00b6 Admonitions can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. The PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Collapsible blocks \u00b6 The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default. Types \u00b6 Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by Material for MkDocs, whereas the default type, and thus fallback for unknown type qualifiers, is note . Note \u00b6 Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso Abstract \u00b6 Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr Info \u00b6 Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo Tip \u00b6 Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important Success \u00b6 Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done Question \u00b6 Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq Warning \u00b6 Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention Failure \u00b6 Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing Danger \u00b6 Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error Bug \u00b6 Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug Example \u00b6 Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example Quote \u00b6 Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Admonition"},{"location":"projects/mkdocs/admonition/#admonition","text":"Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, e.g. summaries, notes, hints or warnings.","title":"Admonition"},{"location":"projects/mkdocs/admonition/#configuration","text":"Add the following lines to mkdocs.yml : markdown_extensions : - admonition","title":"Configuration"},{"location":"projects/mkdocs/admonition/#usage","text":"Admonitions follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Usage"},{"location":"projects/mkdocs/admonition/#changing-the-title","text":"By default, the Admonition title will equal the type qualifier in titlecase. However, it can be changed by adding a quoted string after the type qualifier. Example: !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Changing the title"},{"location":"projects/mkdocs/admonition/#removing-the-title","text":"Similar to changing the title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Removing the title"},{"location":"projects/mkdocs/admonition/#embedded-content","text":"Admonitions can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. The PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Embedded content"},{"location":"projects/mkdocs/admonition/#collapsible-blocks","text":"The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default.","title":"Collapsible blocks"},{"location":"projects/mkdocs/admonition/#types","text":"Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by Material for MkDocs, whereas the default type, and thus fallback for unknown type qualifiers, is note .","title":"Types"},{"location":"projects/mkdocs/admonition/#note","text":"Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso","title":"Note"},{"location":"projects/mkdocs/admonition/#abstract","text":"Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr","title":"Abstract"},{"location":"projects/mkdocs/admonition/#info","text":"Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo","title":"Info"},{"location":"projects/mkdocs/admonition/#tip","text":"Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important","title":"Tip"},{"location":"projects/mkdocs/admonition/#success","text":"Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done","title":"Success"},{"location":"projects/mkdocs/admonition/#question","text":"Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq","title":"Question"},{"location":"projects/mkdocs/admonition/#warning","text":"Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention","title":"Warning"},{"location":"projects/mkdocs/admonition/#failure","text":"Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing","title":"Failure"},{"location":"projects/mkdocs/admonition/#danger","text":"Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error","title":"Danger"},{"location":"projects/mkdocs/admonition/#bug","text":"Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug","title":"Bug"},{"location":"projects/mkdocs/admonition/#example","text":"Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example","title":"Example"},{"location":"projects/mkdocs/admonition/#quote","text":"Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Quote"},{"location":"projects/mkdocs/awesome-pages/","text":"Awesome pages \u00b6 The mkdocs-awesome-pages-plugin omits the need to specify all pages in the nav entry of mkdocs.yml and gives you control over page visibility, titles and order on a directory level. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker. Installation \u00b6 Install the plugin using pip : pip install mkdocs-awesome-pages-plugin Configuration \u00b6 Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - awesome-pages Usage \u00b6 If the nav entry in mkdocs.yml is omitted, MkDocs will automatically include all pages in a specific order. This plugin allows for more fine-grained control on a per-directory basis. In order to configure behavior for a specific directory, create a YAML file named .pages in it and set one of the following options. Setting a directory title \u00b6 The directory title, which is shown as part of the navigation, can be set with: title : Lorem ipsum dolor sit amet Changing the order of pages \u00b6 The order of pages and subsections can be configured with: arrange : - page-1.md - page-2.md - subdirectory Excluding a directory \u00b6 A directory can be hidden (i.e. excluded) with: hide : true Collapsing single-page directories \u00b6 Directories which contain a single page can be collapsed with: collapse : true","title":"Awesome pages"},{"location":"projects/mkdocs/awesome-pages/#awesome-pages","text":"The mkdocs-awesome-pages-plugin omits the need to specify all pages in the nav entry of mkdocs.yml and gives you control over page visibility, titles and order on a directory level. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker.","title":"Awesome pages"},{"location":"projects/mkdocs/awesome-pages/#installation","text":"Install the plugin using pip : pip install mkdocs-awesome-pages-plugin","title":"Installation"},{"location":"projects/mkdocs/awesome-pages/#configuration","text":"Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - awesome-pages","title":"Configuration"},{"location":"projects/mkdocs/awesome-pages/#usage","text":"If the nav entry in mkdocs.yml is omitted, MkDocs will automatically include all pages in a specific order. This plugin allows for more fine-grained control on a per-directory basis. In order to configure behavior for a specific directory, create a YAML file named .pages in it and set one of the following options.","title":"Usage"},{"location":"projects/mkdocs/awesome-pages/#setting-a-directory-title","text":"The directory title, which is shown as part of the navigation, can be set with: title : Lorem ipsum dolor sit amet","title":"Setting a directory title"},{"location":"projects/mkdocs/awesome-pages/#changing-the-order-of-pages","text":"The order of pages and subsections can be configured with: arrange : - page-1.md - page-2.md - subdirectory","title":"Changing the order of pages"},{"location":"projects/mkdocs/awesome-pages/#excluding-a-directory","text":"A directory can be hidden (i.e. excluded) with: hide : true","title":"Excluding a directory"},{"location":"projects/mkdocs/awesome-pages/#collapsing-single-page-directories","text":"Directories which contain a single page can be collapsed with: collapse : true","title":"Collapsing single-page directories"},{"location":"projects/mkdocs/changelog/","text":"Upgrading \u00b6 To upgrade to the latest version: pip install --upgrade mkdocs-material To inspect the currently installed version: pip show mkdocs-material Changelog \u00b6 5.1.5 _ May 3, 2020 \u00b6 Added name attribute for social links to set link title Fixed #1623 : Allow arbitrary links in social links Fixed #1664 : Height of iframe is not adjustable Fixed #1667 : Sidebars are scrolled to bottom on load (bug in Chrome 81+) 5.1.4 _ April 30, 2020 \u00b6 Switched to @mdi/svg Material Design icon package Fixed #1655 : Navigation may disappear after switching viewports Fixed #1659 : Unnecessary scrollbar for search results on Windows Fixed occasional distortions for images with explicit dimensions Fixed errors in German translations 5.1.3 _ April 26, 2020 \u00b6 Fixed overflowing content area after switch to flexbox 5.1.2 _ April 26, 2020 \u00b6 Added status information to search observable Added status information to search modal Removed announcement bar from print media Removed media query packing logic due to race conditions Fixed #1520 : Gracefully disable search on file:// if Worker fails Fixed re-submission of query after search is initialized Fixed jitter of sidebars on all browsers by switching to sticky 5.1.1 _ April 17, 2020 \u00b6 Added new FontAwesome icons Fixed #1609 : Instant loading doesn't honor target=_blank Fixed GitHub stars count rounding errors Fixed GitLab stars count retrieval 5.1.0 _ April 12, 2020 \u00b6 Added support for icons from Markdown through mkdocs-material-extensions 5.0.2 _ April 10, 2020 \u00b6 Added CSS source maps to distribution files Fixed errors in Chinese (Traditional) translations Fixed creation of stale directory on installation from git Improved overflow scrolling behavior on iOS (reduced bundle size by 4kb ) 5.0.1 _ April 7, 2020 \u00b6 Fixed syntax error in Spanish translation 5.0.0 _ April 7, 2020 \u00b6 Reactive architecture \u2013 try app.dialog$.next(\"Hi!\") in the console Instant loading \u2013 make Material behave like a Single Page Application Improved CSS customization with CSS variables \u2013 set your brand's colors Improved CSS resilience, e.g. proper sidebar locking for customized headers Improved icon integration and configuration \u2013 now including over 5k icons Added possibility to use any icon for logo, repository and social links Search UI does not freeze anymore (moved to web worker) Search index built only once when using instant loading Improved extensible keyboard handling Support for prebuilt search indexes Support for displaying stars and forks for GitLab repositories Support for scroll snapping of sidebars and search results Reduced HTML and CSS footprint due to deprecation of Internet Explorer support Slight facelifting of some UI elements (Admonitions, tables, ...) 4.6.3 _ February 14, 2020 \u00b6 Removed optional third-party plugins from requirements.txt Updated Docker image to contain all supported third-party plugins 4.6.2 _ February 8, 2020 \u00b6 Added Romanian translations Fixed #1451 : Inconsistent spacing for fenced code blocks 4.6.1 _ February 8, 2020 \u00b6 Fixed #1324 : Metadata author only rendering first character Fixed #1393 : Set tabindex to 0 for skip to content link Fixed code blocks after Markdown 3.2 release Fixed errors in Japanese translations Improved Google Lighthouse score 4.6.0 _ December 11, 2019 \u00b6 Added support for mkdocs-git-revision-date-localized-plugin Fixed invalid character in Google Fonts URL 4.5.1 _ December 2, 2019 \u00b6 Added Thai translations Fixed missing assets in GitHub release .zip and .tar.gz 4.5.0 _ November 16, 2019 \u00b6 Fixed #1330 : Upgraded EmojiOne to Tweomji due to licensing issues Fixed #1339 : Temporarily pinned PyMdown and Markdown due to Fixed errors in Greek translations Improved GitHub statistics retrieval 4.4.3 _ October 3, 2019 \u00b6 Added Estonian translations Fixed removal of copyright banners in minified JavaScript Removed unnecessary title attributes from links in table of contents 4.4.2 _ August 27, 2019 \u00b6 Added Afrikaans translations Fixed broken page title when h1 contained HTML tags Improved accessibility for IE users Removed unnecessary title attributes from links in navigation 4.4.1 _ August 22, 2019 \u00b6 Added support for black as a primary color Fixed broken footer bar when h1 contained HTML tags 4.4.0 _ June 15, 2019 \u00b6 Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16 4.3.1 _ May 23, 2019 \u00b6 Fixed spelling error in Danish translations 4.3.0 _ May 17, 2019 \u00b6 Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop 4.2.0 _ April 28, 2019 \u00b6 Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize 4.1.2 _ April 16, 2019 \u00b6 Fixed #1072 : HTML tags appearing in navigation link titles 4.1.1 _ March 28, 2019 \u00b6 Fixed minor CSS errors detected during validation 4.1.0 _ March 22, 2019 \u00b6 Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys 4.0.2 _ March 1, 2019 \u00b6 Fixed #1012 : HTML character entities appear in search result titles 4.0.1 _ February 13, 2019 \u00b6 Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing 4.0.0 _ February 13, 2019 \u00b6 Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese ( BREAKING ) Fixed #976 : Removed FastClick 3.3.0 _ January 29, 2019 \u00b6 Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll 3.2.0 _ December 28, 2018 \u00b6 Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously 3.1.0 _ November 17, 2018 \u00b6 Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js) 3.0.6 _ October 26, 2018 \u00b6 Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results 3.0.5 _ October 23, 2018 \u00b6 Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link 3.0.4 _ September 3, 2018 \u00b6 Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled 3.0.3 _ August 7, 2018 \u00b6 Fixed #841 : Additional path levels for extra CSS and JS 3.0.2 _ August 6, 2018 \u00b6 Fixed #839 : Lunr.js stemmer imports incorrect 3.0.1 _ August 5, 2018 \u00b6 Fixed #838 : Search result links incorrect 3.0.0 _ August 5, 2018 \u00b6 Upgraded MkDocs to 1.0 ( BREAKING ) Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations 2.9.4 _ July 29, 2018 \u00b6 Fixed build error after MkDocs upgrade 2.9.3 _ July 29, 2018 \u00b6 Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado 2.9.2 _ June 29, 2018 \u00b6 Added Hindi and Czech translations 2.9.1 _ June 18, 2018 \u00b6 Added support for different spellings for theme color Fixed #799 : Added support for webfont minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable 2.9.0 _ June 13, 2018 \u00b6 Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks 2.8.0 _ June 10, 2018 \u00b6 Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes 2.7.3 _ April 26, 2018 \u00b6 Added Finnish translations 2.7.2 _ April 9, 2018 \u00b6 Fixed rendering issue for details on Edge 2.7.1 _ March 21, 2018 \u00b6 Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation 2.7.0 _ March 6, 2018 \u00b6 Added ability to set absolute URL for logo Added Hebrew translations 2.6.6 _ February 22, 2018 \u00b6 Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available 2.6.5 _ February 22, 2018 \u00b6 Reverted --dev-addr flag removal from Dockerfile 2.6.4 _ February 21, 2018 \u00b6 Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility 2.6.3 _ February 18, 2018 \u00b6 Added Vietnamese translations 2.6.2 _ February 12, 2018 \u00b6 Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables 2.6.1 _ February 11, 2018 \u00b6 Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations 2.6.0 _ February 2, 2018 \u00b6 Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons 2.5.5 _ January 31, 2018 \u00b6 Added Hungarian translations 2.5.4 _ January 29, 2018 \u00b6 Fixed #683 : gh-deploy fails inside Docker 2.5.3 _ January 25, 2018 \u00b6 Added Ukrainian translations 2.5.2 _ January 22, 2018 \u00b6 Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled 2.5.1 _ January 20, 2018 \u00b6 Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link 2.5.0 _ January 13, 2018 \u00b6 Added support for right-to-left languages 2.4.0 _ January 11, 2018 \u00b6 Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop 2.3.0 _ January 9, 2018 \u00b6 Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition 2.2.6 _ December 27, 2017 \u00b6 Added Turkish translations Fixed unclickable area below header in case JavaScript is not available 2.2.5 _ December 18, 2017 \u00b6 Fixed #639 : Broken default favicon 2.2.4 _ December 18, 2017 \u00b6 Fixed #638 : Build breaks with Jinja < 2.9 2.2.3 _ December 13, 2017 \u00b6 Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations 2.2.2 _ December 8, 2017 \u00b6 Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug 2.2.1 _ December 2, 2017 \u00b6 Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing 2.2.0 _ November 22, 2017 \u00b6 Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded 2.1.1 _ November 21, 2017 \u00b6 Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header 2.1.0 _ November 19, 2017 \u00b6 Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment 2.0.4 _ November 5, 2017 \u00b6 Fixed details not opening with footnote reference 2.0.3 _ November 5, 2017 \u00b6 Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer 2.0.2 _ November 1, 2017 \u00b6 Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream 2.0.1 _ October 31, 2017 \u00b6 Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details 2.0.0 _ October 31, 2017 \u00b6 Upgraded MkDocs to 0.17.1 ( BREAKING ) Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations 1.12.2 _ October 26, 2017 \u00b6 Added Italian, Norwegian, French and Chinese translations 1.12.1 _ October 22, 2017 \u00b6 Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file 1.12.0 _ October 20, 2017 \u00b6 Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens 1.11.0 _ October 19, 2017 \u00b6 Added localization to clipboard Refactored localization logic 1.10.4 _ October 18, 2017 \u00b6 Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages 1.10.3 _ October 11, 2017 \u00b6 Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression 1.10.2 _ October 6, 2017 \u00b6 Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari) 1.10.1 _ September 14, 2017 \u00b6 Fixed #455 : Bold code blocks rendered with normal font weight 1.10.0 _ September 1, 2017 \u00b6 Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package 1.9.0 _ August 29, 2017 \u00b6 Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4 1.8.1 _ August 7, 2017 \u00b6 Fixed #421 : Missing pagination for GitHub API 1.8.0 _ August 2, 2017 \u00b6 Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words 1.7.5 _ July 25, 2017 \u00b6 Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility 1.7.4 _ June 21, 2017 \u00b6 Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11 1.7.3 _ June 7, 2017 \u00b6 Fixed error when setting language to Japanese for site search 1.7.2 _ June 6, 2017 \u00b6 Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip 1.7.1 _ June 1, 2017 \u00b6 Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references 1.7.0 _ June 1, 2017 \u00b6 Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages 1.6.4 _ May 24, 2017 \u00b6 Fixed #337 : JavaScript error for GitHub organization URLs 1.6.3 _ May 16, 2017 \u00b6 Fixed #329 : Broken source stats for private or unknown GitHub repos 1.6.2 _ May 15, 2017 \u00b6 Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare 1.6.1 _ April 23, 2017 \u00b6 Fixed following of active/focused element if search input is focused Fixed layer order of search component elements 1.6.0 _ April 22, 2017 \u00b6 Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes 1.5.5 _ April 20, 2017 \u00b6 Fixed #282 : Browser search ( Meta + F ) is hijacked 1.5.4 _ April 8, 2017 \u00b6 Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android 1.5.3 _ April 7, 2017 \u00b6 Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container 1.5.2 _ April 5, 2017 \u00b6 Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form 1.5.1 _ March 30, 2017 \u00b6 Fixed rendering and offset of targeted footnotes Fixed #238 : Link on logo is not set to site_url 1.5.0 _ March 24, 2017 \u00b6 Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings 1.4.1 _ March 16, 2017 \u00b6 Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE) 1.4.0 _ March 16, 2017 \u00b6 Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability 1.3.0 _ March 11, 2017 \u00b6 Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly) 1.2.0 _ March 3, 2017 \u00b6 Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies 1.1.1 _ February 26, 2017 \u00b6 Fixed incorrectly displayed nested lists when using tabs 1.1.0 _ February 26, 2017 \u00b6 Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts 1.0.5 _ February 18, 2017 \u00b6 Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short 1.0.4 _ February 16, 2017 \u00b6 Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem 1.0.3 _ January 22, 2017 \u00b6 Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints 1.0.2 _ January 15, 2017 \u00b6 Fixed #108 : Horizontal scrollbar in content area 1.0.1 _ January 14, 2017 \u00b6 Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows 1.0.0 _ January 13, 2017 \u00b6 Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design icon set Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16 0.2.4 _ June 26, 2016 \u00b6 Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes 0.2.3 _ May 16, 2016 \u00b6 Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP 0.2.2 _ March 20, 2016 \u00b6 Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables 0.2.1 _ March 12, 2016 \u00b6 Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics 0.2.0 _ February 24, 2016 \u00b6 Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible) 0.1.3 _ February 21, 2016 \u00b6 Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8 0.1.2 _ February 16, 2016 \u00b6 Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics 0.1.1 _ February 11, 2016 \u00b6 Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis 0.1.0 _ February 9, 2016 \u00b6 Initial release","title":"Upgrading"},{"location":"projects/mkdocs/changelog/#upgrading","text":"To upgrade to the latest version: pip install --upgrade mkdocs-material To inspect the currently installed version: pip show mkdocs-material","title":"Upgrading"},{"location":"projects/mkdocs/changelog/#changelog","text":"","title":"Changelog"},{"location":"projects/mkdocs/changelog/#515-_-may-3-2020","text":"Added name attribute for social links to set link title Fixed #1623 : Allow arbitrary links in social links Fixed #1664 : Height of iframe is not adjustable Fixed #1667 : Sidebars are scrolled to bottom on load (bug in Chrome 81+)","title":"5.1.5 _ May 3, 2020"},{"location":"projects/mkdocs/changelog/#514-_-april-30-2020","text":"Switched to @mdi/svg Material Design icon package Fixed #1655 : Navigation may disappear after switching viewports Fixed #1659 : Unnecessary scrollbar for search results on Windows Fixed occasional distortions for images with explicit dimensions Fixed errors in German translations","title":"5.1.4 _ April 30, 2020"},{"location":"projects/mkdocs/changelog/#513-_-april-26-2020","text":"Fixed overflowing content area after switch to flexbox","title":"5.1.3 _ April 26, 2020"},{"location":"projects/mkdocs/changelog/#512-_-april-26-2020","text":"Added status information to search observable Added status information to search modal Removed announcement bar from print media Removed media query packing logic due to race conditions Fixed #1520 : Gracefully disable search on file:// if Worker fails Fixed re-submission of query after search is initialized Fixed jitter of sidebars on all browsers by switching to sticky","title":"5.1.2 _ April 26, 2020"},{"location":"projects/mkdocs/changelog/#511-_-april-17-2020","text":"Added new FontAwesome icons Fixed #1609 : Instant loading doesn't honor target=_blank Fixed GitHub stars count rounding errors Fixed GitLab stars count retrieval","title":"5.1.1 _ April 17, 2020"},{"location":"projects/mkdocs/changelog/#510-_-april-12-2020","text":"Added support for icons from Markdown through mkdocs-material-extensions","title":"5.1.0 _ April 12, 2020"},{"location":"projects/mkdocs/changelog/#502-_-april-10-2020","text":"Added CSS source maps to distribution files Fixed errors in Chinese (Traditional) translations Fixed creation of stale directory on installation from git Improved overflow scrolling behavior on iOS (reduced bundle size by 4kb )","title":"5.0.2 _ April 10, 2020"},{"location":"projects/mkdocs/changelog/#501-_-april-7-2020","text":"Fixed syntax error in Spanish translation","title":"5.0.1 _ April 7, 2020"},{"location":"projects/mkdocs/changelog/#500-_-april-7-2020","text":"Reactive architecture \u2013 try app.dialog$.next(\"Hi!\") in the console Instant loading \u2013 make Material behave like a Single Page Application Improved CSS customization with CSS variables \u2013 set your brand's colors Improved CSS resilience, e.g. proper sidebar locking for customized headers Improved icon integration and configuration \u2013 now including over 5k icons Added possibility to use any icon for logo, repository and social links Search UI does not freeze anymore (moved to web worker) Search index built only once when using instant loading Improved extensible keyboard handling Support for prebuilt search indexes Support for displaying stars and forks for GitLab repositories Support for scroll snapping of sidebars and search results Reduced HTML and CSS footprint due to deprecation of Internet Explorer support Slight facelifting of some UI elements (Admonitions, tables, ...)","title":"5.0.0 _ April 7, 2020"},{"location":"projects/mkdocs/changelog/#463-_-february-14-2020","text":"Removed optional third-party plugins from requirements.txt Updated Docker image to contain all supported third-party plugins","title":"4.6.3 _ February 14, 2020"},{"location":"projects/mkdocs/changelog/#462-_-february-8-2020","text":"Added Romanian translations Fixed #1451 : Inconsistent spacing for fenced code blocks","title":"4.6.2 _ February 8, 2020"},{"location":"projects/mkdocs/changelog/#461-_-february-8-2020","text":"Fixed #1324 : Metadata author only rendering first character Fixed #1393 : Set tabindex to 0 for skip to content link Fixed code blocks after Markdown 3.2 release Fixed errors in Japanese translations Improved Google Lighthouse score","title":"4.6.1 _ February 8, 2020"},{"location":"projects/mkdocs/changelog/#460-_-december-11-2019","text":"Added support for mkdocs-git-revision-date-localized-plugin Fixed invalid character in Google Fonts URL","title":"4.6.0 _ December 11, 2019"},{"location":"projects/mkdocs/changelog/#451-_-december-2-2019","text":"Added Thai translations Fixed missing assets in GitHub release .zip and .tar.gz","title":"4.5.1 _ December 2, 2019"},{"location":"projects/mkdocs/changelog/#450-_-november-16-2019","text":"Fixed #1330 : Upgraded EmojiOne to Tweomji due to licensing issues Fixed #1339 : Temporarily pinned PyMdown and Markdown due to Fixed errors in Greek translations Improved GitHub statistics retrieval","title":"4.5.0 _ November 16, 2019"},{"location":"projects/mkdocs/changelog/#443-_-october-3-2019","text":"Added Estonian translations Fixed removal of copyright banners in minified JavaScript Removed unnecessary title attributes from links in table of contents","title":"4.4.3 _ October 3, 2019"},{"location":"projects/mkdocs/changelog/#442-_-august-27-2019","text":"Added Afrikaans translations Fixed broken page title when h1 contained HTML tags Improved accessibility for IE users Removed unnecessary title attributes from links in navigation","title":"4.4.2 _ August 27, 2019"},{"location":"projects/mkdocs/changelog/#441-_-august-22-2019","text":"Added support for black as a primary color Fixed broken footer bar when h1 contained HTML tags","title":"4.4.1 _ August 22, 2019"},{"location":"projects/mkdocs/changelog/#440-_-june-15-2019","text":"Added Slovenian translations Reverted template minification in favor of mkdocs-minify-plugin Fixed #1114 : Tabs don't reappear when default font-size is smaller than 16","title":"4.4.0 _ June 15, 2019"},{"location":"projects/mkdocs/changelog/#431-_-may-23-2019","text":"Fixed spelling error in Danish translations","title":"4.3.1 _ May 23, 2019"},{"location":"projects/mkdocs/changelog/#430-_-may-17-2019","text":"Added support for changing header through metadata title property Added font-display: swap to Google Font loading logic Removed whitespace from templates, saving 4kb ( .7kb gzipped) per request Fixed alignment of repository icons on tablet and desktop","title":"4.3.0 _ May 17, 2019"},{"location":"projects/mkdocs/changelog/#420-_-april-28-2019","text":"Added Norwegian (Nynorsk) translations Fixed loss of focus in non-form input elements due to search hotkeys Fixed #1067 : Search hotkeys not working for mobile/tablet screensize Fixed #1068 : Search not correctly aligned for tablet screensize","title":"4.2.0 _ April 28, 2019"},{"location":"projects/mkdocs/changelog/#412-_-april-16-2019","text":"Fixed #1072 : HTML tags appearing in navigation link titles","title":"4.1.2 _ April 16, 2019"},{"location":"projects/mkdocs/changelog/#411-_-march-28-2019","text":"Fixed minor CSS errors detected during validation","title":"4.1.1 _ March 28, 2019"},{"location":"projects/mkdocs/changelog/#410-_-march-22-2019","text":"Fixed #1023 : Search for Asian languages broken after Lunr.js update Fixed #1026 : contenteditable elements loose focus on hotkeys","title":"4.1.0 _ March 22, 2019"},{"location":"projects/mkdocs/changelog/#402-_-march-1-2019","text":"Fixed #1012 : HTML character entities appear in search result titles","title":"4.0.2 _ March 1, 2019"},{"location":"projects/mkdocs/changelog/#401-_-february-13-2019","text":"Fixed #762 , #816 : Glitch in sidebar when collapsing items Fixed #869 : Automatically expand details before printing","title":"4.0.1 _ February 13, 2019"},{"location":"projects/mkdocs/changelog/#400-_-february-13-2019","text":"Added background on hover for table rows Removed Google Tag Manager and reverted to Google Analytics Removed blocks in partials - Jinja doesn't support them Fixed #911 : Chrome breaks layout if system language is Chinese ( BREAKING ) Fixed #976 : Removed FastClick","title":"4.0.0 _ February 13, 2019"},{"location":"projects/mkdocs/changelog/#330-_-january-29-2019","text":"Moved Google Analytics integration into head using Google Tag Manager Fixed #972 : Unicode slugifier breaks table of contents blur on scroll Fixed #974 : Additional links in table of contents break blur on scroll","title":"3.3.0 _ January 29, 2019"},{"location":"projects/mkdocs/changelog/#320-_-december-28-2018","text":"Added support for redirects using metadata refresh Fixed #921 : Load Google Analytics snippet asynchronously","title":"3.2.0 _ December 28, 2018"},{"location":"projects/mkdocs/changelog/#310-_-november-17-2018","text":"Added support for Progressive Web App Manifest Fixed #915 : Search bug in Safari (upgraded Lunr.js)","title":"3.1.0 _ November 17, 2018"},{"location":"projects/mkdocs/changelog/#306-_-october-26-2018","text":"Added Taiwanese translations Fixed #906 : JavaScript code blocks evaluated in search results","title":"3.0.6 _ October 26, 2018"},{"location":"projects/mkdocs/changelog/#305-_-october-23-2018","text":"Added Croatian and Indonesian translations Fixed #899 : Skip-to-content link invalid from 2 nd level on Fixed #902 : Missing URL filter in footer for FontAwesome link","title":"3.0.5 _ October 23, 2018"},{"location":"projects/mkdocs/changelog/#304-_-september-3-2018","text":"Updated Dutch translations Fixed #856 : Removed preconnect meta tag if Google Fonts are disabled","title":"3.0.4 _ September 3, 2018"},{"location":"projects/mkdocs/changelog/#303-_-august-7-2018","text":"Fixed #841 : Additional path levels for extra CSS and JS","title":"3.0.3 _ August 7, 2018"},{"location":"projects/mkdocs/changelog/#302-_-august-6-2018","text":"Fixed #839 : Lunr.js stemmer imports incorrect","title":"3.0.2 _ August 6, 2018"},{"location":"projects/mkdocs/changelog/#301-_-august-5-2018","text":"Fixed #838 : Search result links incorrect","title":"3.0.1 _ August 5, 2018"},{"location":"projects/mkdocs/changelog/#300-_-august-5-2018","text":"Upgraded MkDocs to 1.0 ( BREAKING ) Upgraded Python in official Docker image to 3.6 Added Serbian and Serbo-Croatian translations","title":"3.0.0 _ August 5, 2018"},{"location":"projects/mkdocs/changelog/#294-_-july-29-2018","text":"Fixed build error after MkDocs upgrade","title":"2.9.4 _ July 29, 2018"},{"location":"projects/mkdocs/changelog/#293-_-july-29-2018","text":"Added link to home for logo in drawer Fixed dependency problems between MkDocs and Tornado","title":"2.9.3 _ July 29, 2018"},{"location":"projects/mkdocs/changelog/#292-_-june-29-2018","text":"Added Hindi and Czech translations","title":"2.9.2 _ June 29, 2018"},{"location":"projects/mkdocs/changelog/#291-_-june-18-2018","text":"Added support for different spellings for theme color Fixed #799 : Added support for webfont minification in production Fixed #800 : Added .highlighttable as an alias for .codehilitetable","title":"2.9.1 _ June 18, 2018"},{"location":"projects/mkdocs/changelog/#290-_-june-13-2018","text":"Added support for theme color on Android Fixed #796 : Rendering of nested tabbed code blocks","title":"2.9.0 _ June 13, 2018"},{"location":"projects/mkdocs/changelog/#280-_-june-10-2018","text":"Added support for grouping code blocks with tabs Added Material and FontAwesome icon fonts to distribution files (GDPR) Added note on compliance with GDPR Added Slovak translations Fixed #790 : Prefixed id attributes with __ to avoid name clashes","title":"2.8.0 _ June 10, 2018"},{"location":"projects/mkdocs/changelog/#273-_-april-26-2018","text":"Added Finnish translations","title":"2.7.3 _ April 26, 2018"},{"location":"projects/mkdocs/changelog/#272-_-april-9-2018","text":"Fixed rendering issue for details on Edge","title":"2.7.2 _ April 9, 2018"},{"location":"projects/mkdocs/changelog/#271-_-march-21-2018","text":"Added Galician translations Fixed #730 : Scroll chasing error on home page if Disqus is enabled Fixed #736 : Reset drawer and search upon back button invocation","title":"2.7.1 _ March 21, 2018"},{"location":"projects/mkdocs/changelog/#270-_-march-6-2018","text":"Added ability to set absolute URL for logo Added Hebrew translations","title":"2.7.0 _ March 6, 2018"},{"location":"projects/mkdocs/changelog/#266-_-february-22-2018","text":"Added preconnect for Google Fonts for faster loading Fixed #710 : With tabs sidebar disappears if JavaScript is not available","title":"2.6.6 _ February 22, 2018"},{"location":"projects/mkdocs/changelog/#265-_-february-22-2018","text":"Reverted --dev-addr flag removal from Dockerfile","title":"2.6.5 _ February 22, 2018"},{"location":"projects/mkdocs/changelog/#264-_-february-21-2018","text":"Added Catalan translations Fixed incorrect margins for buttons in Firefox and Safari Replaced package manager yarn with npm 5.6 Reverted GitHub stars rounding method Removed --dev-addr flag from Dockerfile for Windows compatibility","title":"2.6.4 _ February 21, 2018"},{"location":"projects/mkdocs/changelog/#263-_-february-18-2018","text":"Added Vietnamese translations","title":"2.6.3 _ February 18, 2018"},{"location":"projects/mkdocs/changelog/#262-_-february-12-2018","text":"Added Arabic translations Fixed incorrect rounding of amount of GitHub stars Fixed double-layered borders for tables","title":"2.6.2 _ February 12, 2018"},{"location":"projects/mkdocs/changelog/#261-_-february-11-2018","text":"Added ability to override Disqus integration using metadata Fixed #690 : Duplicate slashes in source file URLs Fixed #696 : Active page highlight not working with default palette Adjusted German translations","title":"2.6.1 _ February 11, 2018"},{"location":"projects/mkdocs/changelog/#260-_-february-2-2018","text":"Moved default search configuration to default translation (English) Added support to automatically set text direction from translation Added support to disable search stop word filter in translation Added support to disable search trimmer in translation Added Persian translations Fixed support for Polish search Fixed disappearing GitHub, GitLab and Bitbucket repository icons","title":"2.6.0 _ February 2, 2018"},{"location":"projects/mkdocs/changelog/#255-_-january-31-2018","text":"Added Hungarian translations","title":"2.5.5 _ January 31, 2018"},{"location":"projects/mkdocs/changelog/#254-_-january-29-2018","text":"Fixed #683 : gh-deploy fails inside Docker","title":"2.5.4 _ January 29, 2018"},{"location":"projects/mkdocs/changelog/#253-_-january-25-2018","text":"Added Ukrainian translations","title":"2.5.3 _ January 25, 2018"},{"location":"projects/mkdocs/changelog/#252-_-january-22-2018","text":"Added default search language mappings for all localizations Fixed #673 : Error loading non-existent search language Fixed #675 : Uncaught reference error when search plugin disabled","title":"2.5.2 _ January 22, 2018"},{"location":"projects/mkdocs/changelog/#251-_-january-20-2018","text":"Fixed permalink for main headline Improved missing translation handling with English as a fallback Improved accessibility with skip-to-content link","title":"2.5.1 _ January 20, 2018"},{"location":"projects/mkdocs/changelog/#250-_-january-13-2018","text":"Added support for right-to-left languages","title":"2.5.0 _ January 13, 2018"},{"location":"projects/mkdocs/changelog/#240-_-january-11-2018","text":"Added focus state for clipboard buttons Fixed #400 : Search bar steals tab focus Fixed search not closing on Enter when result is selected Fixed search not closing when losing focus due to Tab Fixed collapsed navigation links getting focus Fixed outline being cut off on Tab focus of navigation links Fixed bug with first search result navigation being ignored Removed search result navigation via Tab (use Up and Down ) Removed outline resets for links Improved general tabbing behavior on desktop","title":"2.4.0 _ January 11, 2018"},{"location":"projects/mkdocs/changelog/#230-_-january-9-2018","text":"Added example (synonym: snippet ) style for Admonition Added synonym abstract for summary style for Admonition","title":"2.3.0 _ January 9, 2018"},{"location":"projects/mkdocs/changelog/#226-_-december-27-2017","text":"Added Turkish translations Fixed unclickable area below header in case JavaScript is not available","title":"2.2.6 _ December 27, 2017"},{"location":"projects/mkdocs/changelog/#225-_-december-18-2017","text":"Fixed #639 : Broken default favicon","title":"2.2.5 _ December 18, 2017"},{"location":"projects/mkdocs/changelog/#224-_-december-18-2017","text":"Fixed #638 : Build breaks with Jinja < 2.9","title":"2.2.4 _ December 18, 2017"},{"location":"projects/mkdocs/changelog/#223-_-december-13-2017","text":"Fixed #630 : Admonition sets padding on any last child Adjusted Chinese (Traditional) translations","title":"2.2.3 _ December 13, 2017"},{"location":"projects/mkdocs/changelog/#222-_-december-8-2017","text":"Added Dutch translations Adjusted targeted link and footnote offsets Simplified Admonition styles and fixed padding bug","title":"2.2.2 _ December 8, 2017"},{"location":"projects/mkdocs/changelog/#221-_-december-2-2017","text":"Fixed #616 : Minor styling error with title-only admonition blocks Removed border for table of contents and improved spacing","title":"2.2.1 _ December 2, 2017"},{"location":"projects/mkdocs/changelog/#220-_-november-22-2017","text":"Added support for hero teaser Added Portuguese translations Fixed #586 : Footnote backref target offset regression Fixed #605 : Search stemmers not correctly loaded","title":"2.2.0 _ November 22, 2017"},{"location":"projects/mkdocs/changelog/#211-_-november-21-2017","text":"Replaced deprecated babel-preset-es2015 with babel-preset-env Refactored Gulp build pipeline with Webpack Removed right border on sidebars Fixed broken color transition on header","title":"2.1.1 _ November 21, 2017"},{"location":"projects/mkdocs/changelog/#210-_-november-19-2017","text":"Added support for white as a primary color Added support for sliding site name and title Fixed redundant clipboard button when using line numbers on code blocks Improved header appearance by making it taller Improved tabs appearance Improved CSS customizability by leveraging inheritance Removed scroll shadows via background-attachment","title":"2.1.0 _ November 19, 2017"},{"location":"projects/mkdocs/changelog/#204-_-november-5-2017","text":"Fixed details not opening with footnote reference","title":"2.0.4 _ November 5, 2017"},{"location":"projects/mkdocs/changelog/#203-_-november-5-2017","text":"Added Japanese translations Fixed #540 : Jumping to anchor inside details doesn't open it Fixed active link colors in footer","title":"2.0.3 _ November 5, 2017"},{"location":"projects/mkdocs/changelog/#202-_-november-1-2017","text":"Added Russian translations Fixed #542 : Horizontal scrollbar between 1220px and 1234px Fixed #553 : Metadata values only rendering first character Fixed #558 : Flash of unstyled content Fixed favicon regression caused by deprecation upstream","title":"2.0.2 _ November 1, 2017"},{"location":"projects/mkdocs/changelog/#201-_-october-31-2017","text":"Fixed error when initializing search Fixed styles for link to edit the current page Fixed styles on nested admonition in details","title":"2.0.1 _ October 31, 2017"},{"location":"projects/mkdocs/changelog/#200-_-october-31-2017","text":"Upgraded MkDocs to 0.17.1 ( BREAKING ) Added support for easier configuration of search tokenizer Added support to disable search Added Korean translations","title":"2.0.0 _ October 31, 2017"},{"location":"projects/mkdocs/changelog/#1122-_-october-26-2017","text":"Added Italian, Norwegian, French and Chinese translations","title":"1.12.2 _ October 26, 2017"},{"location":"projects/mkdocs/changelog/#1121-_-october-22-2017","text":"Added Polish, Swedish and Spanish translations Improved downward compatibility with custom partials Temporarily pinned MkDocs version within Docker image to 0.16.3 Fixed #519 : Missing theme configuration file","title":"1.12.1 _ October 22, 2017"},{"location":"projects/mkdocs/changelog/#1120-_-october-20-2017","text":"Added support for setting language(s) via mkdocs.yml Added support for default localization Added German and Danish translations Fixed #374 : Search bar misalignment on big screens","title":"1.12.0 _ October 20, 2017"},{"location":"projects/mkdocs/changelog/#1110-_-october-19-2017","text":"Added localization to clipboard Refactored localization logic","title":"1.11.0 _ October 19, 2017"},{"location":"projects/mkdocs/changelog/#1104-_-october-18-2017","text":"Improved print styles of code blocks Improved search UX (don't close on enter if no selection) Fixed #495 : Vertical scrollbar on short pages","title":"1.10.4 _ October 18, 2017"},{"location":"projects/mkdocs/changelog/#1103-_-october-11-2017","text":"Fixed #484 : Vertical scrollbar on some MathJax formulas Fixed #483 : Footnote backref target offset regression","title":"1.10.3 _ October 11, 2017"},{"location":"projects/mkdocs/changelog/#1102-_-october-6-2017","text":"Fixed #468 : Sidebar shows scrollbar if content is shorter (in Safari)","title":"1.10.2 _ October 6, 2017"},{"location":"projects/mkdocs/changelog/#1101-_-september-14-2017","text":"Fixed #455 : Bold code blocks rendered with normal font weight","title":"1.10.1 _ September 14, 2017"},{"location":"projects/mkdocs/changelog/#1100-_-september-1-2017","text":"Added support to make logo default icon configurable Fixed uninitialized overflow scrolling on main pane for iOS Fixed error in mobile navigation in case JavaScript is not available Fixed incorrect color transition for nested panes in mobile navigation Improved checkbox styles for Tasklist from PyMdown Extension package","title":"1.10.0 _ September 1, 2017"},{"location":"projects/mkdocs/changelog/#190-_-august-29-2017","text":"Added info (synonym: todo ) style for Admonition Added question (synonym: help , faq ) style for Admonition Added support for Details from PyMdown Extensions package Improved Admonition styles to match Details Improved styles for social links in footer Replaced ligatures with Unicode code points to avoid broken layout Upgraded PyMdown Extensions package dependency to >= 3.4","title":"1.9.0 _ August 29, 2017"},{"location":"projects/mkdocs/changelog/#181-_-august-7-2017","text":"Fixed #421 : Missing pagination for GitHub API","title":"1.8.1 _ August 7, 2017"},{"location":"projects/mkdocs/changelog/#180-_-august-2-2017","text":"Added support for lazy-loading of search results for better performance Added support for customization of search tokenizer/separator Fixed #424 : Search doesn't handle capital letters anymore Fixed #419 : Search doesn't work on whole words","title":"1.8.0 _ August 2, 2017"},{"location":"projects/mkdocs/changelog/#175-_-july-25-2017","text":"Fixed #398 : Forms broken due to search shortcuts Improved search overall user experience Improved search matching and highlighting Improved search accessibility","title":"1.7.5 _ July 25, 2017"},{"location":"projects/mkdocs/changelog/#174-_-june-21-2017","text":"Fixed functional link colors in table of contents for active palette Fixed #368 : Compatibility issues with IE11","title":"1.7.4 _ June 21, 2017"},{"location":"projects/mkdocs/changelog/#173-_-june-7-2017","text":"Fixed error when setting language to Japanese for site search","title":"1.7.3 _ June 7, 2017"},{"location":"projects/mkdocs/changelog/#172-_-june-6-2017","text":"Fixed offset of search box when repo_url is not set Fixed non-disappearing tooltip","title":"1.7.2 _ June 6, 2017"},{"location":"projects/mkdocs/changelog/#171-_-june-1-2017","text":"Fixed wrong z-index order of header, overlay and drawer Fixed wrong offset of targeted footnote back references","title":"1.7.1 _ June 1, 2017"},{"location":"projects/mkdocs/changelog/#170-_-june-1-2017","text":"Added \"copy to clipboard\" buttons to code blocks Added support for multilingual site search Fixed search term highlighting for non-latin languages","title":"1.7.0 _ June 1, 2017"},{"location":"projects/mkdocs/changelog/#164-_-may-24-2017","text":"Fixed #337 : JavaScript error for GitHub organization URLs","title":"1.6.4 _ May 24, 2017"},{"location":"projects/mkdocs/changelog/#163-_-may-16-2017","text":"Fixed #329 : Broken source stats for private or unknown GitHub repos","title":"1.6.3 _ May 16, 2017"},{"location":"projects/mkdocs/changelog/#162-_-may-15-2017","text":"Fixed #316 : Fatal error for git clone on Windows Fixed #320 : Chrome 58 creates double underline for abbr tags Fixed #323 : Ligatures rendered inside code blocks Fixed miscalculated sidebar height due to missing margin collapse Changed deprecated MathJax CDN to Cloudflare","title":"1.6.2 _ May 15, 2017"},{"location":"projects/mkdocs/changelog/#161-_-april-23-2017","text":"Fixed following of active/focused element if search input is focused Fixed layer order of search component elements","title":"1.6.1 _ April 23, 2017"},{"location":"projects/mkdocs/changelog/#160-_-april-22-2017","text":"Added build test for Docker image on Travis Added search overlay for better user experience (focus) Added language from localizations to html tag Fixed #270 : source links broken for absolute URLs Fixed missing top spacing for first targeted element in content Fixed too small footnote divider when using larger font sizes","title":"1.6.0 _ April 22, 2017"},{"location":"projects/mkdocs/changelog/#155-_-april-20-2017","text":"Fixed #282 : Browser search ( Meta + F ) is hijacked","title":"1.5.5 _ April 20, 2017"},{"location":"projects/mkdocs/changelog/#154-_-april-8-2017","text":"Fixed broken highlighting for two or more search terms Fixed missing search results when only a h1 is present Fixed unresponsive overlay on Android","title":"1.5.4 _ April 8, 2017"},{"location":"projects/mkdocs/changelog/#153-_-april-7-2017","text":"Fixed deprecated calls for template variables Fixed wrong palette color for focused search result Fixed JavaScript errors on 404 page Fixed missing top spacing on 404 page Fixed missing right spacing on overflow of source container","title":"1.5.3 _ April 7, 2017"},{"location":"projects/mkdocs/changelog/#152-_-april-5-2017","text":"Added requirements as explicit dependencies in setup.py Fixed non-synchronized transitions in search form","title":"1.5.2 _ April 5, 2017"},{"location":"projects/mkdocs/changelog/#151-_-march-30-2017","text":"Fixed rendering and offset of targeted footnotes Fixed #238 : Link on logo is not set to site_url","title":"1.5.1 _ March 30, 2017"},{"location":"projects/mkdocs/changelog/#150-_-march-24-2017","text":"Added support for localization of search placeholder Added keyboard events for quick access of search Added keyboard events for search control Added opacity on hover for search buttons Added git hook to skip CI build on non-src changes Fixed non-resetting search placeholder when input is cleared Fixed error for unescaped parentheses in search term Fixed #229 : Button to clear search missing Fixed #231 : Escape key doesn't exit search Removed old-style figures from font feature settings","title":"1.5.0 _ March 24, 2017"},{"location":"projects/mkdocs/changelog/#141-_-march-16-2017","text":"Fixed invalid destructuring attempt on NodeList (in Safari, Edge, IE)","title":"1.4.1 _ March 16, 2017"},{"location":"projects/mkdocs/changelog/#140-_-march-16-2017","text":"Added support for grouping searched sections by documents Added support for highlighting of search terms Added support for localization of search results Fixed #216 : table of contents icon doesn't show if h1 is not present Reworked style and layout of search results for better usability","title":"1.4.0 _ March 16, 2017"},{"location":"projects/mkdocs/changelog/#130-_-march-11-2017","text":"Added support for page-specific title and description using metadata Added support for linking source files to documentation Fixed jitter and offset of sidebar when zooming browser Fixed incorrectly initialized tablet sidebar height Fixed regression for #1 : GitHub stars break if repo_url ends with a / Fixed undesired white line below copyright footer due to base font scaling Fixed issue with whitespace in path for scripts Fixed #205 : support non-fixed (static) header Refactored footnote references for better visibility Reduced repaints to a minimum for non-tabs configuration Reduced contrast of edit button (slightly)","title":"1.3.0 _ March 11, 2017"},{"location":"projects/mkdocs/changelog/#120-_-march-3-2017","text":"Added quote (synonym: cite ) style for Admonition Added help message to build pipeline Fixed wrong navigation link colors when applying palette Fixed #197 : Link missing in tabs navigation on deeply nested items Removed unnecessary dev dependencies","title":"1.2.0 _ March 3, 2017"},{"location":"projects/mkdocs/changelog/#111-_-february-26-2017","text":"Fixed incorrectly displayed nested lists when using tabs","title":"1.1.1 _ February 26, 2017"},{"location":"projects/mkdocs/changelog/#110-_-february-26-2017","text":"Added tabs navigation feature (optional) Added Disqus integration (optional) Added a high resolution Favicon with the new logo Added static type checking using Facebook's Flow Fixed #173 : Dictionary elements have no bottom spacing Fixed #175 : Tables cannot be set to 100% width Fixed race conditions in build related to asset revisioning Fixed accidentally re-introduced Permalink on top-level headline Fixed alignment of logo in drawer on IE11 Refactored styles related to tables Refactored and automated Docker build and PyPI release Refactored build scripts","title":"1.1.0 _ February 26, 2017"},{"location":"projects/mkdocs/changelog/#105-_-february-18-2017","text":"Fixed #153 : Sidebar flows out of constrained area in Chrome 56 Fixed #159 : Footer jitter due to JavaScript if content is short","title":"1.0.5 _ February 18, 2017"},{"location":"projects/mkdocs/changelog/#104-_-february-16-2017","text":"Fixed #142 : Documentation build errors if h1 is defined as raw HTML Fixed #164 : PyPI release does not build and install Fixed offsets of targeted headlines Increased sidebar font size by 0.12rem","title":"1.0.4 _ February 16, 2017"},{"location":"projects/mkdocs/changelog/#103-_-january-22-2017","text":"Fixed #117 : Table of contents items don't blur on fast scrolling Refactored sidebar positioning logic Further reduction of repaints","title":"1.0.3 _ January 22, 2017"},{"location":"projects/mkdocs/changelog/#102-_-january-15-2017","text":"Fixed #108 : Horizontal scrollbar in content area","title":"1.0.2 _ January 15, 2017"},{"location":"projects/mkdocs/changelog/#101-_-january-14-2017","text":"Fixed massive repaints happening when scrolling Fixed footer back reference positions in case of overflow Fixed header logo from showing when the menu icon is rendered Changed scrollbar behavior to only show when content overflows","title":"1.0.1 _ January 14, 2017"},{"location":"projects/mkdocs/changelog/#100-_-january-13-2017","text":"Introduced Webpack for more sophisticated JavaScript bundling Introduced ESLint and Stylelint for code style checks Introduced more accurate Material Design colors and shadows Introduced modular scales for harmonic font sizing Introduced git-hooks for better development workflow Rewrite of CSS using the BEM methodology and SassDoc guidelines Rewrite of JavaScript using ES6 and Babel as a transpiler Rewrite of Admonition, Permalinks and CodeHilite integration Rewrite of the complete typographical system Rewrite of Gulp asset pipeline in ES6 and separation of tasks Removed Bower as a dependency in favor of NPM Removed custom icon build in favor of the Material Design icon set Removed _blank targets on links due to vulnerability: http://bit.ly/1Mk2Rtw Removed unversioned assets from build directory Restructured templates into base templates and partials Added build and watch scripts in package.json Added support for Metadata and Footnotes Markdown extensions Added support for PyMdown Extensions package Added support for collapsible sections in navigation Added support for separate table of contents Added support for better accessibility through REM-based layout Added icons for GitHub, GitLab and BitBucket integrations Added more detailed documentation on specimen, extensions etc. Added a 404.html error page for deployment on GitHub Pages Fixed live reload chain in watch mode when saving a template Fixed variable references to work with MkDocs 0.16","title":"1.0.0 _ January 13, 2017"},{"location":"projects/mkdocs/changelog/#024-_-june-26-2016","text":"Fixed improperly set default favicon Fixed #33 : Protocol relative URL for webfonts doesn't work with file:// Fixed #34 : IE11 on Windows 7 doesn't honor max-width on main tag Fixed #35 : Add styling for blockquotes","title":"0.2.4 _ June 26, 2016"},{"location":"projects/mkdocs/changelog/#023-_-may-16-2016","text":"Fixed #25 : Highlight inline fenced blocks Fixed #26 : Better highlighting for keystrokes Fixed #30 : Suboptimal syntax highlighting for PHP","title":"0.2.3 _ May 16, 2016"},{"location":"projects/mkdocs/changelog/#022-_-march-20-2016","text":"Fixed #15 : Document Pygments dependency for CodeHilite Fixed #16 : Favicon could not be set through mkdocs.yml Fixed #17 : Put version into own container for styling Fixed #20 : Fix rounded borders for tables","title":"0.2.2 _ March 20, 2016"},{"location":"projects/mkdocs/changelog/#021-_-march-12-2016","text":"Fixed #10 : Invisible header after closing search bar with ESC key Fixed #13 : Table cells don't wrap Fixed empty list in table of contents when no headline is defined Corrected wrong path for static asset monitoring in Gulpfile.js Set up tracking of site search for Google Analytics","title":"0.2.1 _ March 12, 2016"},{"location":"projects/mkdocs/changelog/#020-_-february-24-2016","text":"Fixed #6 : Include multiple color palettes via mkdocs.yml Fixed #7 : Better colors for links inside admonition notes and warnings Fixed #9 : Text for prev/next footer navigation should be customizable Refactored templates (replaced if / else with modifiers where possible)","title":"0.2.0 _ February 24, 2016"},{"location":"projects/mkdocs/changelog/#013-_-february-21-2016","text":"Fixed #3 : Ordered lists within an unordered list have ::before content Fixed #4 : Click on Logo/Title without Github-Repository: \"None\" Fixed #5 : Page without headlines renders empty list in table of contents Moved Modernizr to top to ensure basic usability in IE8","title":"0.1.3 _ February 21, 2016"},{"location":"projects/mkdocs/changelog/#012-_-february-16-2016","text":"Fixed styles for deep navigational hierarchies Fixed webfont delivery problem when hosted in subdirectories Fixed print styles in mobile/tablet configuration Added option to configure fonts in mkdocs.yml with fallbacks Changed styles for admonition notes and warnings Set download link to latest version if available Set up tracking of outgoing links and actions for Google Analytics","title":"0.1.2 _ February 16, 2016"},{"location":"projects/mkdocs/changelog/#011-_-february-11-2016","text":"Fixed #1 : GitHub stars don't work if the repo_url ends with a / Updated NPM and Bower dependencies to most recent versions Changed footer/copyright link to Material theme to GitHub pages Made MkDocs building/serving in build process optional Set up continuous integration with Travis","title":"0.1.1 _ February 11, 2016"},{"location":"projects/mkdocs/changelog/#010-_-february-9-2016","text":"Initial release","title":"0.1.0 _ February 9, 2016"},{"location":"projects/mkdocs/codehilite/","text":"CodeHilite \u00b6 CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. It uses Pygments during the compilation of the Markdown file to provide syntax highlighting for over 300 languages and has no JavaScript runtime dependency. Configuration \u00b6 Add the following lines to mkdocs.yml : markdown_extensions : - codehilite Usage \u00b6 Specifying the language \u00b6 The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways. via Markdown syntax recommended \u00b6 In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language identifier directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf via Shebang \u00b6 Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf via three colons \u00b6 If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf Adding line numbers \u00b6 Line numbers can be added to a code block by enabling the linenums flag in mkdocs.yml or adding linenums=1 right after the language identifier: markdown_extensions : - codehilite : linenums : true Example: ``` python linenums=\"1\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Grouping code blocks \u00b6 The Tabbed extension which is part of the PyMdown Extensions package adds support for grouping Markdown blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: === \"Bash\" ``` bash #!/bin/bash echo \"Hello world!\" ``` === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` === \"C#\" ``` c# using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Highlighting specific lines \u00b6 Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Supported languages excerpt \u00b6 CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt. Bash \u00b6 #!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $? C \u00b6 extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ } C++ \u00b6 Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); } C# \u00b6 public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); } Clojure \u00b6 ( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ]) Diff \u00b6 Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js', Docker \u00b6 FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ] Elixir \u00b6 require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end Erlang \u00b6 circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end . F# \u00b6 /// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle } Go \u00b6 package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) } HTML \u00b6 <!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html > Java \u00b6 import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } } JavaScript \u00b6 var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports ); JSON \u00b6 { \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... } Julia \u00b6 using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider ) Lua \u00b6 local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" ) MySQL \u00b6 SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ; PHP \u00b6 <?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } } Protocol Buffers \u00b6 syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; } Python \u00b6 \"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) Ruby \u00b6 require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end Scala \u00b6 // Every record of this DataFrame contains the label and // features represented by a vector. val df = sqlContext . createDataFrame ( data ). toDF ( \"label\" , \"features\" ) // Set parameters for the algorithm. // Here, we limit the number of iterations to 10. val lr = new LogisticRegression (). setMaxIter ( 10 ) // Fit the model to the data. val model = lr . fit ( df ) // Inspect the model: get the feature weights. val weights = model . weights // Given a dataset, predict each point's label, and show the results. model . transform ( df ). show () g XML \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"CodeHilite"},{"location":"projects/mkdocs/codehilite/#codehilite","text":"CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. It uses Pygments during the compilation of the Markdown file to provide syntax highlighting for over 300 languages and has no JavaScript runtime dependency.","title":"CodeHilite"},{"location":"projects/mkdocs/codehilite/#configuration","text":"Add the following lines to mkdocs.yml : markdown_extensions : - codehilite","title":"Configuration"},{"location":"projects/mkdocs/codehilite/#usage","text":"","title":"Usage"},{"location":"projects/mkdocs/codehilite/#specifying-the-language","text":"The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways.","title":"Specifying the language"},{"location":"projects/mkdocs/codehilite/#via-markdown-syntax-recommended","text":"In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language identifier directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf","title":"via Markdown syntax recommended"},{"location":"projects/mkdocs/codehilite/#via-shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf","title":"via Shebang"},{"location":"projects/mkdocs/codehilite/#via-three-colons","text":"If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf","title":"via three colons"},{"location":"projects/mkdocs/codehilite/#adding-line-numbers","text":"Line numbers can be added to a code block by enabling the linenums flag in mkdocs.yml or adding linenums=1 right after the language identifier: markdown_extensions : - codehilite : linenums : true Example: ``` python linenums=\"1\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Adding line numbers"},{"location":"projects/mkdocs/codehilite/#grouping-code-blocks","text":"The Tabbed extension which is part of the PyMdown Extensions package adds support for grouping Markdown blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: === \"Bash\" ``` bash #!/bin/bash echo \"Hello world!\" ``` === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` === \"C#\" ``` c# using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Grouping code blocks"},{"location":"projects/mkdocs/codehilite/#highlighting-specific-lines","text":"Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"projects/mkdocs/codehilite/#supported-languages-excerpt","text":"CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt.","title":"Supported languages excerpt"},{"location":"projects/mkdocs/codehilite/#bash","text":"#!/bin/bash for OPT in \" $@ \" do case \" $OPT \" in '-f' ) canonicalize = 1 ;; '-n' ) switchlf = \"-n\" ;; esac done # readlink -f function __readlink_f { target = \" $1 \" while test -n \" $target \" ; do filepath = \" $target \" cd ` dirname \" $filepath \" ` target = ` readlink \" $filepath \" ` done /bin/echo $switchlf ` pwd -P ` / ` basename \" $filepath \" ` } if [ ! \" $canonicalize \" ] ; then readlink $switchlf \" $@ \" else for file in \" $@ \" do case \" $file \" in -* ) ;; * ) __readlink_f \" $file \" ;; esac done fi exit $?","title":"Bash"},{"location":"projects/mkdocs/codehilite/#c","text":"extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data && left ); left = left > 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask & mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] & 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ }","title":"C"},{"location":"projects/mkdocs/codehilite/#c_1","text":"Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ \"signature\" ] = descriptor_ -> full_name (); /* Prepare message symbol */ variables_ [ \"message\" ] = StringReplace ( variables_ [ \"signature\" ], \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"message\" ])); /* Suffix scope to identifiers, if given */ string suffix ( \"\" ); if ( scope_ ) { suffix = scope_ -> full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ -> file () -> package (). compare ( descriptor_ -> file () -> package ())) suffix = StripPrefixString ( suffix , scope_ -> file () -> package () + \".\" ); /* Append to signature */ variables_ [ \"signature\" ] += \".[\" + suffix + \"]\" ; suffix = \"_\" + suffix ; } /* Prepare extension symbol */ variables_ [ \"extension\" ] = StringReplace ( suffix , \".\" , \"_\" , true ); LowerString ( & ( variables_ [ \"extension\" ])); }","title":"C++"},{"location":"projects/mkdocs/codehilite/#c_2","text":"public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount > startTickCount + timeout ) throw new Exception ( \"Timeout.\" ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent < size ); }","title":"C&#35;"},{"location":"projects/mkdocs/codehilite/#clojure","text":"( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % \">>\" )) [ \"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\" ])","title":"Clojure"},{"location":"projects/mkdocs/codehilite/#diff","text":"Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js',","title":"Diff"},{"location":"projects/mkdocs/codehilite/#docker","text":"FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [ \"x11vnc\" , \"-forever\" , \"-usepw\" , \"-create\" ]","title":"Docker"},{"location":"projects/mkdocs/codehilite/#elixir","text":"require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info \"Accepting connections on port #{ port } \" loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket |> read_line () |> write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end","title":"Elixir"},{"location":"projects/mkdocs/codehilite/#erlang","text":"circular ( Defs ) -> [ { { Type , Base }, Fields } || { { Type , Base }, Fields } <- Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) -> Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) -> false ; circular ( Defs , [ Field | Fields ], Path ) -> case Field #field.type of { msg , Type } -> case lists : member ( Type , Path ) of false -> Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false -> circular ( Defs , Fields , Path ); true -> true end ; true -> Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ -> circular ( Defs , Fields , Path ) end .","title":"Erlang"},{"location":"projects/mkdocs/codehilite/#f","text":"/// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [< Js >] getRectangles () : Async < Rectangle [] > = async { let req = XMLHttpRequest () req . Open ( \"POST\" , \"/get\" , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [< Js >] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects |> Array . iter createRectangle }","title":"F&#35;"},{"location":"projects/mkdocs/codehilite/#go","text":"package main import \"fmt\" func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i < 10000000 ; i ++ { fmt . Println ( \"process\" , id , \" send\" , i ) channel <- 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( \"receiving\" ) x += i } fmt . Println ( x ) }","title":"Go"},{"location":"projects/mkdocs/codehilite/#html","text":"<!doctype html> < html class = \"no-js\" lang = \"\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"x-ua-compatible\" content = \"ie=edge\" > < title > HTML5 Boilerplate </ title > < meta name = \"description\" content = \"\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"apple-touch-icon\" href = \"apple-touch-icon.png\" > < link rel = \"stylesheet\" href = \"css/normalize.css\" > < link rel = \"stylesheet\" href = \"css/main.css\" > < script src = \"js/vendor/modernizr-2.8.3.min.js\" ></ script > </ head > < body > < p > Hello world! This is HTML5 Boilerplate. </ p > </ body > </ html >","title":"HTML"},{"location":"projects/mkdocs/codehilite/#java","text":"import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet < E > { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList < E >[] con ; public UnsortedHashSet () { con = ( LinkedList < E >[] )( new LinkedList [ 10 ] ); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList < E > (); if ( ! con [ index ] . contains ( obj )) { con [ index ] . add ( obj ); size ++ ; } if ( 1.0 * size / con . length > LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet < E > temp = new UnsortedHashSet < E > (); temp . con = ( LinkedList < E >[] )( new LinkedList [ con . length * 2 + 1 ] ); for ( int i = 0 ; i < con . length ; i ++ ) { if ( con [ i ] != null ) for ( E e : con [ i ] ) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } }","title":"Java"},{"location":"projects/mkdocs/codehilite/#javascript","text":"var Math = require ( 'lib/math' ); var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ 'default' ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ 'default' ], exports );","title":"JavaScript"},{"location":"projects/mkdocs/codehilite/#json","text":"{ \"name\" : \"mkdocs-material\" , \"version\" : \"0.2.4\" , \"description\" : \"A Material Design theme for MkDocs\" , \"homepage\" : \"http://squidfunk.github.io/mkdocs-material/\" , \"authors\" : [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\" : \"MIT\" , \"main\" : \"Gulpfile.js\" , \"scripts\" : { \"start\" : \"./node_modules/.bin/gulp watch --mkdocs\" , \"build\" : \"./node_modules/.bin/gulp build --production\" } ... }","title":"JSON"},{"location":"projects/mkdocs/codehilite/#julia","text":"using MXNet mlp = @mx . chain mx . Variable ( : data ) => mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) => mx . Activation ( name =: relu1 , act_type =: relu ) => mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) => mx . Activation ( name =: relu2 , act_type =: relu ) => mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) => mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( \"MXNet\" , \"examples\" , \"mnist\" , \"mnist-data.jl\" )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider )","title":"Julia"},{"location":"projects/mkdocs/codehilite/#lua","text":"local ffi = require ( \"ffi\" ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == \"Windows\" then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( \".\" ); io.flush () sleep ( 0.01 ) end io.write ( \" \\n \" )","title":"Lua"},{"location":"projects/mkdocs/codehilite/#mysql","text":"SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = '087652' ;","title":"MySQL"},{"location":"projects/mkdocs/codehilite/#php","text":"<?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( '<html><body>Lucky number: ' . $number . '</body></html>' ); } }","title":"PHP"},{"location":"projects/mkdocs/codehilite/#protocol-buffers","text":"syntax = \"proto2\" ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; }","title":"Protocol Buffers"},{"location":"projects/mkdocs/codehilite/#python","text":"\"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( 'data_dir' , '/tmp/data/' , 'Directory for storing data' ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b )","title":"Python"},{"location":"projects/mkdocs/codehilite/#ruby","text":"require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError ; end class MissingCallback < StandardError ; end class InvalidState < StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, & block @finity ||= Machine . new self , options , & block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end","title":"Ruby"},{"location":"projects/mkdocs/codehilite/#scala","text":"// Every record of this DataFrame contains the label and // features represented by a vector. val df = sqlContext . createDataFrame ( data ). toDF ( \"label\" , \"features\" ) // Set parameters for the algorithm. // Here, we limit the number of iterations to 10. val lr = new LogisticRegression (). setMaxIter ( 10 ) // Fit the model to the data. val model = lr . fit ( df ) // Inspect the model: get the feature weights. val weights = model . weights // Given a dataset, predict each point's label, and show the results. model . transform ( df ). show () g","title":"Scala"},{"location":"projects/mkdocs/codehilite/#xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs= \"http://www.w3.org/2001/XMLSchema\" > <!-- This is a sample comment --> <childTag attribute= \"Quoted Value\" another-attribute= 'Single quoted value' a-third-attribute= '123' > <withTextContent> Some text content </withTextContent> <withEntityContent> Some text content with &lt; entities &gt; and mentioning uint8_t and int32_t </withEntityContent> <otherTag attribute= 'Single quoted Value' /> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"XML"},{"location":"projects/mkdocs/contributing/","text":"Contributing \u00b6 Interested in contributing to the Material for MkDocs? Want to report a bug? Before you do, please read the following guidelines. Submission context \u00b6 Got a question or problem? \u00b6 For quick questions there's no need to open an issue as you can reach us on gitter.im . Found a bug? \u00b6 If you found a bug in the source code, you can help us by submitting an issue to the issue tracker in our GitHub repository. Even better, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines . Missing a feature? \u00b6 You can request a new feature by submitting an issue to our GitHub Repository. If you would like to implement a new feature, please submit an issue with a proposal for your work first, to be sure that it is of use for everyone, as the Material for MkDocs is highly opinionated. Please consider what kind of change it is: For a major feature , first open an issue and outline your proposal so that it can be discussed. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you to craft the change so that it is successfully accepted into the project. Small features and bugs can be crafted and directly submitted as a Pull Request. However, there is no guarantee that your feature will make it into the master , as it's always a matter of opinion whether if benefits the overall functionality of the project. Submission guidelines \u00b6 Submitting an issue \u00b6 Before you submit an issue, please search the issue tracker, maybe an issue for your problem already exists and the discussion might inform you of workarounds readily available. We want to fix all the issues as soon as possible, but before fixing a bug we need to reproduce and confirm it. In order to reproduce bugs we will systematically ask you to provide a minimal reproduction scenario using the custom issue template. Please stick to the issue template. Unfortunately we are not able to investigate / fix bugs without a minimal reproduction scenario, so if we don't hear back from you we may close the issue. Submitting a Pull Request (PR) \u00b6 Search GitHub for an open or closed PR that relates to your submission. You don't want to duplicate effort. If you do not find a related issue or PR, go ahead. Development : Fork the project, set up the development environment , make your changes in a separate git branch and add descriptive messages to your commits. Build : Before submitting a pull requests, build the theme . This is a mandatory requirement for your PR to get accepted, as the theme should at all times be installable through GitHub. Pull Request : After building the theme, commit the compiled output, push your branch to GitHub and send a PR to mkdocs-material:master . If we suggest changes, make the required updates, rebase your branch and push the changes to your GitHub repository, which will automatically update your PR. After your PR is merged, you can safely delete your branch and pull the changes from the main (upstream) repository.","title":"Contributing"},{"location":"projects/mkdocs/contributing/#contributing","text":"Interested in contributing to the Material for MkDocs? Want to report a bug? Before you do, please read the following guidelines.","title":"Contributing"},{"location":"projects/mkdocs/contributing/#submission-context","text":"","title":"Submission context"},{"location":"projects/mkdocs/contributing/#got-a-question-or-problem","text":"For quick questions there's no need to open an issue as you can reach us on gitter.im .","title":"Got a question or problem?"},{"location":"projects/mkdocs/contributing/#found-a-bug","text":"If you found a bug in the source code, you can help us by submitting an issue to the issue tracker in our GitHub repository. Even better, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines .","title":"Found a bug?"},{"location":"projects/mkdocs/contributing/#missing-a-feature","text":"You can request a new feature by submitting an issue to our GitHub Repository. If you would like to implement a new feature, please submit an issue with a proposal for your work first, to be sure that it is of use for everyone, as the Material for MkDocs is highly opinionated. Please consider what kind of change it is: For a major feature , first open an issue and outline your proposal so that it can be discussed. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you to craft the change so that it is successfully accepted into the project. Small features and bugs can be crafted and directly submitted as a Pull Request. However, there is no guarantee that your feature will make it into the master , as it's always a matter of opinion whether if benefits the overall functionality of the project.","title":"Missing a feature?"},{"location":"projects/mkdocs/contributing/#submission-guidelines","text":"","title":"Submission guidelines"},{"location":"projects/mkdocs/contributing/#submitting-an-issue","text":"Before you submit an issue, please search the issue tracker, maybe an issue for your problem already exists and the discussion might inform you of workarounds readily available. We want to fix all the issues as soon as possible, but before fixing a bug we need to reproduce and confirm it. In order to reproduce bugs we will systematically ask you to provide a minimal reproduction scenario using the custom issue template. Please stick to the issue template. Unfortunately we are not able to investigate / fix bugs without a minimal reproduction scenario, so if we don't hear back from you we may close the issue.","title":"Submitting an issue"},{"location":"projects/mkdocs/contributing/#submitting-a-pull-request-pr","text":"Search GitHub for an open or closed PR that relates to your submission. You don't want to duplicate effort. If you do not find a related issue or PR, go ahead. Development : Fork the project, set up the development environment , make your changes in a separate git branch and add descriptive messages to your commits. Build : Before submitting a pull requests, build the theme . This is a mandatory requirement for your PR to get accepted, as the theme should at all times be installable through GitHub. Pull Request : After building the theme, commit the compiled output, push your branch to GitHub and send a PR to mkdocs-material:master . If we suggest changes, make the required updates, rebase your branch and push the changes to your GitHub repository, which will automatically update your PR. After your PR is merged, you can safely delete your branch and pull the changes from the main (upstream) repository.","title":"Submitting a Pull Request (PR)"},{"location":"projects/mkdocs/customization/","text":"Customization \u00b6 A great starting point \u00b6 Project documentation is as diverse as the projects themselves and Material for MkDocs is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve your brand's style. Adding assets \u00b6 MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory. Additional stylesheets \u00b6 If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css : - stylesheets/extra.css Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server supports live reloading. Additional JavaScript \u00b6 The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript : - javascripts/extra.js Further assistance can be found in the MkDocs documentation . Extending the theme \u00b6 If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. MkDocs supports theme extension , an easy way to override parts of a theme without forking and changing the main theme. Setup and theme structure \u00b6 Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides which you reference using custom_dir : theme : name : material custom_dir : overrides Theme extension prerequisites As the custom_dir variable is used for the theme extension process, the Material for MkDocs needs to be installed via pip and referenced with the name parameter in your mkdocs.yml . The structure in the overrides directory must mirror the directory structure of the original theme, as any file in the overrides directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the overrides directory. The directory layout of the theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 integrations/ # 3rd-party integrations \u2502 \u251c\u2500 language/ # Localized languages \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 hero.html # Hero teaser \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source-date.html # Last updated date \u2502 \u251c\u2500 source-link.html # Link to source file \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404 .html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page Overriding partials \u00b6 In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the overrides directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file. Overriding template blocks \u00b6 Besides overriding partials, one can also override so called template blocks , which are defined inside the templates and wrap specific features. To override a template block, create a main.html inside the overrides directory and define the block, e.g.: {% extends \"base.html\" %} {% block htmltitle %} <title>Lorem ipsum dolor sit amet</title> {% endblock %} Material for MkDocs provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration announce Wraps the Announcement bar config Wraps the JavaScript application config content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar hero Wraps the hero teaser (if available) htmltitle Wraps the <title> tag libs Wraps the JavaScript libraries (header) scripts Wraps the JavaScript application (footer) source Wraps the linked source files site_meta Wraps the meta tags in the document head site_nav Wraps the site navigation and table of contents styles Wraps the stylesheets (also extra sources) tabs Wraps the tabs navigation (if available) For more on this topic refer to the MkDocs documentation Theme development \u00b6 Material for MkDocs uses Webpack as a build tool to leverage modern web technologies like TypeScript and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the theme and recompile it. This is fairly easy. Environment setup \u00b6 In order to start development on Material for MkDocs, a Node.js version of at least 12 is required. First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt npm install Development mode \u00b6 Start the Webpack watchdog with: npm start Then, in a second session, start the MkDocs server with: mkdocs serve Point your browser to localhost:8000 and you should see this documentation in front of you. Automatically generated files Never make any changes in the material directory, as the contents of this directory are automatically generated from the src directory and will be overridden when the theme is built. Build process \u00b6 When you've finished making your changes, you can build the theme by invoking: npm run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final files are located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original theme.","title":"Customization"},{"location":"projects/mkdocs/customization/#customization","text":"","title":"Customization"},{"location":"projects/mkdocs/customization/#a-great-starting-point","text":"Project documentation is as diverse as the projects themselves and Material for MkDocs is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve your brand's style.","title":"A great starting point"},{"location":"projects/mkdocs/customization/#adding-assets","text":"MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory.","title":"Adding assets"},{"location":"projects/mkdocs/customization/#additional-stylesheets","text":"If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css : - stylesheets/extra.css Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server supports live reloading.","title":"Additional stylesheets"},{"location":"projects/mkdocs/customization/#additional-javascript","text":"The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript : - javascripts/extra.js Further assistance can be found in the MkDocs documentation .","title":"Additional JavaScript"},{"location":"projects/mkdocs/customization/#extending-the-theme","text":"If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. MkDocs supports theme extension , an easy way to override parts of a theme without forking and changing the main theme.","title":"Extending the theme"},{"location":"projects/mkdocs/customization/#setup-and-theme-structure","text":"Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides which you reference using custom_dir : theme : name : material custom_dir : overrides Theme extension prerequisites As the custom_dir variable is used for the theme extension process, the Material for MkDocs needs to be installed via pip and referenced with the name parameter in your mkdocs.yml . The structure in the overrides directory must mirror the directory structure of the original theme, as any file in the overrides directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the overrides directory. The directory layout of the theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 integrations/ # 3rd-party integrations \u2502 \u251c\u2500 language/ # Localized languages \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 hero.html # Hero teaser \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source-date.html # Last updated date \u2502 \u251c\u2500 source-link.html # Link to source file \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404 .html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page","title":"Setup and theme structure"},{"location":"projects/mkdocs/customization/#overriding-partials","text":"In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the overrides directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file.","title":"Overriding partials"},{"location":"projects/mkdocs/customization/#overriding-template-blocks","text":"Besides overriding partials, one can also override so called template blocks , which are defined inside the templates and wrap specific features. To override a template block, create a main.html inside the overrides directory and define the block, e.g.: {% extends \"base.html\" %} {% block htmltitle %} <title>Lorem ipsum dolor sit amet</title> {% endblock %} Material for MkDocs provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration announce Wraps the Announcement bar config Wraps the JavaScript application config content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar hero Wraps the hero teaser (if available) htmltitle Wraps the <title> tag libs Wraps the JavaScript libraries (header) scripts Wraps the JavaScript application (footer) source Wraps the linked source files site_meta Wraps the meta tags in the document head site_nav Wraps the site navigation and table of contents styles Wraps the stylesheets (also extra sources) tabs Wraps the tabs navigation (if available) For more on this topic refer to the MkDocs documentation","title":"Overriding template blocks"},{"location":"projects/mkdocs/customization/#theme-development","text":"Material for MkDocs uses Webpack as a build tool to leverage modern web technologies like TypeScript and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the theme and recompile it. This is fairly easy.","title":"Theme development"},{"location":"projects/mkdocs/customization/#environment-setup","text":"In order to start development on Material for MkDocs, a Node.js version of at least 12 is required. First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt npm install","title":"Environment setup"},{"location":"projects/mkdocs/customization/#development-mode","text":"Start the Webpack watchdog with: npm start Then, in a second session, start the MkDocs server with: mkdocs serve Point your browser to localhost:8000 and you should see this documentation in front of you. Automatically generated files Never make any changes in the material directory, as the contents of this directory are automatically generated from the src directory and will be overridden when the theme is built.","title":"Development mode"},{"location":"projects/mkdocs/customization/#build-process","text":"When you've finished making your changes, you can build the theme by invoking: npm run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final files are located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original theme.","title":"Build process"},{"location":"projects/mkdocs/data-privacy/","text":"Data privacy \u00b6 In itself, Material for MkDocs does not perform any tracking and should adhere to the General Data Protection Regulation (GDPR), but it integrates with some third-party services that may not. Third-party services \u00b6 Google Fonts \u00b6 Material for MkDocs makes fonts easily configurable by relying on Google Fonts CDN. Embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . If you need to ensure GDPR compliance, you may disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material for MkDocs will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You can easily include your own, self-hosted webfont by overriding the fonts block. Google Analytics and Disqus \u00b6 Material for MkDocs comes with optional Google Analytics and Disqus integrations, both of which must be enabled explicitly.","title":"Data privacy"},{"location":"projects/mkdocs/data-privacy/#data-privacy","text":"In itself, Material for MkDocs does not perform any tracking and should adhere to the General Data Protection Regulation (GDPR), but it integrates with some third-party services that may not.","title":"Data privacy"},{"location":"projects/mkdocs/data-privacy/#third-party-services","text":"","title":"Third-party services"},{"location":"projects/mkdocs/data-privacy/#google-fonts","text":"Material for MkDocs makes fonts easily configurable by relying on Google Fonts CDN. Embedding fonts from Google is currently within a gray area as there's no official statement or ruling regarding GDPR compliance and the topic is still actively discussed . If you need to ensure GDPR compliance, you may disable the usage of the Google Font CDN with: theme : font : false When Google Fonts are disabled, Material for MkDocs will default to Helvetica Neue and Monaco with their corresponding fall backs, relying on system fonts. You can easily include your own, self-hosted webfont by overriding the fonts block.","title":"Google Fonts"},{"location":"projects/mkdocs/data-privacy/#google-analytics-and-disqus","text":"Material for MkDocs comes with optional Google Analytics and Disqus integrations, both of which must be enabled explicitly.","title":"Google Analytics and Disqus"},{"location":"projects/mkdocs/footnotes/","text":"Footnotes \u00b6 Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add inline footnotes to your documentation. Configuration \u00b6 Add the following lines to mkdocs.yml : markdown_extensions : - footnotes Usage \u00b6 The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document. Inserting the reference \u00b6 The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Inserting the content \u00b6 The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference. on a single line \u00b6 Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page on multiple lines \u00b6 Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Footnotes"},{"location":"projects/mkdocs/footnotes/#footnotes","text":"Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add inline footnotes to your documentation.","title":"Footnotes"},{"location":"projects/mkdocs/footnotes/#configuration","text":"Add the following lines to mkdocs.yml : markdown_extensions : - footnotes","title":"Configuration"},{"location":"projects/mkdocs/footnotes/#usage","text":"The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document.","title":"Usage"},{"location":"projects/mkdocs/footnotes/#inserting-the-reference","text":"The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Inserting the reference"},{"location":"projects/mkdocs/footnotes/#inserting-the-content","text":"The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference.","title":"Inserting the content"},{"location":"projects/mkdocs/footnotes/#on-a-single-line","text":"Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page","title":"on a single line"},{"location":"projects/mkdocs/footnotes/#on-multiple-lines","text":"Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"on multiple lines"},{"location":"projects/mkdocs/getting-started/","text":"Getting started \u00b6 Installation \u00b6 While there are several ways of installing Material for MkDocs, the recommended methods are either by using pip \u2013 the Python package manager \u2013 or by pulling the official Docker image . with pip recommended \u00b6 Material for MkDocs can be installed with pip : pip install mkdocs-material Note that this will automatically install compatible versions of MkDocs , Markdown , Pygments and PyMdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately. Installation in a virtual environment The best way to make sure that you end up with the correct versions and without any incompatibility problems between packages it to use a virtual environment . Don't know what this is or how to set it up? We recommend to start by reading a tutorial on virtual environments for Python. Installation on macOS When you're running the pre-installed version of Python on macOS, pip tries to install packages in a folder for which your user might not have the adequate permissions. There are two possible solutions for this: Installing in user space (recommended): Provide the --user flag to the install command and pip will install the package in a user-site location. This is the recommended way. Switching to a homebrewed Python : Upgrade your Python installation to a self-contained solution by installing Python with Homebrew. This should eliminate a lot of problems you could be having with pip . Error: unrecognized theme 'material' If you run into this error, the most common reason is that you installed MkDocs through some package manager (e.g. Homebrew or apt-get ) and Material for MkDocs through pip , so both packages end up in different locations. MkDocs only checks its install location for themes. with docker recommended \u00b6 The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. Start the development server in your project root \u2013 the folder where mkdocs.yml resides \u2014 with: Unix docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material Windows docker run --rm -it -p 8000:8000 -v \"%cd%\":/docs squidfunk/mkdocs-material with git \u00b6 Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material . Configuration \u00b6 Depending on your installation method, you can now add the following lines to mkdocs.yml in your project root. If you installed Material for MkDocs using a package manager, add: theme : name : material If you cloned Material for MkDocs from GitHub add: theme : name : null custom_dir : mkdocs-material/material MkDocs includes a development server, so you can preview your changes as you write your documentation. The development server can be started with the following command: mkdocs serve Point your browser to http://localhost:8000 and your documentation should greet you in a new look. If you're starting from scratch, the following configuration can be used as a starting point: Example configuration This is an excerpt from the mkdocs.yml used to render these pages: # Project information site_name : Material for MkDocs site_description : A Material Design theme for MkDocs site_author : Martin Donath site_url : https://squidfunk.github.io/mkdocs-material/ # Repository repo_name : squidfunk/mkdocs-material repo_url : https://github.com/squidfunk/mkdocs-material # Copyright copyright : Copyright &copy; 2016 - 2020 Martin Donath # Configuration theme : name : material language : en palette : primary : indigo accent : indigo font : text : Roboto code : Roboto Mono # Extras extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk - icon : fontawesome/brands/twitter link : https://twitter.com/squidfunk - icon : fontawesome/brands/linkedin link : https://linkedin.com/in/squidfunk # Google Analytics google_analytics : - UA-XXXXXXXX-X - auto # Extensions markdown_extensions : - admonition - codehilite : guess_lang : false - toc : permalink : true Feature flags \u00b6 These optional features are hidden behind flags and can be explicitly enabled in mkdocs.yml . Instant loading \u00b6 The (still experimental) instant loading feature will intercept clicks on all internal links and dispatch them directly via XHR without a full page reload. It can be enabled from mkdocs.yml with: theme : features : - instant The resulting page is parsed and injected and all event handlers and components are automatically rebound. This means that Material for MkDocs behaves like a Single Page Application , which is especially useful for large documentation sites that come with a huge search index, as the search index will now remain intact in-between document switches. Tabs \u00b6 The tabs feature will render top-level subsections in another navigational layer below the header on big screens (but leave them untouched on mobile). It can be enabled from mkdocs.yml with: theme : features : - tabs Note that all top-level pages (i.e. all top-level entries that directly refer to an *.md file) defined inside the nav entry of mkdocs.yml will be grouped under the first tab which will receive the title of the first page. This means that there will effectively be no collapsible subsections for the first tab, as each subsection is rendered as another tab. If you want more fine-grained control, i.e., collapsible subsections for the first tab, you can move all top-level pages into a subsection , so that the top-level is entirely made up of subsections. Note that tabs are only shown for larger screens, so make sure that navigation is plausible on mobile devices. As an example, see the mkdocs.yml used to render these pages. Language \u00b6 Default: en Material for MkDocs supports internationalization (i18n) and provides translations for all template variables and labels. You can set the language from mkdocs.yml with: theme : language : en The following language codes are supported: .md-language-list { -webkit-columns: 2; -moz-columns: 2; columns: 2; } .md-language-list li { -webkit-column-break-inside: avoid; page-break-inside: avoid; break-inside: avoid; } af / Afrikaans ar / Arabic ca / Catalan zh / Chinese (Simplified) zh-Hant / Chinese (Traditional) zh-TW / Chinese (Taiwanese) hr / Croatian cs / Czech da / Danish nl / Dutch en / English et / Estonian fi / Finnish fr / French gl / Galician de / German gr / Greek he / Hebrew hi / Hindi hu / Hungarian id / Indonesian it / Italian ja / Japanese kr / Korean no / Norwegian nn / Norwegian (Nynorsk) fa / Persian pl / Polish pt / Portuguese ro / Romanian ru / Russian sr / Serbian sh / Serbo-Croatian sk / Slovak si / Slovenian es / Spanish sv / Swedish th / Thai tr / Turkish uk / Ukrainian vi / Vietnamese Add language While many languages are read ltr (left-to-right), Material for MkDocs also supports rtl (right-to-left) directionality which is inferred from the selected language, but can also be set with: theme : direction : rtl Color palette \u00b6 The Material Design color palette comes with 20 hues, all of which are included with Material for MkDocs. Primary and accent colors can be set from the project root's mkdocs.yml : theme : palette : primary : indigo accent : indigo If the colors are set with these configuration options, an additional CSS file that includes the hues of the color palette is automatically included and linked from the template. Custom colors with CSS variables Material for MkDocs defines all colors as CSS variables. If you want to customize the colors beyond the palette (e.g. to use your brand's colors), you can add an additional stylesheet and override the defaults: : root { /* Default color shades */ -- md-default-fg-color : ... ; -- md-default-fg-color--light : ... ; -- md-default-fg-color--lighter : ... ; -- md-default-fg-color--lightest : ... ; -- md-default-bg-color : ... ; -- md-default-bg-color--light : ... ; -- md-default-bg-color--lighter : ... ; -- md-default-bg-color--lightest : ... ; /* Primary color shades */ -- md-primary-fg-color : ... ; -- md-primary-fg-color--light : ... ; -- md-primary-fg-color--dark : ... ; -- md-primary-bg-color : ... ; -- md-primary-bg-color--light : ... ; /* Accent color shades */ -- md-accent-fg-color : ... ; -- md-accent-fg-color--transparent : ... ; -- md-accent-bg-color : ... ; -- md-accent-bg-color--light : ... ; /* Code block color shades */ -- md-code-bg-color : ... ; -- md-code-fg-color : ... ; } Primary color \u00b6 Default: indigo Click on a color name to change the primary color of the theme: .md-typeset button[data-md-color-primary] { cursor: pointer; transition: opacity 250ms; } .md-typeset button[data-md-color-primary]:hover { opacity: 0.75; } .md-typeset button[data-md-color-primary] > code { display: block; color: var(--md-primary-bg-color); background-color: var(--md-primary-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { var attr = \"data-md-color-primary\" button.addEventListener(\"click\", function() { document.body.setAttribute(attr, this.getAttribute(attr)) }) }) Accent color \u00b6 Default: indigo Click on a color name to change the accent color of the theme: .md-typeset button[data-md-color-accent] { cursor: pointer; transition: opacity 250ms; } .md-typeset button[data-md-color-accent]:hover { opacity: 0.75; } .md-typeset button[data-md-color-accent] > code { display: block; color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { var attr = \"data-md-color-accent\" button.addEventListener(\"click\", function() { document.body.setAttribute(attr, this.getAttribute(attr)) }) }) Fonts \u00b6 Default: Roboto and Roboto Mono The Roboto font family is the default font included with the theme, specifically the regular sans-serif type for text and the monospaced type for code. Both fonts are loaded from Google Fonts and can be changed to any valid webfont, like for example the Ubuntu font family : theme : font : text : Ubuntu code : Ubuntu Mono The text font will be loaded in weights 400 and 700 , the monospaced font in regular weight. If you want to load fonts from other destinations or don't want to use Google Fonts for data privacy reasons, just set font to false : theme : font : false Icons \u00b6 Default: material/library and fontawesome/brands/git-alt Material for MkDocs uses icons in several places. Currently, the following icons can be changed from mkdocs.yml : the logo icon, the repository icon and the social link icons . While the social link icons are tied to the respective entries, the other icons can be changed by referencing a valid path (without the trailing .svg ) relative to the .icons folder which comes with the theme: theme : icon : logo : material/library repo : fontawesome/brands/git-alt All icons are directly inlined as *.svg files, so no further requests will be made. Icon sets which are bundled with Material for MkDocs: Material Design icons ( material ): 5.1k icons FontAwesome icons ( fontawesome ): 1.6k icons GitHub's Octicons ( octicons ): 200 icons You can use all those icons directly from Markdown ! Logo \u00b6 Default: icon set through theme.icon.logo If you want to replace the icon in the header (screen) and drawer (mobile) with your brand's logo, you can place an image file in your docs folder and use the following option in mkdocs.yml : theme : logo : images/logo.svg Ideally, the image should be a square with a minimum resolution of 96x96, leave some room towards the edges and be composed of high contrast areas on a transparent ground, as it will be placed on the colored header and drawer. Favicon \u00b6 Default: assets/images/favicon.png The default favicon can be changed with: theme : favicon : images/favicon.png Extras \u00b6 Adding a source repository \u00b6 To include a link to the repository of your project within your documentation, set the following variables via your project's mkdocs.yml : repo_name : squidfunk/mkdocs-material repo_url : https://github.com/squidfunk/mkdocs-material The name of the repository will be rendered next to the search bar on big screens and as part of the main navigation drawer on smaller screen sizes. Additionally, for GitHub and GitLab, the number of stars and forks is shown. Note that the repository icon can be explicitly set through theme.icon.repo . Why is there an edit button at the top of every article? If the repo_url is set to a GitHub or BitBucket repository, and the repo_name is set to GitHub or BitBucket (implied by default), an edit button will appear at the top of every article. This is the automatic behavior that MkDocs implements. See the MkDocs documentation on more guidance regarding the edit_uri attribute, which defines whether the edit button is shown or not. Adding social links \u00b6 Social accounts can be linked in the footer of the documentation using the icons which are bundled with the theme. Note that each icon must point to a valid path (without the trailing .svg ) relative to the .icons folder which comes with the theme: extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk - icon : fontawesome/brands/twitter link : https://twitter.com/squidfunk - icon : fontawesome/brands/linkedin link : https://linkedin.com/in/squidfunk By default, the link title will be set to the domain name, e.g. github.com . If you want to set a discernable name, e.g., to improve your Lighthouse score, you can set the name attribute on each social link. Adding a Web App Manifest \u00b6 A Web App Manifest is a simple JSON file that tells the browser about your web application and how it should behave when installed on the user's mobile device or desktop. You can specify such a manifest in mkdocs.yml : extra : manifest : manifest.webmanifest Integrations \u00b6 Google Analytics \u00b6 MkDocs makes it easy to integrate site tracking with Google Analytics. To enable tracking, which is disabled by default, you must add your tracking identifier to mkdocs.yml : google_analytics : - UA-XXXXXXXX-X - auto Besides basic page views, site search can also be tracked to better understand how people use your documentation and what they expect to find. To enable search tracking: Go to your Google Analytics admin settings Select the property for the respective tracking code Go to the view settings tab. Scroll down and enable site search settings Set the query parameter to q . Disqus \u00b6 Material for MkDocs is integrated with Disqus , so if you want to add a comments section to your documentation set the shortname of your Disqus project in mkdocs.yml : extra : disqus : your-shortname The comments section is inserted on every page, except the index page . The necessary JavaScript is automatically included. Requirements Note that site_url must be set in mkdocs.yml for the Disqus integration to load properly. Disqus can also be enabled or disabled for specific pages using Metadata . Extensions \u00b6 Markdown comes with several very useful extensions, the following of which are not enabled by default but highly recommended, so enabling them should definitely be a good idea: markdown_extensions : - admonition - codehilite : guess_lang : false - toc : permalink : true See the following list of extensions supported by Material for MkDocs including some more information on configuration and usage: Admonition Codehilite Footnotes Metadata Permalinks PyMdown Extensions Plugins \u00b6 MkDocs' plugin architecture makes it possible to add pre- or post-processing steps that sit between the theme and your documentation. For more information, see the following list of plugins tested and supported by Material for MkDocs including more information regarding installation and usage: Search (enabled by default) Minification Revision date Awesome pages For further reference, the MkDocs wiki contains a list of all available plugins .","title":"Getting started"},{"location":"projects/mkdocs/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"projects/mkdocs/getting-started/#installation","text":"While there are several ways of installing Material for MkDocs, the recommended methods are either by using pip \u2013 the Python package manager \u2013 or by pulling the official Docker image .","title":"Installation"},{"location":"projects/mkdocs/getting-started/#with-pip-recommended","text":"Material for MkDocs can be installed with pip : pip install mkdocs-material Note that this will automatically install compatible versions of MkDocs , Markdown , Pygments and PyMdown Extensions . Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately. Installation in a virtual environment The best way to make sure that you end up with the correct versions and without any incompatibility problems between packages it to use a virtual environment . Don't know what this is or how to set it up? We recommend to start by reading a tutorial on virtual environments for Python. Installation on macOS When you're running the pre-installed version of Python on macOS, pip tries to install packages in a folder for which your user might not have the adequate permissions. There are two possible solutions for this: Installing in user space (recommended): Provide the --user flag to the install command and pip will install the package in a user-site location. This is the recommended way. Switching to a homebrewed Python : Upgrade your Python installation to a self-contained solution by installing Python with Homebrew. This should eliminate a lot of problems you could be having with pip . Error: unrecognized theme 'material' If you run into this error, the most common reason is that you installed MkDocs through some package manager (e.g. Homebrew or apt-get ) and Material for MkDocs through pip , so both packages end up in different locations. MkDocs only checks its install location for themes.","title":"with pip recommended"},{"location":"projects/mkdocs/getting-started/#with-docker-recommended","text":"The official Docker image is a great way to get up and running in a few minutes, as it comes with all dependencies pre-installed. Pull the image for the latest version with: docker pull squidfunk/mkdocs-material The mkdocs executable is provided as an entry point and serve is the default command. Start the development server in your project root \u2013 the folder where mkdocs.yml resides \u2014 with: Unix docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material Windows docker run --rm -it -p 8000:8000 -v \"%cd%\":/docs squidfunk/mkdocs-material","title":"with docker recommended"},{"location":"projects/mkdocs/getting-started/#with-git","text":"Material for MkDocs can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/squidfunk/mkdocs-material.git The theme will reside in the folder mkdocs-material/material .","title":"with git"},{"location":"projects/mkdocs/getting-started/#configuration","text":"Depending on your installation method, you can now add the following lines to mkdocs.yml in your project root. If you installed Material for MkDocs using a package manager, add: theme : name : material If you cloned Material for MkDocs from GitHub add: theme : name : null custom_dir : mkdocs-material/material MkDocs includes a development server, so you can preview your changes as you write your documentation. The development server can be started with the following command: mkdocs serve Point your browser to http://localhost:8000 and your documentation should greet you in a new look. If you're starting from scratch, the following configuration can be used as a starting point: Example configuration This is an excerpt from the mkdocs.yml used to render these pages: # Project information site_name : Material for MkDocs site_description : A Material Design theme for MkDocs site_author : Martin Donath site_url : https://squidfunk.github.io/mkdocs-material/ # Repository repo_name : squidfunk/mkdocs-material repo_url : https://github.com/squidfunk/mkdocs-material # Copyright copyright : Copyright &copy; 2016 - 2020 Martin Donath # Configuration theme : name : material language : en palette : primary : indigo accent : indigo font : text : Roboto code : Roboto Mono # Extras extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk - icon : fontawesome/brands/twitter link : https://twitter.com/squidfunk - icon : fontawesome/brands/linkedin link : https://linkedin.com/in/squidfunk # Google Analytics google_analytics : - UA-XXXXXXXX-X - auto # Extensions markdown_extensions : - admonition - codehilite : guess_lang : false - toc : permalink : true","title":"Configuration"},{"location":"projects/mkdocs/getting-started/#feature-flags","text":"These optional features are hidden behind flags and can be explicitly enabled in mkdocs.yml .","title":"Feature flags"},{"location":"projects/mkdocs/getting-started/#instant-loading","text":"The (still experimental) instant loading feature will intercept clicks on all internal links and dispatch them directly via XHR without a full page reload. It can be enabled from mkdocs.yml with: theme : features : - instant The resulting page is parsed and injected and all event handlers and components are automatically rebound. This means that Material for MkDocs behaves like a Single Page Application , which is especially useful for large documentation sites that come with a huge search index, as the search index will now remain intact in-between document switches.","title":"Instant loading"},{"location":"projects/mkdocs/getting-started/#tabs","text":"The tabs feature will render top-level subsections in another navigational layer below the header on big screens (but leave them untouched on mobile). It can be enabled from mkdocs.yml with: theme : features : - tabs Note that all top-level pages (i.e. all top-level entries that directly refer to an *.md file) defined inside the nav entry of mkdocs.yml will be grouped under the first tab which will receive the title of the first page. This means that there will effectively be no collapsible subsections for the first tab, as each subsection is rendered as another tab. If you want more fine-grained control, i.e., collapsible subsections for the first tab, you can move all top-level pages into a subsection , so that the top-level is entirely made up of subsections. Note that tabs are only shown for larger screens, so make sure that navigation is plausible on mobile devices. As an example, see the mkdocs.yml used to render these pages.","title":"Tabs"},{"location":"projects/mkdocs/getting-started/#language","text":"Default: en Material for MkDocs supports internationalization (i18n) and provides translations for all template variables and labels. You can set the language from mkdocs.yml with: theme : language : en The following language codes are supported: .md-language-list { -webkit-columns: 2; -moz-columns: 2; columns: 2; } .md-language-list li { -webkit-column-break-inside: avoid; page-break-inside: avoid; break-inside: avoid; } af / Afrikaans ar / Arabic ca / Catalan zh / Chinese (Simplified) zh-Hant / Chinese (Traditional) zh-TW / Chinese (Taiwanese) hr / Croatian cs / Czech da / Danish nl / Dutch en / English et / Estonian fi / Finnish fr / French gl / Galician de / German gr / Greek he / Hebrew hi / Hindi hu / Hungarian id / Indonesian it / Italian ja / Japanese kr / Korean no / Norwegian nn / Norwegian (Nynorsk) fa / Persian pl / Polish pt / Portuguese ro / Romanian ru / Russian sr / Serbian sh / Serbo-Croatian sk / Slovak si / Slovenian es / Spanish sv / Swedish th / Thai tr / Turkish uk / Ukrainian vi / Vietnamese Add language While many languages are read ltr (left-to-right), Material for MkDocs also supports rtl (right-to-left) directionality which is inferred from the selected language, but can also be set with: theme : direction : rtl","title":"Language"},{"location":"projects/mkdocs/getting-started/#color-palette","text":"The Material Design color palette comes with 20 hues, all of which are included with Material for MkDocs. Primary and accent colors can be set from the project root's mkdocs.yml : theme : palette : primary : indigo accent : indigo If the colors are set with these configuration options, an additional CSS file that includes the hues of the color palette is automatically included and linked from the template. Custom colors with CSS variables Material for MkDocs defines all colors as CSS variables. If you want to customize the colors beyond the palette (e.g. to use your brand's colors), you can add an additional stylesheet and override the defaults: : root { /* Default color shades */ -- md-default-fg-color : ... ; -- md-default-fg-color--light : ... ; -- md-default-fg-color--lighter : ... ; -- md-default-fg-color--lightest : ... ; -- md-default-bg-color : ... ; -- md-default-bg-color--light : ... ; -- md-default-bg-color--lighter : ... ; -- md-default-bg-color--lightest : ... ; /* Primary color shades */ -- md-primary-fg-color : ... ; -- md-primary-fg-color--light : ... ; -- md-primary-fg-color--dark : ... ; -- md-primary-bg-color : ... ; -- md-primary-bg-color--light : ... ; /* Accent color shades */ -- md-accent-fg-color : ... ; -- md-accent-fg-color--transparent : ... ; -- md-accent-bg-color : ... ; -- md-accent-bg-color--light : ... ; /* Code block color shades */ -- md-code-bg-color : ... ; -- md-code-fg-color : ... ; }","title":"Color palette"},{"location":"projects/mkdocs/getting-started/#primary-color","text":"Default: indigo Click on a color name to change the primary color of the theme: .md-typeset button[data-md-color-primary] { cursor: pointer; transition: opacity 250ms; } .md-typeset button[data-md-color-primary]:hover { opacity: 0.75; } .md-typeset button[data-md-color-primary] > code { display: block; color: var(--md-primary-bg-color); background-color: var(--md-primary-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { var attr = \"data-md-color-primary\" button.addEventListener(\"click\", function() { document.body.setAttribute(attr, this.getAttribute(attr)) }) })","title":"Primary color"},{"location":"projects/mkdocs/getting-started/#accent-color","text":"Default: indigo Click on a color name to change the accent color of the theme: .md-typeset button[data-md-color-accent] { cursor: pointer; transition: opacity 250ms; } .md-typeset button[data-md-color-accent]:hover { opacity: 0.75; } .md-typeset button[data-md-color-accent] > code { display: block; color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { var attr = \"data-md-color-accent\" button.addEventListener(\"click\", function() { document.body.setAttribute(attr, this.getAttribute(attr)) }) })","title":"Accent color"},{"location":"projects/mkdocs/getting-started/#fonts","text":"Default: Roboto and Roboto Mono The Roboto font family is the default font included with the theme, specifically the regular sans-serif type for text and the monospaced type for code. Both fonts are loaded from Google Fonts and can be changed to any valid webfont, like for example the Ubuntu font family : theme : font : text : Ubuntu code : Ubuntu Mono The text font will be loaded in weights 400 and 700 , the monospaced font in regular weight. If you want to load fonts from other destinations or don't want to use Google Fonts for data privacy reasons, just set font to false : theme : font : false","title":"Fonts"},{"location":"projects/mkdocs/getting-started/#icons","text":"Default: material/library and fontawesome/brands/git-alt Material for MkDocs uses icons in several places. Currently, the following icons can be changed from mkdocs.yml : the logo icon, the repository icon and the social link icons . While the social link icons are tied to the respective entries, the other icons can be changed by referencing a valid path (without the trailing .svg ) relative to the .icons folder which comes with the theme: theme : icon : logo : material/library repo : fontawesome/brands/git-alt All icons are directly inlined as *.svg files, so no further requests will be made. Icon sets which are bundled with Material for MkDocs: Material Design icons ( material ): 5.1k icons FontAwesome icons ( fontawesome ): 1.6k icons GitHub's Octicons ( octicons ): 200 icons You can use all those icons directly from Markdown !","title":"Icons"},{"location":"projects/mkdocs/getting-started/#logo","text":"Default: icon set through theme.icon.logo If you want to replace the icon in the header (screen) and drawer (mobile) with your brand's logo, you can place an image file in your docs folder and use the following option in mkdocs.yml : theme : logo : images/logo.svg Ideally, the image should be a square with a minimum resolution of 96x96, leave some room towards the edges and be composed of high contrast areas on a transparent ground, as it will be placed on the colored header and drawer.","title":"Logo"},{"location":"projects/mkdocs/getting-started/#favicon","text":"Default: assets/images/favicon.png The default favicon can be changed with: theme : favicon : images/favicon.png","title":"Favicon"},{"location":"projects/mkdocs/getting-started/#extras","text":"","title":"Extras"},{"location":"projects/mkdocs/getting-started/#adding-a-source-repository","text":"To include a link to the repository of your project within your documentation, set the following variables via your project's mkdocs.yml : repo_name : squidfunk/mkdocs-material repo_url : https://github.com/squidfunk/mkdocs-material The name of the repository will be rendered next to the search bar on big screens and as part of the main navigation drawer on smaller screen sizes. Additionally, for GitHub and GitLab, the number of stars and forks is shown. Note that the repository icon can be explicitly set through theme.icon.repo . Why is there an edit button at the top of every article? If the repo_url is set to a GitHub or BitBucket repository, and the repo_name is set to GitHub or BitBucket (implied by default), an edit button will appear at the top of every article. This is the automatic behavior that MkDocs implements. See the MkDocs documentation on more guidance regarding the edit_uri attribute, which defines whether the edit button is shown or not.","title":"Adding a source repository"},{"location":"projects/mkdocs/getting-started/#adding-social-links","text":"Social accounts can be linked in the footer of the documentation using the icons which are bundled with the theme. Note that each icon must point to a valid path (without the trailing .svg ) relative to the .icons folder which comes with the theme: extra : social : - icon : fontawesome/brands/github-alt link : https://github.com/squidfunk - icon : fontawesome/brands/twitter link : https://twitter.com/squidfunk - icon : fontawesome/brands/linkedin link : https://linkedin.com/in/squidfunk By default, the link title will be set to the domain name, e.g. github.com . If you want to set a discernable name, e.g., to improve your Lighthouse score, you can set the name attribute on each social link.","title":"Adding social links"},{"location":"projects/mkdocs/getting-started/#adding-a-web-app-manifest","text":"A Web App Manifest is a simple JSON file that tells the browser about your web application and how it should behave when installed on the user's mobile device or desktop. You can specify such a manifest in mkdocs.yml : extra : manifest : manifest.webmanifest","title":"Adding a Web App Manifest"},{"location":"projects/mkdocs/getting-started/#integrations","text":"","title":"Integrations"},{"location":"projects/mkdocs/getting-started/#google-analytics","text":"MkDocs makes it easy to integrate site tracking with Google Analytics. To enable tracking, which is disabled by default, you must add your tracking identifier to mkdocs.yml : google_analytics : - UA-XXXXXXXX-X - auto Besides basic page views, site search can also be tracked to better understand how people use your documentation and what they expect to find. To enable search tracking: Go to your Google Analytics admin settings Select the property for the respective tracking code Go to the view settings tab. Scroll down and enable site search settings Set the query parameter to q .","title":"Google Analytics"},{"location":"projects/mkdocs/getting-started/#disqus","text":"Material for MkDocs is integrated with Disqus , so if you want to add a comments section to your documentation set the shortname of your Disqus project in mkdocs.yml : extra : disqus : your-shortname The comments section is inserted on every page, except the index page . The necessary JavaScript is automatically included. Requirements Note that site_url must be set in mkdocs.yml for the Disqus integration to load properly. Disqus can also be enabled or disabled for specific pages using Metadata .","title":"Disqus"},{"location":"projects/mkdocs/getting-started/#extensions","text":"Markdown comes with several very useful extensions, the following of which are not enabled by default but highly recommended, so enabling them should definitely be a good idea: markdown_extensions : - admonition - codehilite : guess_lang : false - toc : permalink : true See the following list of extensions supported by Material for MkDocs including some more information on configuration and usage: Admonition Codehilite Footnotes Metadata Permalinks PyMdown Extensions","title":"Extensions"},{"location":"projects/mkdocs/getting-started/#plugins","text":"MkDocs' plugin architecture makes it possible to add pre- or post-processing steps that sit between the theme and your documentation. For more information, see the following list of plugins tested and supported by Material for MkDocs including more information regarding installation and usage: Search (enabled by default) Minification Revision date Awesome pages For further reference, the MkDocs wiki contains a list of all available plugins .","title":"Plugins"},{"location":"projects/mkdocs/install_mkdocs/","text":"Install MKdocs \u00b6 Step-by-step MKdocs \u00b6 Id officia fugiat amet elit excepteur eu dolore sit veniam aute excepteur ea laborum id. Laboris non ad elit sint sit non ullamco adipisicing ullamco. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Step 1 - install MKDocs \u00b6 Exercitation culpa occaecat dolore sunt consectetur esse voluptate ipsum eu occaecat Lorem eiusmod cillum. Anim sint ad mollit sit nulla. Ea ea irure est officia cupidatat enim sint aliquip aute est minim amet. Nostrud cupidatat enim est nostrud elit laboris quis aliquip. Dolor non cillum consequat nulla aute ut laboris pariatur occaecat veniam elit aliquip enim aliqua. Proident occaecat exercitation labore mollit amet sunt fugiat deserunt anim id ipsum. pip install mkdocs Step 2 - go to your folder \u00b6 Tempor officia mollit commodo laborum ipsum aliqua nostrud ex. Consequat consequat id ex sit sint nostrud commodo proident reprehenderit adipisicing aute. Labore sunt magna mollit elit reprehenderit ea. Consectetur ipsum esse enim ut tempor ad id. Proident consectetur Lorem aliqua eu. cd go/to/your/folder Step 3 - create folder \u00b6 Consectetur veniam elit ullamco sit deserunt nulla Lorem ex pariatur enim. Irure aute tempor voluptate dolore incididunt ut. Veniam dolor aute nostrud Lorem ut. Excepteur mollit ullamco officia adipisicing est qui ad. Non sint irure quis excepteur cupidatat voluptate duis fugiat quis. Voluptate pariatur dolore sint nulla ullamco Lorem aliquip aliquip qui Lorem excepteur. mkdocs new meu-projeto Step 4 - go to your created folder \u00b6 Laborum duis Lorem ad ad culpa. Reprehenderit adipisicing tempor aliqua pariatur. Nisi ad labore incididunt sit anim non exercitation in eiusmod. Eu quis ex reprehenderit duis in ad sit consequat non sit do aliquip nulla. cd go/to/your/created/folder Step 5 - Create server \u00b6 Eiusmod duis ut culpa sint aliqua cillum deserunt incididunt ea. Consectetur velit magna ad occaecat ut incididunt in pariatur cillum id amet. In duis Lorem exercitation qui eu laborum nisi est sint minim duis eiusmod. Anim cupidatat Lorem nisi est. Consectetur reprehenderit reprehenderit elit nulla et voluptate reprehenderit consectetur minim. Nostrud excepteur proident aute eu dolor. mkdocs serve Step 6 - Install Layout mkdocs-material \u00b6 Eiusmod officia ullamco est sit ullamco consectetur minim dolore labore amet. Labore et occaecat non est duis sit ipsum cillum. Duis non ex esse in consequat ex do et aute dolor ad officia sit. pip install mkdocs-material Step 7 - Initialize Git \u00b6 Exercitation minim ipsum in sint duis sint fugiat exercitation dolor sint tempor cupidatat sint. Culpa irure incididunt non do reprehenderit duis ea nisi labore minim minim ex duis cillum. Eiusmod anim labore et incididunt magna labore ut cupidatat excepteur amet officia voluptate non. Labore tempor excepteur id irure quis ea laborum dolore esse nostrud veniam sunt consectetur. Anim sunt non pariatur incididunt dolore excepteur culpa dolore anim excepteur dolor labore laboris dolore. Lorem Lorem aute Lorem veniam exercitation. git init Step 8 - Add remote to Git \u00b6 Deserunt minim duis id eu reprehenderit qui eiusmod. Aliqua aliquip aliquip dolor excepteur dolore sint sit quis irure dolore veniam tempor elit. Commodo dolore dolore dolor ipsum. Cupidatat voluptate mollit nostrud voluptate laboris deserunt ex reprehenderit dolore esse ut mollit. Aliqua consectetur sit mollit commodo aliquip voluptate. Labore id id non dolor fugiat commodo velit laboris pariatur tempor consequat incididunt. Consequat dolor est fugiat minim est adipisicing dolor culpa excepteur aliquip dolore. git remote add origin https://github.com/erickmattoso/braskem_playbook Step 9 - Construct website \u00b6 Aliqua cupidatat elit ipsum proident id aliquip proident ea fugiat. Cillum labore est eiusmod enim laboris ad. Proident anim adipisicing tempor exercitation labore nostrud ea. Amet cupidatat enim dolore minim ex est reprehenderit consequat officia incididunt non. mkdocs gh-deploy Updating Website \u00b6 Step 1 - Go to your folder \u00b6 cd go/to/your/folder Step 2 - Open conection with the local host \u00b6 mkdocs serve Step 2 - remote add \u00b6 git remote add origin https://github.com/erickmattoso/braskem_playbook Step 4 - Construct website \u00b6 mkdocs gh-deploy That is it. Thanks!","title":"Install MKdocs"},{"location":"projects/mkdocs/install_mkdocs/#install-mkdocs","text":"","title":"Install MKdocs"},{"location":"projects/mkdocs/install_mkdocs/#step-by-step-mkdocs","text":"Id officia fugiat amet elit excepteur eu dolore sit veniam aute excepteur ea laborum id. Laboris non ad elit sint sit non ullamco adipisicing ullamco. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Step-by-step MKdocs"},{"location":"projects/mkdocs/install_mkdocs/#step-1-install-mkdocs","text":"Exercitation culpa occaecat dolore sunt consectetur esse voluptate ipsum eu occaecat Lorem eiusmod cillum. Anim sint ad mollit sit nulla. Ea ea irure est officia cupidatat enim sint aliquip aute est minim amet. Nostrud cupidatat enim est nostrud elit laboris quis aliquip. Dolor non cillum consequat nulla aute ut laboris pariatur occaecat veniam elit aliquip enim aliqua. Proident occaecat exercitation labore mollit amet sunt fugiat deserunt anim id ipsum. pip install mkdocs","title":"Step 1 - install MKDocs"},{"location":"projects/mkdocs/install_mkdocs/#step-2-go-to-your-folder","text":"Tempor officia mollit commodo laborum ipsum aliqua nostrud ex. Consequat consequat id ex sit sint nostrud commodo proident reprehenderit adipisicing aute. Labore sunt magna mollit elit reprehenderit ea. Consectetur ipsum esse enim ut tempor ad id. Proident consectetur Lorem aliqua eu. cd go/to/your/folder","title":"Step 2 - go to your folder"},{"location":"projects/mkdocs/install_mkdocs/#step-3-create-folder","text":"Consectetur veniam elit ullamco sit deserunt nulla Lorem ex pariatur enim. Irure aute tempor voluptate dolore incididunt ut. Veniam dolor aute nostrud Lorem ut. Excepteur mollit ullamco officia adipisicing est qui ad. Non sint irure quis excepteur cupidatat voluptate duis fugiat quis. Voluptate pariatur dolore sint nulla ullamco Lorem aliquip aliquip qui Lorem excepteur. mkdocs new meu-projeto","title":"Step 3 - create folder"},{"location":"projects/mkdocs/install_mkdocs/#step-4-go-to-your-created-folder","text":"Laborum duis Lorem ad ad culpa. Reprehenderit adipisicing tempor aliqua pariatur. Nisi ad labore incididunt sit anim non exercitation in eiusmod. Eu quis ex reprehenderit duis in ad sit consequat non sit do aliquip nulla. cd go/to/your/created/folder","title":"Step 4 - go to your created folder"},{"location":"projects/mkdocs/install_mkdocs/#step-5-create-server","text":"Eiusmod duis ut culpa sint aliqua cillum deserunt incididunt ea. Consectetur velit magna ad occaecat ut incididunt in pariatur cillum id amet. In duis Lorem exercitation qui eu laborum nisi est sint minim duis eiusmod. Anim cupidatat Lorem nisi est. Consectetur reprehenderit reprehenderit elit nulla et voluptate reprehenderit consectetur minim. Nostrud excepteur proident aute eu dolor. mkdocs serve","title":"Step 5 - Create server"},{"location":"projects/mkdocs/install_mkdocs/#step-6-install-layout-mkdocs-material","text":"Eiusmod officia ullamco est sit ullamco consectetur minim dolore labore amet. Labore et occaecat non est duis sit ipsum cillum. Duis non ex esse in consequat ex do et aute dolor ad officia sit. pip install mkdocs-material","title":"Step 6 - Install Layout mkdocs-material"},{"location":"projects/mkdocs/install_mkdocs/#step-7-initialize-git","text":"Exercitation minim ipsum in sint duis sint fugiat exercitation dolor sint tempor cupidatat sint. Culpa irure incididunt non do reprehenderit duis ea nisi labore minim minim ex duis cillum. Eiusmod anim labore et incididunt magna labore ut cupidatat excepteur amet officia voluptate non. Labore tempor excepteur id irure quis ea laborum dolore esse nostrud veniam sunt consectetur. Anim sunt non pariatur incididunt dolore excepteur culpa dolore anim excepteur dolor labore laboris dolore. Lorem Lorem aute Lorem veniam exercitation. git init","title":"Step 7 - Initialize Git"},{"location":"projects/mkdocs/install_mkdocs/#step-8-add-remote-to-git","text":"Deserunt minim duis id eu reprehenderit qui eiusmod. Aliqua aliquip aliquip dolor excepteur dolore sint sit quis irure dolore veniam tempor elit. Commodo dolore dolore dolor ipsum. Cupidatat voluptate mollit nostrud voluptate laboris deserunt ex reprehenderit dolore esse ut mollit. Aliqua consectetur sit mollit commodo aliquip voluptate. Labore id id non dolor fugiat commodo velit laboris pariatur tempor consequat incididunt. Consequat dolor est fugiat minim est adipisicing dolor culpa excepteur aliquip dolore. git remote add origin https://github.com/erickmattoso/braskem_playbook","title":"Step 8 - Add remote to Git"},{"location":"projects/mkdocs/install_mkdocs/#step-9-construct-website","text":"Aliqua cupidatat elit ipsum proident id aliquip proident ea fugiat. Cillum labore est eiusmod enim laboris ad. Proident anim adipisicing tempor exercitation labore nostrud ea. Amet cupidatat enim dolore minim ex est reprehenderit consequat officia incididunt non. mkdocs gh-deploy","title":"Step 9 - Construct website"},{"location":"projects/mkdocs/install_mkdocs/#updating-website","text":"","title":"Updating Website"},{"location":"projects/mkdocs/install_mkdocs/#step-1-go-to-your-folder","text":"cd go/to/your/folder","title":"Step 1 - Go to your folder"},{"location":"projects/mkdocs/install_mkdocs/#step-2-open-conection-with-the-local-host","text":"mkdocs serve","title":"Step 2 - Open conection with the local host"},{"location":"projects/mkdocs/install_mkdocs/#step-2-remote-add","text":"git remote add origin https://github.com/erickmattoso/braskem_playbook","title":"Step 2 - remote add"},{"location":"projects/mkdocs/install_mkdocs/#step-4-construct-website","text":"mkdocs gh-deploy That is it. Thanks!","title":"Step 4 - Construct website"},{"location":"projects/mkdocs/license/","text":"License \u00b6 MIT License Copyright \u00a9 2016 - 2020 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"projects/mkdocs/license/#license","text":"MIT License Copyright \u00a9 2016 - 2020 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"projects/mkdocs/metadata/","text":"Metadata \u00b6 Metadata is an extension included in the standard Markdown library that makes it possible to control certain properties in a page-specific context, e.g. the page title or description. Configuration \u00b6 Add the following lines to mkdocs.yml : markdown_extensions : - meta Usage \u00b6 Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: --- title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js --- # Headline ... See the next section which covers the supported metadata. Setting a hero \u00b6 Material for MkDocs exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Set heroes with metadata Linking sources \u00b6 When a document is related to a specific source file and the repo_url is defined inside the project's mkdocs.yml , the file can be linked using the source key: source: file.js The filename is appended to the repo_url set in mkdocs.yml , but can be prefixed with a path to ensure correct path resolving. The name of the source file is shown in the tooltip. Example: path: tree/master/docs/extensions source: metadata.md Redirecting to another page \u00b6 It's sometimes necessary to move documents around in the navigation tree and redirect users from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url . Overrides \u00b6 Page title \u00b6 The page title can be overridden on a per-document basis: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title. Page description \u00b6 The page description can also be overridden on a per-document basis: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value. Disqus \u00b6 As described in the getting started guide , Disqus can be enabled on a per-document basis: disqus: your-shortname Disqus can also be disabled for a specific page by setting it to an empty value: disqus:","title":"Metadata"},{"location":"projects/mkdocs/metadata/#metadata","text":"Metadata is an extension included in the standard Markdown library that makes it possible to control certain properties in a page-specific context, e.g. the page title or description.","title":"Metadata"},{"location":"projects/mkdocs/metadata/#configuration","text":"Add the following lines to mkdocs.yml : markdown_extensions : - meta","title":"Configuration"},{"location":"projects/mkdocs/metadata/#usage","text":"Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: --- title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js --- # Headline ... See the next section which covers the supported metadata.","title":"Usage"},{"location":"projects/mkdocs/metadata/#setting-a-hero","text":"Material for MkDocs exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Set heroes with metadata","title":"Setting a hero"},{"location":"projects/mkdocs/metadata/#linking-sources","text":"When a document is related to a specific source file and the repo_url is defined inside the project's mkdocs.yml , the file can be linked using the source key: source: file.js The filename is appended to the repo_url set in mkdocs.yml , but can be prefixed with a path to ensure correct path resolving. The name of the source file is shown in the tooltip. Example: path: tree/master/docs/extensions source: metadata.md","title":"Linking sources"},{"location":"projects/mkdocs/metadata/#redirecting-to-another-page","text":"It's sometimes necessary to move documents around in the navigation tree and redirect users from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url .","title":"Redirecting to another page"},{"location":"projects/mkdocs/metadata/#overrides","text":"","title":"Overrides"},{"location":"projects/mkdocs/metadata/#page-title","text":"The page title can be overridden on a per-document basis: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title.","title":"Page title"},{"location":"projects/mkdocs/metadata/#page-description","text":"The page description can also be overridden on a per-document basis: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value.","title":"Page description"},{"location":"projects/mkdocs/metadata/#disqus","text":"As described in the getting started guide , Disqus can be enabled on a per-document basis: disqus: your-shortname Disqus can also be disabled for a specific page by setting it to an empty value: disqus:","title":"Disqus"},{"location":"projects/mkdocs/minification/","text":"Minification \u00b6 The mkdocs-minify-plugin will minify all *.html files generated by mkdocs build in a post-processing step, stripping all unnecessary characters to reduce the payload served to the client. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker. Installation \u00b6 Install the plugin using pip : pip install mkdocs-minify-plugin Configuration \u00b6 Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - minify : minify_html : true Usage \u00b6 When enabled, all *.html will be minified automatically.","title":"Minification"},{"location":"projects/mkdocs/minification/#minification","text":"The mkdocs-minify-plugin will minify all *.html files generated by mkdocs build in a post-processing step, stripping all unnecessary characters to reduce the payload served to the client. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker.","title":"Minification"},{"location":"projects/mkdocs/minification/#installation","text":"Install the plugin using pip : pip install mkdocs-minify-plugin","title":"Installation"},{"location":"projects/mkdocs/minification/#configuration","text":"Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - minify : minify_html : true","title":"Configuration"},{"location":"projects/mkdocs/minification/#usage","text":"When enabled, all *.html will be minified automatically.","title":"Usage"},{"location":"projects/mkdocs/permalinks/","text":"Permalinks \u00b6 Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a specific section of the document. Configuration \u00b6 Add the following lines to mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which Material for MkDocs will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions : - toc : permalink : Link Usage \u00b6 When enabled, permalinks are inserted automatically.","title":"Permalinks"},{"location":"projects/mkdocs/permalinks/#permalinks","text":"Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a specific section of the document.","title":"Permalinks"},{"location":"projects/mkdocs/permalinks/#configuration","text":"Add the following lines to mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which Material for MkDocs will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions : - toc : permalink : Link","title":"Configuration"},{"location":"projects/mkdocs/permalinks/#usage","text":"When enabled, permalinks are inserted automatically.","title":"Usage"},{"location":"projects/mkdocs/pymdown/","text":"PyMdown Extensions \u00b6 PyMdown Extensions is a collection of Markdown extensions that add some great missing features to the standard Markdown library. A compatible version is always included with the theme. Configuration \u00b6 The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with Material for MkDocs: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tabbed - pymdownx.tilde Usage \u00b6 Arithmatex MathJax \u00b6 Arithmatex integrates Material for MkDocs with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This can be done with additional JavaScript : extra_javascript : - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime which contains the MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; Then, add the following lines to mkdocs.yml : extra_javascript : - javascripts/extra.js - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML Blocks \u00b6 Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline \u00b6 Inline equations must be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} BetterEm \u00b6 BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes . Caret \u00b6 Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ . Critic \u00b6 Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Details \u00b6 Details adds collapsible Admonition blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes. Emoji \u00b6 Emoji adds the ability to insert, well, emojis! By default, Emoji uses JoyPixles' emoji under the former name EmojiOne. Recent versions of the extension lock support to an older version (2.2.7) due to JoyPixels' newer, less permissible licenses included in later releases. This restricts support to Unicode 9. To get the latest support for the current Unicode version, you can use Twemoji instead which has a much more permissible license. Simply override the default emoji index being used: markdown_extensions : - pymdownx.emoji : emoji_index : !!python/name:pymdownx.emoji.twemoji emoji_generator : !!python/name:pymdownx.emoji.to_svg To view all the available short names and emoji available, see Emoji's documentation on your chosen index which includes links to the files containing the short names and emoji associated with each supported index. Legal disclaimer Material has no affiliation with JoyPixles or Twemoji , both of which are licensed under CC BY 4.0 . When including images or CSS from either provider, please read their licenses to ensure proper attribution: EmojiOne or Twemoji . Icons \u00b6 In addition, you can embed the Material Design icons, Fontawesome icons and GitHub's Octicons directly from Markdown by using Material for MkDocs's custom emoji index . It extends the Twemoji index with new short names that access any of the included icons. To use the custom index, you need to use materialx.emoji instead of pymdownx.emoji : markdown_extensions : - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg Example: * :material-account-circle: \u2013 we can use Material Design icons * :fontawesome-regular-laugh-wink: \u2013 we can also use FontAwesome icons * :octicons-octoface: \u2013 that's not all, we can also use GitHub's Octicons Result: \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons InlineHilite \u00b6 InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be activated by prefixing inline code with a shebang and language identifier, e.g. #!js . MagicLink \u00b6 MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses. Mark \u00b6 Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== . SmartSymbols \u00b6 SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...). SuperFences \u00b6 SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs . Tabbed \u00b6 Tabbed adds support for creating tabbed groups of Markdown content. Example: === \"Fruit List\" - :apple: Apple - :banana: Banana - :kiwi: Kiwi === \"Fruit Table\" Fruit | Color --------------- | ----- :apple: Apple | Red :banana: Banana | Yellow :kiwi: Kiwi | Green Result: Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Tasklist \u00b6 Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Tilde \u00b6 Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"PyMdown Extensions"},{"location":"projects/mkdocs/pymdown/#pymdown-extensions","text":"PyMdown Extensions is a collection of Markdown extensions that add some great missing features to the standard Markdown library. A compatible version is always included with the theme.","title":"PyMdown Extensions"},{"location":"projects/mkdocs/pymdown/#configuration","text":"The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with Material for MkDocs: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tabbed - pymdownx.tilde","title":"Configuration"},{"location":"projects/mkdocs/pymdown/#usage","text":"","title":"Usage"},{"location":"projects/mkdocs/pymdown/#arithmatex-mathjax","text":"Arithmatex integrates Material for MkDocs with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This can be done with additional JavaScript : extra_javascript : - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime which contains the MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \"\\\\(\" , \"\\\\)\" ] ], displayMath : [ [ \"\\\\[\" , \"\\\\]\" ] ] }, TeX : { TagSide : \"right\" , TagIndent : \".8em\" , MultLineWidth : \"85%\" , equationNumbers : { autoNumber : \"AMS\" , }, unicode : { fonts : \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign : \"left\" , showProcessingMessages : false , messageStyle : \"none\" }; Then, add the following lines to mkdocs.yml : extra_javascript : - javascripts/extra.js - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML","title":"Arithmatex MathJax"},{"location":"projects/mkdocs/pymdown/#blocks","text":"Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Blocks"},{"location":"projects/mkdocs/pymdown/#inline","text":"Inline equations must be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline"},{"location":"projects/mkdocs/pymdown/#betterem","text":"BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes .","title":"BetterEm"},{"location":"projects/mkdocs/pymdown/#caret","text":"Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ .","title":"Caret"},{"location":"projects/mkdocs/pymdown/#critic","text":"Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Critic"},{"location":"projects/mkdocs/pymdown/#details","text":"Details adds collapsible Admonition blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes.","title":"Details"},{"location":"projects/mkdocs/pymdown/#emoji","text":"Emoji adds the ability to insert, well, emojis! By default, Emoji uses JoyPixles' emoji under the former name EmojiOne. Recent versions of the extension lock support to an older version (2.2.7) due to JoyPixels' newer, less permissible licenses included in later releases. This restricts support to Unicode 9. To get the latest support for the current Unicode version, you can use Twemoji instead which has a much more permissible license. Simply override the default emoji index being used: markdown_extensions : - pymdownx.emoji : emoji_index : !!python/name:pymdownx.emoji.twemoji emoji_generator : !!python/name:pymdownx.emoji.to_svg To view all the available short names and emoji available, see Emoji's documentation on your chosen index which includes links to the files containing the short names and emoji associated with each supported index. Legal disclaimer Material has no affiliation with JoyPixles or Twemoji , both of which are licensed under CC BY 4.0 . When including images or CSS from either provider, please read their licenses to ensure proper attribution: EmojiOne or Twemoji .","title":"Emoji"},{"location":"projects/mkdocs/pymdown/#icons","text":"In addition, you can embed the Material Design icons, Fontawesome icons and GitHub's Octicons directly from Markdown by using Material for MkDocs's custom emoji index . It extends the Twemoji index with new short names that access any of the included icons. To use the custom index, you need to use materialx.emoji instead of pymdownx.emoji : markdown_extensions : - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg Example: * :material-account-circle: \u2013 we can use Material Design icons * :fontawesome-regular-laugh-wink: \u2013 we can also use FontAwesome icons * :octicons-octoface: \u2013 that's not all, we can also use GitHub's Octicons Result: \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons","title":"Icons"},{"location":"projects/mkdocs/pymdown/#inlinehilite","text":"InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be activated by prefixing inline code with a shebang and language identifier, e.g. #!js .","title":"InlineHilite"},{"location":"projects/mkdocs/pymdown/#magiclink","text":"MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses.","title":"MagicLink"},{"location":"projects/mkdocs/pymdown/#mark","text":"Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== .","title":"Mark"},{"location":"projects/mkdocs/pymdown/#smartsymbols","text":"SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...).","title":"SmartSymbols"},{"location":"projects/mkdocs/pymdown/#superfences","text":"SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs .","title":"SuperFences"},{"location":"projects/mkdocs/pymdown/#tabbed","text":"Tabbed adds support for creating tabbed groups of Markdown content. Example: === \"Fruit List\" - :apple: Apple - :banana: Banana - :kiwi: Kiwi === \"Fruit Table\" Fruit | Color --------------- | ----- :apple: Apple | Red :banana: Banana | Yellow :kiwi: Kiwi | Green Result: Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green","title":"Tabbed"},{"location":"projects/mkdocs/pymdown/#tasklist","text":"Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Tasklist"},{"location":"projects/mkdocs/pymdown/#tilde","text":"Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"Tilde"},{"location":"projects/mkdocs/revision-date/","text":"Revision date \u00b6 The mkdocs-git-revision-date-localized-plugin will add the date on which a Markdown file was last updated at the bottom of each page. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker. Requirements The date is extracted at the time of the build, so mkdocs build must be triggered from within a git repository. Installation \u00b6 Install the plugin using pip : pip install mkdocs-git-revision-date-localized-plugin Configuration \u00b6 Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - git-revision-date-localized Note that the date is printed according to the locale which is determined through the theme language that was set in mkdocs.yml . Language \u00b6 The language (i.e. locale) is deduced from the theme.language option. Format \u00b6 Default: date To change the date format, set the type parameter to one of date , datetime , iso_date , iso_datetime or timeago , e.g.: plugins : - search # necessary for search to work - git-revision-date-localized : type : date The following formats are supported: 28 November, 2019 # type: date 28 November, 2019 13:57:28 # type: datetime 2019-11-28 # type: iso_date 2019-11-28 13:57:26 # type: iso_datetime 20 hours ago # type: timeago Usage \u00b6 When enabled, the respective date is automatically added at the bottom of each page, e.g.: Last updated: 28 November, 2019","title":"Revision date"},{"location":"projects/mkdocs/revision-date/#revision-date","text":"The mkdocs-git-revision-date-localized-plugin will add the date on which a Markdown file was last updated at the bottom of each page. Bundled with the official Docker image This plugin is already installed for your convenience when you use the official Docker image , so the installation step can be skipped. Read the getting started guide to get up and running with Docker. Requirements The date is extracted at the time of the build, so mkdocs build must be triggered from within a git repository.","title":"Revision date"},{"location":"projects/mkdocs/revision-date/#installation","text":"Install the plugin using pip : pip install mkdocs-git-revision-date-localized-plugin","title":"Installation"},{"location":"projects/mkdocs/revision-date/#configuration","text":"Add the following lines to mkdocs.yml : plugins : - search # necessary for search to work - git-revision-date-localized Note that the date is printed according to the locale which is determined through the theme language that was set in mkdocs.yml .","title":"Configuration"},{"location":"projects/mkdocs/revision-date/#language","text":"The language (i.e. locale) is deduced from the theme.language option.","title":"Language"},{"location":"projects/mkdocs/revision-date/#format","text":"Default: date To change the date format, set the type parameter to one of date , datetime , iso_date , iso_datetime or timeago , e.g.: plugins : - search # necessary for search to work - git-revision-date-localized : type : date The following formats are supported: 28 November, 2019 # type: date 28 November, 2019 13:57:28 # type: datetime 2019-11-28 # type: iso_date 2019-11-28 13:57:26 # type: iso_datetime 20 hours ago # type: timeago","title":"Format"},{"location":"projects/mkdocs/revision-date/#usage","text":"When enabled, the respective date is automatically added at the bottom of each page, e.g.: Last updated: 28 November, 2019","title":"Usage"},{"location":"projects/mkdocs/search/","text":"Search \u00b6 The built-in search plugin provides client-side search inside the browser and is implemented using lunr.js which includes stemmers for the English language by default, while stemmers for other languages are included with lunr-languages , both of which are integrated with this theme. Make search work offline While search will not work for the file:// protocol, as web workers and the use of XMLHTTPRequest are both blocked by modern browsers for security reasons, the localsearch plugin and @squidfunk 's iframe-worker polyfill add support for cases where this is a mandatory requirement, e.g., for offline use. Installation \u00b6 The search plugin is a built-in plugin, and thus doesn't need to be installed. Configuration \u00b6 Add the following lines to mkdocs.yml : plugins : - search Language \u00b6 Default: best match for theme.language , automatically set Material for MkDocs selects the (best-)matching stemmer for the given theme language. Multilingual search can be enabled in mkdocs.yml by explicitly defining the search language(s): plugins : - search : lang : - en - de - ru The following language codes are supported: .md-language-list { -webkit-columns: 2; -moz-columns: 2; columns: 2; } .md-language-list li { -webkit-column-break-inside: avoid; page-break-inside: avoid; break-inside: avoid; } ar / Arabic da / Danish du / Dutch en / English fi / Finnish fr / French de / German hu / Hungarian it / Italian ja / Japanese no / Norwegian pt / Portuguese ro / Romanian ru / Russian es / Spanish sv / Swedish th / Thai tr / Turkish vi / Vietnamese Only specify the languages you really need Be aware that including support for other languages increases the general JavaScript payload by around 20kb (before gzip ) and by another 15-30kb per language. Tokenization \u00b6 Default: [\\s\\-]+ The separator for tokenization can be customized which makes it possible to index parts of words that are separated by - or . : plugins : - search : separator : '[\\s\\-\\.]+' Prebuilding \u00b6 Default: false MkDocs can generate a prebuilt index of all pages during build time, which provides performance improvements at the cost of more bandwidth. This may be beneficial for large documentation projects that are served with appropriate HTTP headers (e.g. Content-Encoding: gzip ). Material for MkDocs 5 finally brings experimental support for prebuilt indexes which can be enabled by adding the following lines to mkdocs.yml : plugins : - search : prebuild_index : true Usage \u00b6 When enabled, a search bar is shown in the header.","title":"Search"},{"location":"projects/mkdocs/search/#search","text":"The built-in search plugin provides client-side search inside the browser and is implemented using lunr.js which includes stemmers for the English language by default, while stemmers for other languages are included with lunr-languages , both of which are integrated with this theme. Make search work offline While search will not work for the file:// protocol, as web workers and the use of XMLHTTPRequest are both blocked by modern browsers for security reasons, the localsearch plugin and @squidfunk 's iframe-worker polyfill add support for cases where this is a mandatory requirement, e.g., for offline use.","title":"Search"},{"location":"projects/mkdocs/search/#installation","text":"The search plugin is a built-in plugin, and thus doesn't need to be installed.","title":"Installation"},{"location":"projects/mkdocs/search/#configuration","text":"Add the following lines to mkdocs.yml : plugins : - search","title":"Configuration"},{"location":"projects/mkdocs/search/#language","text":"Default: best match for theme.language , automatically set Material for MkDocs selects the (best-)matching stemmer for the given theme language. Multilingual search can be enabled in mkdocs.yml by explicitly defining the search language(s): plugins : - search : lang : - en - de - ru The following language codes are supported: .md-language-list { -webkit-columns: 2; -moz-columns: 2; columns: 2; } .md-language-list li { -webkit-column-break-inside: avoid; page-break-inside: avoid; break-inside: avoid; } ar / Arabic da / Danish du / Dutch en / English fi / Finnish fr / French de / German hu / Hungarian it / Italian ja / Japanese no / Norwegian pt / Portuguese ro / Romanian ru / Russian es / Spanish sv / Swedish th / Thai tr / Turkish vi / Vietnamese Only specify the languages you really need Be aware that including support for other languages increases the general JavaScript payload by around 20kb (before gzip ) and by another 15-30kb per language.","title":"Language"},{"location":"projects/mkdocs/search/#tokenization","text":"Default: [\\s\\-]+ The separator for tokenization can be customized which makes it possible to index parts of words that are separated by - or . : plugins : - search : separator : '[\\s\\-\\.]+'","title":"Tokenization"},{"location":"projects/mkdocs/search/#prebuilding","text":"Default: false MkDocs can generate a prebuilt index of all pages during build time, which provides performance improvements at the cost of more bandwidth. This may be beneficial for large documentation projects that are served with appropriate HTTP headers (e.g. Content-Encoding: gzip ). Material for MkDocs 5 finally brings experimental support for prebuilt indexes which can be enabled by adding the following lines to mkdocs.yml : plugins : - search : prebuild_index : true","title":"Prebuilding"},{"location":"projects/mkdocs/search/#usage","text":"When enabled, a search bar is shown in the header.","title":"Usage"}]}